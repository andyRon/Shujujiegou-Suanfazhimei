# 入门篇

https://github.com/wangzheng0822/algo

## 1-为什么要学习数据结构和算法？

### **1. 想要通关大厂面试，千万别让数据结构和算法拖了后腿**

很多大公司，比如 BAT、Google、Facebook，面试的时候都喜欢考算法、让人现场写代码。越是厉害的公司，越是注重考察数据结构与算法这类基础知识。相比短期能力，他们更看中你的==长期潜力==。 

我们学任何知识都是为了“**用**”的，是<font color=#FF8C00>**为了解决实际工作问题**</font>的，学习数据结构和算法自然也不例外。 

### **2. 业务开发工程师，你真的愿意做一辈子 CRUD boy 吗？**

对于大部分业务开发来说，我们平时可能更多的是利用已经封装好的现成的接口、类库来堆砌、翻译业务逻辑，很少需要自己实现数据结构和算法。但是，**不需要自己实现，并不代表什么都不需要了解。** 

如果不知道这些类库背后的原理，不懂得时间、空间复杂度分析，你如何能用好、用对它们？存储某个业务数据的时候，你如何知道应该用 **ArrayList**，还是 **Linked List** 呢？调用了某个函数之后，你又该<font color=#FF8C00>**如何评估代码的性能和资源的消耗呢？**</font> 

作为业务开发，我们会用到各种**框架、中间件和底层系统**，比如 Spring、RPC 框架、消息中间件、Redis 等等。**在这些==基础框架==中，一般都揉和了很多基础数据结构和算法的设计思想。** 

> Redis 中的有序集合为什么要用跳表来实现呢？为什么不用二叉树呢？ 🔖
>
> 因为它可以实现快速的查找数据，其数据查找的时间复杂度为O(logn)。此时可以对比一般的链表查询，时间复杂度O(n)，当数据规模n越大时跳表的查询效率优势就会越明显

**掌握数据结构和算法，不管对于阅读框架源码，还是理解其背后的设计思想，都是非常有用的。** 

> **如何实时地统计业务接口的 99% 响应时间？** 
>
> 两个堆🔖



### **3. 基础架构研发工程师，写出达到开源水平的框架才是你的目标！**

现在互联网上的技术文章、架构分享、开源项目满天飞，照猫画虎做一套基础框架并不难。我就拿RPC框架举例。 

不同的公司、不同的人做出的RPC框架，架构设计思路都差不多，最后实现的功能也都差不多。但是有的人做出来的框架，Bug很多、性能一般、扩展性也不好，只能在自己公司仅有的几个项目里面用一下。而有的人做的框架可以开源到 GitHub 上给很多人用，甚至被 Apache 收录。为什么会有这么大的差距呢？ 

高手之间的竞争其实就在<font color=#FF8C00>**细节**</font>。这些细节包括：**算法是不是够优化**，**数据存取的效率是不是够高**，**内存是不是够节省**等等。这些累积起来，决定了一个框架是不是优秀。 



### **4.对编程还有追求？不想被行业淘汰？那就不要只会写凑合能用的代码！**

**性能好坏起码是其中一个非常重要的评判标准**。

有的人写代码的时候，从来都不考虑**==非功能性的需求==**，只是完成功能，凑合能用就好；做事情的时候，也从来没有长远规划，只把眼前事情做好就满足了。  

如果你在一家成熟的公司，或者 BAT 这样的大公司，面对的是千万级甚至亿级的用户，开发的是 TB、PB 级别数据的处理系统。性能几乎是开发过程中时刻都要考虑的问题。一个简单的 ArrayList、Linked List 的选择问题，就可能会产生成千上万倍的性能差别。这个时候，数据结构和算法的意义就完全凸显出来了。 

其实，我觉得，**数据结构和算法这个东西，如果你不去学，可能真的这辈子都用不到，也感受不到它的好。但是一旦掌握，你就会常常被它的强大威力所折服**。之前你可能需要费很大劲儿来优化的代码，需要花很多心思来设计的架构，用了数据结构和算法之后，很容易就可以解决了。 



### **内容小结**

目的是**==建立时间复杂度、空间复杂度意识，写出高质量的代码，能够设计基础架构，提升编程技能，训练逻辑思维，积攒人生经验，以此获得工作回报，实现你的价值，完善你的人生==**。 

不管你是**业务开发工程师**，还是**基础架构工程师**；不管你是初入职场的初级工程师，还是工作多年的资深架构师，又或者是想转人工智能、区块链这些热门领域的程序员，数据结构与算法作为计算机的基础知识、核心知识，都是必须要掌握的。 

**掌握了数据结构与算法，你看待问题的==深度==，解决问题的角度就会完全不一样。**



### **课后思考**

- 你为什么要学习数据结构和算法呢？ 

  写出高质量的代码

  解决问题的新思路

  锻炼大脑🧠

  不当菜鸟

  

- 在过去的软件开发中，数据结构和算法在哪些地方帮到了你？



## 2-如何抓住重点，系统高效地学习数据结构与算法？

**没有找到好的学习方法，没有抓住学习的重点**。

### 什么是数据结构？什么是算法？

> 现在学习，并不是为了考试。

**虽然我们说没必要深挖严格的定义，但是这并不等于不需要理解概念。** 

广义上，**数据结构**是一组==数据==的==存储结构==。**算法**是**==操作==数据的一组==方法==**。 

图书馆储藏书籍。图书**按照一定规律编号**，就是书籍这种“数据”的存储结构。**查找书籍方法**都是算法。 

**经典数据结构和算法**，都是前人从很多实际操作场景中**抽象**出来的，经过非常多的**求证和检验**，可以高效地帮助我们解决很多实际的开发问题。 

**数据结构是为算法服务的，算法要作用在特定的数据结构之上**。它们是**相辅相成**的。

数据结构是**静态**的，它只是组织数据的一种方式。如果不在它的基础上操作、构建算法，**孤立存在的数据结构就是没用的**。 

 

### 学习的重点在什么地方？

**掌握一个数据结构与算法中最重要的概念**——<font color=#FF8C00>**复杂度分析**</font>。<u>考量效率和资源消耗的方法</u>。

一张图了解所有数据结构和算法的知识点。 

<img src="images/SJJG+SFZM-1-0.jpg" style="zoom: 25%;" />

**20 个最常用的、最基础**数据结构与算法，10 个数据结构：<font color=#FF8C00>数组、链表、栈、队列、散列表、二叉树、堆、跳表、图、Trie树</font>；10 个算法：<font color=#FF8C00>递归、排序、二分查找、搜索、哈希算法、贪心算法、分治算法、回溯算法、动态规划、字符串匹配算法</font>。 

要学习它的**“来历”，“自身的特点”，“适合解决的问题”以及“实际的应用场景”**。 

不要被动地记忆，要多辩证地思考，多问为什么。

### 一些可以让你事半功倍的学习技巧

#### 边学边练，适度刷题

**可以“适度”刷题，但一定不要浪费太多时间在刷题上**。

#### 多问、多思考、多互动

#### 打怪升级学习法



#### 知识需要沉淀，不要想试图一下子掌握所有

**学习知识的过程是==反复迭代、不断沉淀==的过程。** 



## 3-复杂度分析（上）：如何分析、统计算法的执行效率和资源消耗？

数据结构和算法本身解决的是“**快**”和“**省**”的问题，即如何让代码运行得更快，如何让代码更省存储空间。

**复杂度分析是整个算法学习的精髓，只要掌握了它，数据结构和算法的内容基本上就掌握了一半。** 

### 为什么需要复杂度分析？

**==事后统计法==**的局限性：

#### **1 测试结果非常依赖==测试环境==**

#### **2 测试结果受==数据规模==的影响很大**

对同一个排序算法，待排序数据的有序度不一样，排序的执行时间就会有很大的差别。极端情况下，如果数据已经是有序的，那排序算法不需要做任何操作，执行时间就会非常短。除此之外，如果测试数据规模太小，测试结果可能无法真实地反应算法的性能。比如，对于小规模的数据排序，插入排序可能反倒会比快速排序要快！ 

所以，**我们需要一个不用具体的测试数据来测试，就可以粗略地估计算法的执行效率的方法。**

### 大O复杂度表示法

算法的**执行效率**，粗略地讲，就是算法代码**执行的时间**。

如何在不运行代码的情况下，估算一下这段代码的执行时间。 


```c
int cal(int n) { 
   int sum = 0; 
   int i = 1; 
   for (; i <= n; ++i) { 
      sum = sum + i; 
   } 
   return sum; 
} 
```


从 CPU 的角度来看，这段代码的每一行都执行着类似的操作：**==读数据-运算-写数据==**。尽管每行代码对应的 CPU 执行的个数、执行的时间都不一样，但是，我们这里只是粗略估计，所以可以假设每行代码执行的时间都一样，为 **==unit_time==**。在这个假设的基础之上，这段代码的总执行时间是多少呢？ 

第 2、3 行代码分别需要 1 个 unit_time 的执行时间，第 4、5 行都运行了 n 遍，所以需要 **2n*unit_time**的执行时间，所以这段代码总的执行时间就是 **(2n+2)*unit_time**。可以看出来，**所有代码的执行时间 T(n) 与每行代码的执行次数成正比。** 

按照这个分析思路，我们再来看这段代码。 


```c
 int cal(int n) {
   int sum = 0;
   int i = 1;
   int j = 1;
   for (; i <= n; ++i) {
     j = 1;
     for (; j <= n; ++j) {
       sum = sum +  i * j;
     }
   }
 }
```


我们依旧假设每个语句的执行时间是 unit_time。那这段代码的总执行时间 T(n) 是多少呢？ 

第 2、3、4 行代码，每行都需要 1 个 unit_time 的执行时间，第 5、6 行代码循环执行了 n 遍，需要 **2n * unit_time** 的执行时间，第 7、8 行代码循环执行了 n<sup>2</sup>遍，所以需要 **2n<sup>2</sup> * unit_time** 的执行时间。所以，整段代码总的执行时间 **T(n) = (2n<sup>2</sup>+2n+3)*unit_time**。 

尽管我们不知道 unit_time 的具体值，但是通过这两段代码执行时间的推导过程，我们可以得到一个非常重要的规律，那就是，**所有代码的执行时间 T(n) 与每行代码的执行次数 n 成==正比==。** 

![](images/SJJG+SFZM-03-01.jpg)

其中，<u>T(n) 表示代码执行的时间；n 表示数据规模的大小；f(n) 表示每行代码执行的次数总和</u>。因为这是一个公式，所以用 f(n) 来表示。公式中的 O，表示代码的执行时间 T(n) 与 f(n) 表达式成**正比**。 

所以，第一个例子中的 T(n) = O(2n+2)，第二个例子中的 T(n) = O(2n<sup>2</sup>+2n+3)。这就是**大O时间复杂度表示法**。大O时间复杂度实际上并不具体表示代码真正的执行时间，而是表示**==代码执行时间随数据规模增长的变化趋势==**，所以，也叫作渐进时间复杂度（asymptotic time complexity），简称**时间复杂度**。 

当 n 很大时，你可以把它想象成 10000、100000。而公式中的**低阶、常量、系数**三部分并不左右增长趋势，所以都可以忽略。我们只需要记录一个最大量级就可以了，如果用大 O 表示法表示刚讲的那两段代码的时间复杂度，就可以记为：T(n) = O(n)； T(n) = O(n<sup>2</sup>)。 



### 时间复杂度分析

如何分析一段代码的时间复杂度？三个比较实用的方法。 

#### 1 只关注循环执行次数最多的一段代码

**在分析一个算法、一段代码的时间复杂度的时候，只关注循环执行次数最多的那一段代码就可以了。**这段核心代码执行次数的n的量级，就是整段要分析代码的时间复杂度。 


```c
 int cal(int n) {
   int sum = 0;
   int i = 1;
   for (; i <= n; ++i) {
     sum = sum + i;
   }
   return sum;
 }
```


其中第 2、3 行代码都是常量级的执行时间，与 n 的大小无关，所以对于复杂度并没有影响。循环执行次数最多的是第 4、5 行代码，所以这块代码要重点分析。前面我们也讲过，这两行代码被执行了 n 次，所以总的时间复杂度就是 O(n)。 

#### 2 加法法则：总复杂度等于量级最大的那段代码的复杂度


```c
int cal(int n) {
   int sum_1 = 0;
   int p = 1;
   for (; p < 100; ++p) {
     sum_1 = sum_1 + p;
   }

   int sum_2 = 0;
   int q = 1;
   for (; q < n; ++q) {
     sum_2 = sum_2 + q;
   }
 
   int sum_3 = 0;
   int i = 1;
   int j = 1;
   for (; i <= n; ++i) {
     j = 1; 
     for (; j <= n; ++j) {
       sum_3 = sum_3 +  i * j;
     }
   }
 
   return sum_1 + sum_2 + sum_3;
 }
```


这个代码分为三部分，分别是求 sum_1、sum_2、sum_3。我们可以分别分析每一部分的时间复杂度，然后把它们放到一块儿，再取一个量级最大的作为整段代码的复杂度。 

第一段的时间复杂度是多少呢？这段代码循环执行了 100 次，所以是一个常量的执行时间，跟 n 的规模无关。 

第二段代码和第三段代码的时间复杂度是多少呢？答案是 O(n) 和 O(n<sup>2</sup>)。 

综合这三段代码的时间复杂度，我们取其中最大的量级。所以，整段代码的时间复杂度就为 O(n<sup>2</sup>)。也就是说：**总的时间复杂度就等于量级最大的那段代码的时间复杂度**。

将这个规律抽象成公式就是： 

如果 T1(n)=O(f(n))，T2(n)=O(g(n))；那么 T(n)=T1(n)+T2(n)=max(O(f(n)), O(g(n))) =**O(max(f(n), g(n)))**. 

#### 3 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积

还有一个**乘法法则**。如果 T1(n)=O(f(n))，T2(n)=O(g(n))；那么`T(n)=T1(n)*T2(n)=O(f(n))*O(g(n))=O(f(n)*g(n))`. 

也就是说，假设 T1(n) = O(n)，T2(n) = O(n<sup>2</sup>)，则 T1(n) * T2(n) = O(n<sup>3</sup>)。落实到具体的代码上，我们可以把乘法法则看成是**==嵌套循环==**，我举个例子给你解释一下。 


```c
int cal(int n) {
   int ret = 0; 
   int i = 1;
   for (; i < n; ++i) {
     ret = ret + f(i);
   } 
 } 
 
 int f(int n) {
  int sum = 0;
  int i = 1;
  for (; i < n; ++i) {
    sum = sum + i;
  } 
  return sum;
 }
```


我们单独看 cal() 函数。假设 f() 只是一个普通的操作，那第 4～6 行的时间复杂度就是，T1(n) = O(n)。但 f() 函数本身不是一个简单的操作，它的时间复杂度是 T2(n) = O(n)，所以，整个 cal() 函数的时间复杂度就是，T(n) = T1(n) * T2(n) = O(n*n) = O(n<sup>2</sup>)。 



### 几种常见时间复杂度实例分析

虽然代码千差万别，但是常见的复杂度量级并不多。下面👇复杂度量级几乎涵盖了你今后可以接触的所有代码的复杂度量级。 

![](images/SJJG+SFZM-03-02.jpg)

对于上面罗列的复杂度量级，我们可以粗略地分为两类，**==多项式量级==**和**==非多项式量级==**。其中，非多项式量级只有两个：O(2<sup>n</sup>) 和 O(n!)，也叫[**==NP问题==**](https://zh.wikipedia.org/wiki/P/NP问题)(Non-Deterministic Polynomial, 非确定多项式)。 

当数据规模n越来越大时，非多项式量级算法的执行时间会**急剧增加**，求解问题的执行时间会无限增长。所以，非多项式时间复杂度的算法其实是非常低效的算法。我们主要关注几种常见的**多项式时间复杂度**。 

![](images/1678135-514c3ef55574c28b.jpg)

#### O(1)  


```c
int i = 8; 
int j = 6; 
int sum = i + j; 
```

**一般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万行的代码，其时间复杂度也是Ο(1)。** 

#### O(logn)、O(nlogn) 

对数阶时间复杂度非常常见，同时也是**最难分析**的一种时间复杂度。 


```c
i=1; 
while (i <= n)  { 
   i = i * 2; 
} 
```


根据我们前面讲的复杂度分析方法，第三行代码是循环执行次数最多的。所以，我们只要能计算出这行代码被执行了多少次，就能知道整段代码的时间复杂度。 

从代码中可以看出，变量 i 的值从 1 开始取，每循环一次就乘以 2。当大于 n 时，循环结束。还记得我们高中学过的等比数列吗？实际上，变量 i 的取值就是一个等比数列。如果我把它一个一个列出来，就应该是这个样子的： 

![](images/SJJG+SFZM-03-03.jpg)

所以，我们只要知道 x 值是多少，就知道这行代码执行的次数了。通过 2<sup>x</sup>=n 求解 x， 得x=log<sub>2</sub>n，所以，这段代码的时间复杂度就是 O(log<sub>2</sub>n)。 

现在，我把代码稍微改下，你再看看，这段代码的时间复杂度是多少？ 


```c
i=1; 
while (i <= n)  { 
   i = i * 3; 
} 
```


同上思路，这段代码的时间复杂度为 O(log<sub>3</sub>n)。 

我们知道，对数之间是可以互相转换的，log<sub>3</sub>n 就等于 log<sub>3</sub>2 * log<sub>2</sub>n，所以 O(log<sub>3</sub>n) = O(C * log<sub>2</sub>n)，其中 C=log<sub>3</sub>2 是一个常量。基于我们前面的一个理论：**在采用大 O 标记复杂度的时候，可以忽略系数，即 O(Cf(n)) = O(f(n))**。所以，O(log<sub>2</sub>n) 就等于 O(log<sub>3</sub>n)。因此，在对数阶时间复杂度的表示方法里，我们忽略对数的“底”，统一表示为 O(logn)。 

如果你理解了我前面讲的 O(logn)，那 O(nlogn) 就很容易理解了。还记得我们刚讲的乘法法则吗？如果一段代码的时间复杂度是 O(logn)，我们循环执行 n 遍，时间复杂度就是 O(nlogn) 了。而且，O(nlogn) 也是一种非常常见的算法时间复杂度。比如，归并排序、快速排序的时间复杂度都是 O(nlogn)。 

#### O(m+n)、O(m*n) 

有的代码的复杂度**由两个数据的规模**来决定。 


```c++
int cal(int m, int n) {
  int sum_1 = 0;
  int i = 1;
  for (; i < m; ++i) {
    sum_1 = sum_1 + i;
  }

  int sum_2 = 0;
  int j = 1;
  for (; j < n; ++j) {
    sum_2 = sum_2 + j;
  }

  return sum_1 + sum_2;
}
```


从代码中可以看出，m 和 n 是表示两个数据规模。我们无法事先评估 m 和 n 谁的量级大，所以我们在表示复杂度的时候，就不能简单地利用加法法则，省略掉其中一个。所以，上面代码的时间复杂度就是 O(m+n)。 

针对这种情况，原来的加法法则就不正确了，我们需要将加法规则改为：**T1(m) + T2(n) = O(f(m) + g(n))**。但是乘法法则继续有效：**T1(m)*T2(n) = O(f(m) * f(n))**。 

### 空间复杂度分析

相对而言，空间复杂度分析方法学起来就简单些。 

时间复杂度的全称是**渐进时间复杂度，表示算法的执行时间与数据规模之间的增长关系**。类比一下，空间复杂度全称就是渐进空间复杂度（asymptotic space complexity），**表示算法的==存储空间与数据规模之间的增长关系==。** 

我还是拿具体的例子来给你说明。（这段代码有点“傻”，一般没人会这么写，我这么写只是为了方便给你解释。） 


```c++
void print(int n) {
  int i = 0;
  int[] a = new int[n];
  for (i; i < n; ++i) {
    a[i] = i * i;
  }

  for (i = n-1; i >= 0; --i) {
    print out a[i]
  }
}
```


跟时间复杂度分析一样，我们可以看到，第 2 行代码中，我们申请了一个空间存储变量 i，但是它是常量阶的，跟数据规模 n 没有关系，所以我们可以忽略。第 3 行申请了一个大小为 n 的 int 类型数组，除此之外，剩下的代码都没有占用更多的空间，所以整段代码的空间复杂度就是 O(n)。 

我们常见的空间复杂度就是 **O(1)、O(n)、O(n^2^ )**，像 O(logn)、O(nlogn) 这样的对数阶复杂度平时都用不到。而且，空间复杂度分析比时间复杂度分析要简单很多。所以，对于空间复杂度，掌握刚我说的这些内容已经足够了。 



### 小结

复杂度也叫**渐进复杂度**，包括时间复杂度和空间复杂度，用来分析算法执行效率与数据规模之间的增长关系，可以粗略地表示，越高阶复杂度的算法，执行效率越低。常见的复杂度并不多，从低阶到高阶有：O(1)、O(logn)、O(n)、O(nlogn)、O(n<sup>2</sup> )。几乎所有的数据结构和算法的复杂度都跑不出这几个。 

![](images/SJJG+SFZM-03-04.jpg)

**复杂度分析并不难，关键在于多练**。  



### **课后思考**

有人说，我们项目之前都会进行性能测试，再做代码的时间复杂度、空间复杂度分析，是不是多此一举呢？而且，每段代码都分析一下时间复杂度、空间复杂度，是不是很浪费时间呢？你怎么看待这个问题呢？ 



我不认为是多此一举，渐进时间，空间复杂度分析为我们提供了一个很好的**==理论分析的方向==**，并且它是宿主平台无关的，能够让我们对我们的程序或算法有一个大致的认识，让我们知道，比如在最坏的情况下程序的执行效率如何，同时也为我们交流提供了一个不错的桥梁，我们可以说，算法1的时间复杂度是O(n)，算法2的时间复杂度是O(logN)，这样我们立刻就对不同的算法有了一个“效率”上的**感性认识**。

当然，渐进式时间，空间复杂度分析只是一个理论模型，只能提供给粗略的估计分析，我们不能直接断定就觉得O(logN)的算法一定优于O(n), 针对不同的宿主环境，不同的数据集，不同的数据量的大小，在实际应用上面可能真正的性能会不同，个人觉得，针对不同的实际情况，进而进行一定的性能基准测试是很有必要的，比如在统一一批手机上(同样的硬件，系统等等)进行横向基准测试，进而选择适合特定应用场景下的最有算法。

综上所述，渐进式时间，==空间复杂度分析与性能基准测试并不冲突，而是相辅相成的==，但是一个低阶的时间复杂度程序有极大的可能性会优于一个高阶的时间复杂度程序，所以在实际编程中，时刻关心理论时间，空间度模型是有助于产出效率高的程序的，同时，因为渐进式时间，空间复杂度分析只是提供一个粗略的分析模型，因此也不会浪费太多时间，重点在于在编程时，要具有这种复杂度分析的思维。



## 4-复杂度分析（下）：浅析最好、最坏、平均、均摊时间复杂度🔖



四个复杂度分析：<font color=#FF8C00>最好情况时间复杂度</font>（best case time complexity）、<font color=#FF8C00>最坏情况时间复杂度</font>（worst case time complexity）、<font color=#FF8C00>平均情况时间复杂度</font>（average case time complexity）、<font color=#FF8C00>均摊时间复杂度</font>（amortized time complexity）。

### **1.最好、最坏情况时间复杂度**

分析一下这段代码的时间复杂度。 

```c
// n 表示数组 array 的长度
int find(int[] array, int n, int x) {
  int i = 0;
  int pos = -1;
  for (; i < n; ++i) {
    if (array[i] == x) pos = i;
  }
  return pos;
}
```

这段代码要实现的功能是，在一个无序的数组（array）中，查找变量 x 出现的位置。如果没有找到，就返回 -1。按照上节课讲的分析方法，这段代码的复杂度是 O(n)，其中，n 代表数组的长度。 

在数组中查找一个数据，并不需要每次都把整个数组都遍历一遍，因为有可能中途找到就可以提前结束循环了。但是，这段代码写得不够高效。我们可以这样优化一下这段查找代码。 

```c
// n 表示数组 array 的长度
int find(int[] array, int n, int x) {
  int i = 0;
  int pos = -1;
  for (; i < n; ++i) {
    if (array[i] == x) {
       pos = i;
       break;
    }
  }
  return pos;
} 
```

这个时候，问题就来了。我们优化完之后，这段代码的时间复杂度还是 O(n) 吗？很显然，咱们上一节讲的分析方法，解决不了这个问题。 

因为，要查找的变量 x 可能出现在数组的任意位置。如果数组中第一个元素正好是要查找的变量 x，那就不需要继续遍历剩下的 n-1 个数据了，那时间复杂度就是 O(1)。但如果数组中不存在变量 x，那我们就需要把整个数组都遍历一遍，时间复杂度就成了 O(n)。所以，不同的情况下，这段代码的时间复杂度是不一样的。 

为了表示代码在不同情况下的不同时间复杂度，我们需要引入三个概念：最好情况时间复杂度、最坏情况时间复杂度和平均情况时间复杂度。 

顾名思义，**最好情况时间复杂度就是，在最理想的情况下，执行这段代码的时间复杂度**。就像我们刚刚讲到的，在最理想的情况下，要查找的变量 x 正好是数组的第一个元素，这个时候对应的时间复杂度就是最好情况时间复杂度。 

同理，**最坏情况时间复杂度就是，在最糟糕的情况下，执行这段代码的时间复杂度**。就像刚举的那个例子，如果数组中没有要查找的变量 x，我们需要把整个数组都遍历一遍才行，所以这种最糟糕情况下对应的时间复杂度就是最坏情况时间复杂度。 

### **2.平均情况时间复杂度**

上面两种情况都是极端情况下的代码复杂度，引入一个更贴合实际情况的概念：平均情况时间复杂度，后面我简称为**平均时间复杂度**。

要查找的变量 x 在数组中的位置，有 n+1 种情况：**在数组的 0～n-1 位置中和不在数组中**。我们把每种情况下，查找需要遍历的元素个数累加起来，然后再除以 n+1，就可以得到需要遍历的元素个数的平均值，即： 

![](images/SJJG+SFZM-04-01.jpg)

根据大O标记法，省略掉系数、低阶、常量，简化后得到平均时间复杂度就是 O(n)。 

这个结论虽然是正确的，但是计算过程稍微有点儿问题。究竟是什么问题呢？我们刚讲的这 n+1 种情况，出现的概率并不是一样的。我带你具体分析一下。

我们知道，要查找的变量 x，要么在数组里，要么就不在数组里。这两种情况对应的概率统计起来很麻烦，为了方便你理解，我们假设在数组中与不在数组中的概率都为 1/2。另外，要查找的数据出现在 0～n-1 这 n 个位置的概率也是一样的，为 1/n。所以，根据**概率乘法法则**，要查找的数据出现在 0～n-1 中任意位置的概率就是 1/(2n)。 

因此，前面的推导过程中存在的最大问题就是，没有将各种情况发生的概率考虑进去。如果我们把每种情况发生的概率也考虑进去，那平均时间复杂度的计算过程就变成了这样： 

![](images/SJJG+SFZM-04-02.jpg)

这个值就是概率论中的**加权平均值**，也叫作**期望值**，所以平均时间复杂度的全称应该叫**加权平均时间复杂度**或者**期望时间复杂度**。 

引入概率之后，前面那段代码的加权平均值为 (3n+1)/4。用大 O 表示法来表示，去掉系数和常量，这段代码的加权平均时间复杂度仍然是 O(n)。 

你可能会说，平均时间复杂度分析好复杂啊，还要涉及概率论的知识。<u>实际上，在大多数情况下，我们并不需要区分最好、最坏、平均情况时间复杂度三种情况。</u>像我们上一节课举的那些例子那样，很多时候，我们使用一个复杂度就可以满足需求了。**只有同一块代码在不同的情况下，时间复杂度有量级的差距，我们才会使用这三种复杂度表示法来区分。** 

### **3.均摊时间复杂度**

到此为止，你应该已经掌握了算法复杂度分析的大部分内容了。下面我要给你讲一个更加高级的概念，均摊时间复杂度，以及它对应的分析方法，摊还分析（或者叫**平摊分析**）。 

均摊时间复杂度，听起来跟平均时间复杂度有点儿像。对于初学者来说，这两个概念确实非常容易弄混。我前面说了，大部分情况下，我们并不需要区分最好、最坏、平均三种复杂度。平均复杂度只在某些特殊情况下才会用到，而均摊时间复杂度应用的场景比它更加特殊、更加有限。 

老规矩，我还是借助一个具体的例子来帮助你理解。（当然，这个例子只是我为了方便讲解想出来的，实际上没人会这么写。） 

```c
 // array 表示一个长度为 n 的数组
 // 代码中的 array.length 就等于 n
 int[] array = new int[n];
 int count = 0;
 
 void insert(int val) {
    if (count == array.length) {
       int sum = 0;
       for (int i = 0; i < array.length; ++i) {
          sum = sum + array[i];
       }
       array[0] = sum;
       count = 1;
    }

    array[count] = val;
    ++count;
 }
```

我先来解释一下这段代码。这段代码实现了一个往数组中插入数据的功能。当数组满了之后，也就是代码中的 count == array.length 时，我们用 for 循环遍历数组求和，并清空数组，将求和之后的 sum 值放到数组的第一个位置，然后再将新的数据插入。但如果数组一开始就有空闲空间，则直接将数据插入数组。 

那这段代码的时间复杂度是多少呢？你可以先用我们刚讲到的三种时间复杂度的分析方法来分析一下。 

最理想的情况下，数组中有空闲空间，我们只需要将数据插入到数组下标为 count 的位置就可以了，所以最好情况时间复杂度为 O(1)。最坏的情况下，数组中没有空闲空间了，我们需要先做一次数组的遍历求和，然后再将数据插入，所以最坏情况时间复杂度为 O(n)。 

那平均时间复杂度是多少呢？答案是 O(1)。我们还是可以通过前面讲的概率论的方法来分析。 

假设数组的长度是 n，根据数据插入的位置的不同，我们可以分为 n 种情况，每种情况的时间复杂度是 O(1)。除此之外，还有一种“额外”的情况，就是在数组没有空闲空间时插入一个数据，这个时候的时间复杂度是 O(n)。而且，这 n+1 种情况发生的概率一样，都是 1/(n+1)。所以，根据加权平均的计算方法，我们求得的平均时间复杂度就是： 

![](images/SJJG+SFZM-04-03.jpg)



至此为止，前面的最好、最坏、平均时间复杂度的计算，理解起来应该都没有问题。但是这个例子里的平均复杂度分析其实并不需要这么复杂，不需要引入概率论的知识。这是为什么呢？我们先来对比一下这个 insert() 的例子和前面那个 find() 的例子，你就会发现这两者有很大差别。 

首先，find() 函数在极端情况下，复杂度才为 O(1)。但 insert() 在大部分情况下，时间复杂度都为 O(1)。只有个别情况下，复杂度才比较高，为 O(n)。这是 insert()**第一个**区别于 find() 的地方。 

我们再来看**第二个**不同的地方。对于 insert() 函数来说，O(1) 时间复杂度的插入和 O(n) 时间复杂度的插入，出现的频率是非常有规律的，而且有一定的前后时序关系，一般都是一个 O(n) 插入之后，紧跟着 n-1 个 O(1) 的插入操作，循环往复。 

所以，针对这样一种特殊场景的复杂度分析，我们并不需要像之前讲平均复杂度分析方法那样，找出所有的输入情况及相应的发生概率，然后再计算加权平均值。 

针对这种特殊的场景，我们引入了一种更加简单的分析方法：<font color=#FF8C00>**摊还分析法**</font>，通过摊还分析得到的时间复杂度我们起了一个名字，叫**均摊时间复杂度**。 

<u>那究竟如何使用摊还分析法来分析算法的均摊时间复杂度呢？</u> 

我们还是继续看在数组中插入数据的这个例子。每一次 O(n) 的插入操作，都会跟着 n-1 次 O(1) 的插入操作，所以把耗时多的那次操作均摊到接下来的 n-1 次耗时少的操作上，均摊下来，这一组连续的操作的均摊时间复杂度就是 O(1)。这就是均摊分析的大致思路。你都理解了吗？ 

均摊时间复杂度和摊还分析应用场景比较特殊，所以我们并不会经常用到。为了方便你理解、记忆，我这里简单总结一下它们的应用场景。如果你遇到了，知道是怎么回事儿就行了。 

对一个数据结构进行一组连续操作中，大部分情况下时间复杂度都很低，只有个别情况下时间复杂度比较高，而且这些操作之间存在前后连贯的时序关系，这个时候，我们就可以将这一组操作放在一块儿分析，看是否能**将较高时间复杂度那次操作的耗时，平摊到其他那些时间复杂度比较低的操作上**。而且，在能够应用均摊时间复杂度分析的场合，一般均摊时间复杂度就等于最好情况时间复杂度。 

尽管很多数据结构和算法书籍都花了很大力气来区分平均时间复杂度和均摊时间复杂度，但其实我个人认为，**均摊时间复杂度就是一种特殊的平均时间复杂度**，我们没必要花太多精力去区分它们。你最应该掌握的是它的分析方法，摊还分析。至于分析出来的结果是叫平均还是叫均摊，这只是个说法，并不重要。 

### **4.内容小结**

最好、最坏情况下的时间复杂度分析起来比较简单，但平均、均摊两个复杂度分析相对比较复杂。如果你觉得理解得还不是很深入，不用担心，在后续具体的数据结构和算法学习中，我们可以继续慢慢实践！ 

### **5.课后思考**

分析一下下面这个 add() 函数的时间复杂度。 


```c
// 全局变量，大小为 10 的数组 array，长度 len，下标 i。
int array[] = new int[10]; 
int len = 10;
int i = 0;

// 往数组中添加一个元素
void add(int element) {
   if (i >= len) { // 数组空间不够了
     // 重新申请一个 2 倍大小的数组空间
     int new_array[] = new int[len*2];
     // 把原来 array 数组中的数据依次 copy 到 new_array
     for (int j = 0; j < len; ++j) {
       new_array[j] = array[j];
     }
     // new_array 复制给 array，array 现在大小就是 2 倍 len 了
     array = new_array;
     len = 2 * len;
   }
   // 将 element 放到下标为 i 的位置，下标 i 加一
   array[i] = element;
   ++i;
}

```



最好是O(1)，最差是O(n), 均摊是O(1)





# 基础篇

## 5-数组：为什么很多编程语言中数组都从0开始编号？ 

 

### 如何实现随机访问？

**数组（Array）是一种<font color=#FF8C00>线性表</font>数据结构。它用一组<font color=#FF8C00>连续的内存空间</font>，来存储一组具有<font color=#FF8C00>相同类型</font>的数据。** 

1. **<font color=#FF8C00>线性表（Linear List）</font>**就是数据排成像一条线一样的结构。每个线性表上的数据最多只有**前和后**两个方向。其实除了<font color=#FF8C00>数组，链表、队列、栈</font>等也是线性表结构。 

<img src="images/SJJG+SFZM-05-01.jpg"  />

**<font color=#FF8C00>非线性表</font>**，比如<font color=#FF8C00>二叉树、堆、图</font>等。在非线性表中，数据之间并不是简单的**前后关系**。

<img src="images/SJJG+SFZM-05-02.jpg"  />

2. <font color=#FF8C00>连续的内存空间和相同类型的数据</font>。

​	利：“**随机访问**”。

​	弊：很多操作变得非常低效，比如要想在数组中删除、插入一个数据，为了保证连续性，就需要做大量的数据搬移工作。 

#### 数组是如何实现根据下标随机访问数组元素的吗？

以长度为 10 的 int 类型的数组 `int[] a = new int[10]` 为例。

如下图，计算机给数组 a[10]，分配了一块连续内存空间 1000～1039，其中，内存块的首地址为 base_address = 1000。 

<img src="images/SJJG+SFZM-05-03.jpg"  />

计算机会给每个内存单元分配一个地址，计算机通过地址来访问内存中的数据。当计算机需要随机访问数组中的某个元素时，它会首先通过下面的**==寻址公式==**，计算出该元素存储的内存地址： 


```java
a[i]_address = base_address + i * data_type_size 
```


其中 **data_type_size** 表示数组中每个元素的大小。这个例子数组中存储的是 int 类型数据，所以 data_type_size 就为 4 个字节。 

#### 数组和链表的区别？

不准确表达，“链表适合插入、删除，时间复杂度 O(1)；数组适合查找，查找时间复杂度为 O(1)”。 

数组是适合查找操作，但是查找的时间复杂度并不为 O(1)。即便是排好序的数组，你用二分查找，时间复杂度也是 O(logn)。所以，正确的表述应该是，**<font color=#FF8C00>数组支持随机访问，根据下标随机访问的时间复杂度为 O(1)</font>**。 



### 低效的“插入”和“删除”

数组为了保持内存数据的连续性，会导致插入、删除比较低效。

#### **插入操作** 

假设数组的长度为 n，现在，如果我们需要将一个数据插入到数组中的第 k 个位置。为了把第 k 个位置腾出来，给新来的数据，我们需要将第 k～n 这部分的元素都顺序地往后挪一位。那插入操作的时间复杂度是多少呢？

如果在数组的末尾插入元素，那就不需要移动数据了，这时的时间复杂度为 O(1)。但如果在数组的开头插入元素，那所有的数据都需要依次往后移动一位，所以最坏时间复杂度是 O(n)。 因为我们在每个位置插入元素的概率是一样的，所以平均情况时间复杂度为 <u>(1+2+…n)/n=O(n)</u>。 

如果数组中的数据是有序的，我们在某个位置插入一个新的元素时，就必须按照刚才的方法搬移 k 之后的数据。但是，如果数组中存储的数据并没有任何规律，数组只是被当作一个存储数据的集合。在这种情况下，如果要将某个数组插入到第 k 个位置，为了避免大规模的数据搬移，我们还有一个简单的办法就是，==直接将第 k 位的数据搬移到数组元素的最后，把新的元素直接放入第 k 个位置。== 

为了更好地理解，我们举一个例子。假设数组 a[10] 中存储了如下 5 个元素：a，b，c，d，e。 

我们现在需要将元素 x 插入到第 3 个位置。我们只需要将 c 放入到 a[5]，将 a[2] 赋值为 x 即可。最后，数组中的元素如下： a，b，x，d，e，c。 

![](images/SJJG+SFZM-05-04.jpg)

利用这种处理技巧，在特定场景下，在第 k 个位置插入一个元素的时间复杂度就会降为 O(1)。这个处理思想在快排中也会用到。 

#### 删除操作

跟插入数据类似，如果我们要删除第 k 个位置的数据，为了内存的连续性，也需要搬移数据，不然中间就会出现空洞，内存就不连续了。 

和插入类似，如果删除数组末尾的数据，则最好情况时间复杂度为 O(1)；如果删除开头的数据，则最坏情况时间复杂度为 O(n)；平均情况时间复杂度也为 O(n)。 

实际上，在某些特殊场景下，我们并不一定非得追求数组中数据的连续性。如果我们**将多次删除操作集中在一起执行**，删除的效率是不是会提高很多呢？ 

我们继续来看例子。数组 a[10] 中存储了 8 个元素：a，b，c，d，e，f，g，h。现在，我们要依次删除 a，b，c 三个元素。 

![](images/SJJG+SFZM-05-05.jpg)



为了避免 d，e，f，g，h 这几个数据会被搬移三次，我们可以先记录下已经删除的数据。<u>每次的删除操作并不是真正地搬移数据，只是记录数据已经被删除。当数组没有更多空间存储数据时，我们再触发执行一次真正的删除操作，这样就大大减少了删除操作导致的数据搬移。</u> 

如果你了解 **JVM**(JAVAV虚拟机)，你会发现，这不就是**==JVM标记清除垃圾回收算法==**的核心思想吗？没错，数据结构和算法的魅力就在于此，<font color=#FF8C00>很多时候我们并不是要去死记硬背某个数据结构或者算法，而是要学习它背后的思想和处理技巧，这些东西才是最有价值的</font>。如果你细心留意，不管是在软件开发还是架构设计中，总能找到某些算法和数据结构的影子。 

### 警惕数组的访问越界问题

> 在C语言中，只要不是访问受限的内存，所有的内存空间都是可以自由访问的。

数组越界在 C 语言中是一种**==未决行为==**，并没有规定数组访问越界时编译器应该如何处理。因为，**访问数组的本质就是访问一段连续内存，只要数组通过偏移计算得到的内存地址是可用的，那么程序就可能不会报任何错误**。

很多计算机病毒也正是利用到了代码中的数组越界可以访问非法地址的漏洞，来攻击系统。

Java有越界检查，`java.lang.ArrayIndexOutOfBoundsException`。

### 容器能否完全替代数组？

针对数组类型，很多语言都提供了容器类，比如 Java 中的 ArrayList、C++ STL 中的 vector。

Java中`ArrayList`的优势：**将很多数组操作（比如插入、删除等）的细节封装起来**；**支持动态扩容**。

最好在创建 ArrayList 的时候**事先指定数据大小**。事先指定数据大小可以省掉很多次内存申请和数据搬移操作。

数组更适合的情况：

1. ArrayList无法存储基本类型，需要封装为包装类；
2. 如果数据大小事先已知，并且对数据的操作非常简单，用不到 ArrayList 提供的大部分方法，也可以直接使用数组。
3. 多维数组时，用数组往往会更加直观

```java
Object[][] array;
  
ArrayList<ArrayList<Object>> array;
```

总结：对于**业务开发**，直接使用容器就足够了。一些非常**底层的开发**，比如开发网络框架，性能的优化需要做到极致，这个时候数组就会优于容器。



### 思考题

> 为什么数组要从 0 开始编号，而不是从 1 开始呢？

原因一：”下标“理解为”偏移（offset）“，从1开始，对CPU来说，就多了一次减法指令。

```c
a[k]_address = base_address + k * type_size

a[k]_address = base_address + (k-1)*type_size
```

原因二：历史原因。C语言设计者用 0 开始计数数组下标。

> 前面我基于数组的原理引出 **JVM的标记清除垃圾回收算法**的核心理念。那怎么理解的标记清除垃圾回收算法。 

大多数主流虚拟机采用可达性分析算法来判断对象是否存活，在标记阶段，会遍历所有 GC ROOTS，将所有 GC ROOTS 可达的对象标记为存活。只有当标记工作完成后，清理工作才会开始。

不足：

1. 效率问题。标记和清理效率都不高，但是当知道只有少量垃圾产生时会很高效。
2. 空间问题。会产生不连续的内存空间碎片。

> 前面我们讲到一维数组的内存寻址公式，那你可以思考一下，类比一下，二维数组的内存寻址公式是怎样的呢？ 

对于 m * n 的数组，`a [i][j]` (i < m,j < n)的地址为：`address = base_address + ( i * n + j) * type_size` 

🔖 m和n怎么区分先后？



## 06 链表（上）：如何实现LRU缓存淘汰算法?

缓存是一种提高数据读取性能的技术，在硬件设计、软件开发中都有着非常广泛的应用，比如常见的 ==CPU缓存==、==数据库缓存==、==浏览器缓存==等等。

> 当缓存被用满时，哪些数据应该被清理出去，哪些数据应该被保留？

这就需要==缓存淘汰策略==来决定。常见的策略有三种：

- 先进先出策略 FIFO（First In，First Out）
- 最少使用策略 LFU（Least Frequently Used）
- 最近最少使用策略 LRU（Least Recently Used）

> 数组和链表是两个**非常基础、非常常用**的数据结构。
>
> 某种意义上是仅有的两种数据存储结构。 因为使用内存空间时，只有两种方式：要么是==连续==的内存空间——数组，要么是==不连续==的内存空间——链表。 ==其它复杂的数据结构，其实都是在数组和链表的基础上形成的==。

### 五花八门的链表结构

链表并不需要一块连续的内存空间，它通过“指针”将一组零散的内存块串联起来使用。

![](images/SJJG+SFZM-06-01.jpg)



三种最常见的链表结构：

#### 1 单链表

![](images/SJJG+SFZM-06-02.jpg)

>  结点
>
>  后继指针next
>
>  头结点
>
>  尾结点 		
>
>  空地址NULL

链表也支持数据的查找、插入和删除操作。

![](images/SJJG+SFZM-06-03.jpg)

单链表的插入、删除操作的时间复杂度都是 O(1) 。

#### 2 循环链表

循环链表是一种特殊的单链表。

![](images/SJJG+SFZM-06-04.jpg)

和单链表相比，循环链表的优点是从链尾到链头比较方便。当要处理的数据具有**环型结构**特点时，就特别适合采用循环链表。比如著名的**约瑟夫问题**。

#### 3 双向链表

![](images/SJJG+SFZM-06-05.jpg)

双向链表需要额外的两个空间来存储<u>后继结点和前驱结点的地址</u>。

> 前驱指针prev
>
> 双向遍历



从链表中**删除**一个数据无外乎这两种情况：

- 删除结点中“值等于某个给定值”的结点；
- 删除给定指针指向的结点。

第一种情况，单链表和双向链表，为了查找到值等于给定值的结点，都需要从头结点开始一个一个依次遍历对比，直到找到值等于给定值的结点，然后再通过我前面讲的指针操作将其删除。（删除之前先要找到要删除的节点）

第二种情况，已经找到了要删除的结点，但是删除某个结点 q 需要知道其前驱结点，而单链表并不支持直接获取前驱结点，所以，为了找到前驱结点，我们还是要从头结点开始遍历链表，直到 p->next=q，说明 p 是 q 的前驱结点。而双向链表中的结点已经保持前驱结点的指针了，不需要遍历。

**插入**的情况类似。



除了插入、删除操作有优势之外，对于一个==有序链表==，双向链表的按值查询的效率也要比单链表高一些。因为，我们可以记录上次查找的位置 p，每次查询时，根据要查找的值与 p 的大小关系，决定是往前还是往后查找，所以平均只需要查找一半的数据。

> Java中的LinkedHashMap用到双向链表。

> 💡用**==空间换时间==**的设计思想：当内存空间充足的时候，如果我们更加追求代码的执行速度，我们就可以选择空间复杂度相对较高、但时间复杂度相对很低的算法或者数据结构。
>
> 相反，如果内存比较紧缺，比如代码跑在手机或者单片机上，这个时候，就要反过来用**==时间换空间==**的设计思路。



双向循环链表

![](images/d1665043b283ecdf79b157cfc9e5ed91.jpg)

### 链表 VS 数组性能大比拼

![](images/4f63e92598ec2551069a0eef69db7168.jpg)

数组和链表的对比，并不能局限于时间复杂度。

数组简单易用，在实现上使用的是连续的内存空间，可以借助 **CPU的缓存机制**，预读数组中的数据，所以访问效率更高。而链表在内存中并不是连续存储，所以对 CPU缓存不友好，没办法有效预读。

数组的缺点是==大小固定==，一经声明就要占用整块连续内存空间。如果声明的数组过大，系统可能没有足够的连续内存空间分配给它，导致“内存不足（out of memory）”。如果声明的数组过小，则可能出现不够用的情况。这时只能再申请一个更大的内存空间，把原数组拷贝进去，非常费时。链表本身没有大小的限制，天然地支持==动态扩容==，我觉得这也是它与数组最大的区别。



### 思考题

> 如何基于链表实现 LRU 缓存淘汰算法？

思路：维护一个有序单链表，越靠近链表尾部的结点是越早之前访问的。当有一个新的数据被访问时，我们从链表头开始顺序遍历链表。

1. 如果此数据之前已经被缓存在链表中了，我们遍历得到这个数据对应的结点，并将其从原来的位置删除，然后再插入到链表的头部。
2. 如果此数据没有在缓存链表中，又可以分为两种情况：
   - 如果此时缓存未满，则将此结点直接插入到链表的头部；
   - 如果此时缓存已满，则链表尾结点删除，将新的数据结点插入链表的头部。



> 如何利用数组实现 LRU 缓存淘汰策略呢？



> 如果字符串是通过单链表来存储的，那该如何来判断是一个回文串呢？你有什么好的解决思路呢？相应的时间空间复杂度又是多少呢？





> “数组简单易用，在实现上使用的是连续的内存空间，可以借助 CPU 的缓存机制，预读数组中的数据，所以访问效率更高。而链表在内存中并不是连续存储，所以对 CPU 缓存不友好，没办法有效预读。” 
>
> 这里的**CPU缓存机制**指的是什么？为什么就数组更好了？

CPU在从内存读取数据的时候，会先把读取到的数据加载到CPU的缓存中。而CPU每次从内存读取数据并不是只读取那个特定要访问的地址，而是读取一个数据块并保存到CPU缓存中，然后下次访问内存数据的时候就会先从CPU缓存开始查找，如果找到就不需要再从内存中取。

这样就实现了比内存访问速度更快的机制，也就是CPU缓存存在的意义:<u>为了弥补内存访问速度过慢与CPU执行速度快之间的差异而引入</u>。

 对于数组来说，存储空间是连续的，所以在加载某个下标的时候可以把以后的几个下标元素也加载到CPU缓存这样执行速度会快于存储空间不连续的链表存储。



## 07 链表（下）：如何轻松写出正确的链表代码？

复杂的链表操作，比如**==链表反转、有序链表合并==**等。

> 自己有决心并且付出精力是成功的先决条件，除此之外，我们还需要一些方法和技巧。

### 技巧一：理解指针或引用的含义

有些语言有“指针”的概念，比如C语言；有些语言没有指针，取而代之的是“引用”，比如 Java、Python。不管是“指针”还是“引用”，实际上，它们的意思都是一样的，都是存储所指**==对象的内存地址==**。

将某个变量赋值给指针，实际上就是将这个变量的地址赋值给指针，或者反过来说，指针中存储了这个变量的内存地址，指向了这个变量，通过指针就能找到这个变量。

`p->next=q`：p 结点中的 next 指针存储了 q 结点的内存地址。

`p->next=p->next->next`：p 结点的 next 指针存储了 p 结点的下下一个结点的内存地址。



### 技巧二：警惕指针丢失和内存泄漏

![](images/SJJG+SFZM-07-01.jpg)

插入结点时，一定要注意操作的顺序。

C语言，删除链表结点时，也一定要记得手动释放内存空间。

### 技巧三：利用哨兵简化实现难度



![](images/SJJG+SFZM-07-02.jpg)

#### 技巧四：重点留意边界条件处理



### 技巧五：举例画图，辅助思考

![](images/SJJG+SFZM-07-03.jpg)

### 技巧六：多写多练，没有捷径



5个常见的链表操作：

- 单链表反转
- 链表中环的检测
- 两个有序的链表合并
- 删除链表倒数第 n 个结点
- 求链表的中间结点



## 08 栈：如何实现浏览器的前进和后退功能？

### 如何理解“栈”？

后进者先出，先进者后出，这就是典型的“栈”结构。

![](images/SJJG+SFZM-08-01.jpg)

栈是**一种“操作受限”的线性表**，只允许在一端插入和删除数据。

从功能上来说，数组或链表确实可以替代栈，但==特定的数据结构是对特定场景的抽象==，而且，数组或链表暴露了太多的操作接口，操作上的确灵活自由，但使用时就比较不可控，自然也就更容易出错。

> 当某个数据集合只涉及在一端插入和删除数据，并且满足后进先出、先进后出的特性，这时就应该首选“栈”这种数据结构。

### 如何实现一个“栈”？

栈既可以用数组来实现（==顺序栈==），也可以用链表来实现（==链式栈==）。

### 支持动态扩容的顺序栈



![](images/SJJG+SFZM-08-02.jpg)





![](images/SJJG+SFZM-08-03.jpg)

### 栈在函数调用中的应用

```java

int main() {
   int a = 1; 
   int ret = 0;
   int res = 0;
   ret = add(3, 5);
   res = a + ret;
   printf("%d", res);
   reuturn 0;
}

int add(int x, int y) {
   int sum = 0;
   sum = x + y;
   return sum;
}
```



![](images/SJJG+SFZM-08-04.jpg)

### 栈在表达式求值中的应用

编译器如何利用栈来实现表达式求值?

```
34+13*9+44-12/3
```

编译器就是通过两个栈来实现的:

1. 一个保存操作数的栈
2. 另一个是保存运算符的栈。

从左向右遍历表达式，当遇到数字，我们就直接压入操作数栈；当遇到运算符，就与运算符栈的栈顶元素进行比较。如果比运算符栈顶元素的优先级高，就将当前运算符压入栈；如果比运算符栈顶元素的优先级低或者相同，从运算符栈中取栈顶运算符，从操作数栈的栈顶取 2 个操作数，然后进行计算，再把计算完的结果压入操作数栈，继续比较。

![](images/SJJG+SFZM-08-05.jpg)

### 栈在括号匹配中的应用

借助栈来检查表达式中的括号是否匹配。

用栈来保存未匹配的左括号，从左到右依次扫描字符串。当扫描到左括号时，则将其压入栈中；当扫描到右括号时，从栈顶取出一个左括号。如果能够匹配，比如“(”跟“)”匹配，“[”跟“]”匹配，“{”跟“}”匹配，则继续扫描剩下的字符串。如果扫描的过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为非法格式。当所有的括号都扫描完成之后，如果栈为空，则说明字符串为合法格式；否则，说明有未匹配的左括号，为非法格式。



### 思考题

> 1 如何实现浏览器的前进、后退功能？

使用两个栈，X 和 Y，把首次浏览的页面依次压入栈 X，当点击后退按钮时，再依次从栈 X 中出栈，并将出栈的数据依次放入栈 Y。当我们点击前进按钮时，我们依次从栈 Y 中取出数据，放入栈 X 中。当栈 X 中没有数据时，那就说明没有页面可以继续后退浏览了。当栈 Y 中没有数据，那就说明没有页面可以点击前进按钮浏览了。

![](images/SJJG+SFZM-08-06.jpg)

![](images/SJJG+SFZM-08-07.jpg)

![](images/SJJG+SFZM-08-08.jpg)

这个时候，你通过页面 b 又跳转到新的页面 d 了，页面 c 就无法再通过前进、后退按钮重复查看了，所以需要清空栈 Y。此时两个栈的数据这个样子：

![](images/SJJG+SFZM-08-09.jpg)

> 2 为什么函数调用要用“栈”来保存临时变量呢？用其他数据结构不行吗？





## 09 队列：队列在线程池等有限资源池中的应用

当我们向固定大小的线程池中请求一个线程时，如果线程池中没有空闲资源了，这个时候线程池如何处理这个请求？是拒绝请求还是排队请求？各种处理策略又是怎么实现的呢？

### 如何理解“队列”？

先进者先出，这就是典型的==“队列”（queue）==。

![](images/SJJG+SFZM-09-01.jpg)

栈只支持两个基本操作：**入栈 push()，出栈 pop()**。

队列最基本的两个操作：**入队 enqueue()，出队 dequeue()**。

队列跟栈一样，也是一种==操作受限的线性表数据结构==。

作为一种非常基础的数据结构，队列的应用也非常广泛，特别是一些具有某些额外特性的队列，比如循环队列、阻塞队列、并发队列。它们在很多偏底层系统、框架、中间件的开发中，起着关键性的作用。比如[高性能队列 Disruptor](https://github.com/LMAX-Exchange/disruptor)、Linux 环形缓存，都用到了循环并发队列；Java concurrent 并发包利用 `ArrayBlockingQueue` 来实现公平锁等。

### 顺序队列和链式队列

跟栈一样，队列可以用数组来实现（==顺序队列==），也可以用链表来实现（==链式队列==）。

栈来只需要一个==栈顶指针==。但是队列需要两个：一个是 ==head 指针==，指向队头；一个是 ==tail 指针==，指向队尾。

```java
public class ArrayQueue {

    private String[] items;
    /**
     * 数组大小
     */
    private int n = 0;
    /**
     * 队头下标
     */
    private int head = 0;
    /**
     * 队尾下标
     */
    private int tail = 0;

    /**
     * 申请一个大小为capacity的数组
     */
    public ArrayQueue(int capacity) {
        items = new String[capacity];
        n = capacity;
    }
    /**
     * 入队
     */
    public boolean enqueue(String item) {
        if (tail == n) {
            return false;
        }
        items[tail] = item;
        ++tail;
        return true;
    }
    /**
     * 出队
     */
    public String  dequeue() {
        if (head == tail) {
            return null;
        }
        String ret = items[head];
        ++head;
        return ret;
    }
}
```

当 a、b、c、d 依次入队之后，队列中的 head 指针指向下标为 0 的位置，tail 指针指向下标为 4 的位置：

![](images/SJJG+SFZM-09-02.jpg)

当调用两次出队操作之后，队列中 head 指针指向下标为 2 的位置，tail 指针仍然指向下标为 4 的位置：

![](images/SJJG+SFZM-09-03.jpg)

为了避免每一次出队、入队操作都进行**数据搬移**，在入队时进行判断，如果队尾没有空间了，再触发一次数据搬移，改进入队操作：

```java
    public boolean enqueue(String item) {
        // tail为n时表示队尾没有空间了
        if (tail == n) {
            // head为0同时tail为n，表示队列满了
            if (head == 0 ){
                return false;
            }
            // 队前有空间，进行数据搬移
            for (int i = head; i < tail; ++i) {
                items[i - head] = items[i];
            }
            // 搬移完之后，更新head和tail
            tail -= head;
            head = 0;
        }
        items[tail] = item;
        ++tail;
        return true;
    }
```

![](images/094ba7722eeec46ead58b40c097353c7.jpg)



基于链表的实现：入队时，`tail->next = new_node`, `tail = tail->next`；出队时，`head = head->next`。

![](images/SJJG+SFZM-09-04.jpg)

### 循环队列🔖🔖

![](images/SJJG+SFZM-09-06.jpg)

![](images/SJJG+SFZM-09-07.jpg)

最关键的是，==确定好队空和队满的判定条件==。

### 阻塞队列和并发队列

==阻塞队列==就是在队列为空的时候，从队头取数据会被阻塞。因为此时还没有数据可取，直到队列中有了数据才能返回；如果队列已经满了，那么插入数据的操作就会被阻塞，直到队列中有空闲位置后再插入数据，然后再返回。

![](images/SJJG+SFZM-09-08.jpg)

==生产者-消费者模型==

![](images/SJJG+SFZM-09-09.jpg)

线程安全的队列叫作==并发队列==。

最简单直接的实现方式是直接在 enqueue()、dequeue() 方法上加锁，但是锁粒度大并发度会比较低，同一时刻仅允许一个存或者取操作。实际上，基于数组的循环队列，利用 CAS 原子操作，可以实现非常高效的并发队列。这也是循环队列比链式队列应用更加广泛的原因。

### 思考题

> 线程池没有空闲线程时，新的任务请求线程资源时，线程池该如何处理？各种处理策略又是如何实现的呢？

## 10 递归：如何用三行代码找到“最终推荐人”？

> 推荐注册返佣金：用户 A 推荐用户 B 来注册，用户 B 又推荐了用户 C 来注册。我们可以说，用户 C 的“最终推荐人”为用户 A，用户 B 的“最终推荐人”也为用户 A，而用户 A 没有“最终推荐人”。
>
> <u>给定一个用户 ID，如何查找这个用户的“最终推荐人”？</u>

### 如何理解“递归”？

==递归（Recursion）==是一种应用非常广泛的算法（或者编程技巧）。很多数据结构和算法的编码实现都要用到递归，比如 <u>DFS 深度优先搜索、前中后序二叉树遍历</u>等等。

去的过程叫“递”，回来的过程叫“归”。

递推公式：

```
f(n)=f(n-1)+1 	其中，f(1)=1
```



### 递归需要满足的三个条件

#### 1 一个问题的解可以分解为几个子问题的解

#### 2 这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样

#### 3 存在递归终止条件



### 如何编写递归代码？

写递归代码最关键的是**==写出递推公式，找到终止条件==**。



> 写递归代码的关键就是找到如何将大问题分解为小问题的规律，并且基于此写出递推公式，然后再推敲终止条件，最后将递推公式和终止条件翻译成代码。

计算机擅长做重复的事情，所以递归正合它的胃口。而我们人脑更喜欢==平铺直叙的思维方式==。当我们看到递归时，我们总想把递归平铺展开，脑子里就会循环，一层一层往下调，然后再一层一层返回，试图想搞清楚计算机每一步都是怎么执行的，这样就很容易被绕进去。

**如果一个问题 A 可以分解为若干子问题 B、C、D，你可以假设子问题 B、C、D 已经解决，在此基础上思考如何解决问题 A。**而且，你只需要思考问题 A 与子问题 B、C、D 两层之间的关系即可，不需要一层一层往下思考子问题与子子问题，子子问题与子子子问题之间的关系。屏蔽掉递归细节，这样子理解起来就简单多了。

### 递归代码要警惕堆栈溢出

通过在代码中限制递归调用的最大深度的方式来解决

### 递归代码要警惕重复计算



`f(n) = f(n-1)+f(n-2)`：

![](images/SJJG+SFZM-10-02.jpg)

想要计算 f(5)，需要先计算 f(4) 和 f(3)，而计算 f(4) 还需要计算 f(3)，因此，f(3) 就被计算了很多次，这就是重复计算问题。

可以通过一个数据结构（比如散列表）来保存已经求解过的 f(k)。当递归调用到 f(k) 时，先看下是否已经求解过了。如果是，则直接从散列表中取值返回，不需要重复计算，这样就能避免刚讲的问题了。

### 怎么将递归代码改写为非递归代码？

递归有利有弊，利是递归代码的==表达力很强，写起来非常简洁==；而弊就是==空间复杂度高、有堆栈溢出的风险、存在重复计算、过多的函数调用会耗时较多==等问题。

### 思考题



## 11 排序（上）：为什么插入排序比冒泡排序更受欢迎？

排序算法有很多，最经典、最常用的又：

冒泡排序、插入排序、选择排序、归并排序、快速排序、计数排序、基数排序、桶排序。

![](images/SJJG+SFZM-11-01.jpg)

### 如何分析一个“排序算法”？

#### 排序算法的执行效率

##### 1 最好情况、最坏情况、平均情况时间复杂度

有序度

##### 2 时间复杂度的系数、常数 、低阶

小规模排序，需要考虑系数、常熟、低阶。

##### 3 比较次数和交换（或移动）次数

基于比较的排序算法的执行过程，会涉及两种操作，==一种是元素比较大小，另一种是元素交换或移动==。

#### 排序算法的内存消耗

==原地排序（Sorted in place）==，就是特指空间复杂度是 O(1) 的排序算法。

#### 排序算法的稳定性

==稳定性==，如果待排序的序列中存在值相等的元素，经过排序之后，相等元素之间原有的先后顺序不变。

> 按照对象的某个 key 来排序，比如：
>
> 要给电商交易系统中的“订单”排序。订单有两个属性，一个是下单时间，另一个是订单金额。如果我们现在有 10 万条订单数据，我们希望按照金额从小到大对订单数据排序。对于金额相同的订单，我们希望按照下单时间从早到晚有序。
>
> **稳定排序算法可以保持金额相同的两个对象，在排序之后的前后顺序不变。**
>
> ![](images/SJJG+SFZM-11-02.jpg)

### 冒泡排序

冒泡排序（Bubble Sort）**只会操作相邻的两个数据**。每次冒泡操作都会对相邻的两个元素进行比较，看是否满足大小关系要求。如果不满足就让它俩互换。一次冒泡会让至少一个元素移动到它应该在的位置，重复 n 次，就完成了 n 个数据的排序工作。

一组数据 4，5，6，3，2，1，从小到大进行排序，第一次冒泡操作过程：

![](images/SJJG+SFZM-11-03.jpg)

6次冒泡：

![](images/SJJG+SFZM-11-04.jpg)

优化：当某次冒泡操作已经没有数据交换时，说明已经达到完全有序，不用再继续执行后续的冒泡操作。

![](images/SJJG+SFZM-11-05.jpg)

```java

// 冒泡排序，a表示数组，n表示数组大小
public void bubbleSort(int[] a, int n) {
  if (n <= 1) return;
 
 for (int i = 0; i < n; ++i) {
    // 提前退出冒泡循环的标志位
    boolean flag = false;
    for (int j = 0; j < n - i - 1; ++j) {
      if (a[j] > a[j+1]) { // 交换
        int tmp = a[j];
        a[j] = a[j+1];
        a[j+1] = tmp;
        flag = true;  // 表示有数据交换      
      }
    }
    if (!flag) break;  // 没有数据交换，提前退出
  }
}
```

#### 第一，冒泡排序是原地排序算法吗？

是，只涉及相邻数据的交换操作，只需要常量级的临时空间

#### 第二，冒泡排序是稳定的排序算法吗？

是，只有交换才可以改变两个元素的前后顺序。为了保证冒泡排序算法的稳定性，当有相邻的两个元素大小相等的时候，我们不做交换，相同大小的数据在排序前后不会改变顺序。

#### 第三，冒泡排序的时间复杂度是多少？

![](images/SJJG+SFZM-11-06.jpg)

🔖

### 插入排序（Insertion Sort）

![](images/SJJG+SFZM-11-09.jpg)

**动态排序的过程**，即动态地往有序集合中添加数据，可以通过这种方法保持集合中的数据一直有序。

首先，我们将数组中的数据分为两个区间，==已排序区间==和==未排序区间==。初始已排序区间只有一个元素，就是数组的第一个元素。插入算法的核心思想是==取未排序区间中的元素，在已排序区间中找到合适的插入位置将其插入，并保证已排序区间数据一直有序==。重复这个过程，直到未排序区间中元素为空，算法结束。

![](images/SJJG+SFZM-11-10.jpg)

插入排序也包含两种操作，一种是**元素的比较**，一种是**元素的移动**。

🔖

### 选择排序（Selection Sort）

类似插入排序，也分已排序区间和未排序区间。但是选择排序每次会从未排序区间中找到最小的元素，将其放到已排序区间的末尾(未排序区间的第一个元素)。

> 插入排序，是找到插入点之后，原本插入点及其之后的元素后移； 选择排序，是找到插入点之后，待插入的最小元素与原本插入点位置的元素做交换。

![](images/SJJG+SFZM-11-12.jpg)

选择排序空间复杂度为 O(1)，是一种原地排序算法，最好情况时间复杂度、最坏情况和平均情况时间复杂度都为 O(n^2^)。

## 12 排序（下）：如何用快排思想在O(n)内查找第K大元素？

冒泡排序、插入排序、选择排序时间复杂度都为 O(n^2^)，适合小规模数据排序。

归并排序和快速排序时间复杂度为 O(nlogn)，适合大规模的数据排序，它们都用到了==分治思想==。

### 归并排序的原理

归并排序（Merge Sort）

如果要排序一个数组，先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组就都有序了。

![](images/SJJG+SFZM-12-01.jpg)

分治思想跟我们前面讲的递归思想很像。是的，分治算法一般都是用递归来实现的。==分治是一种解决问题的处理思想，递归是一种编程技巧==。



```

递推公式：
merge_sort(p…r) = merge(merge_sort(p…q), merge_sort(q+1…r))

终止条件：
p >= r 不用再继续分解
```

### 归并排序的性能分析



### 快速排序的原理

快速排序算法（Quicksort，快排）

快排的思想：

如果要排序数组中下标从 p 到 r 之间的一组数据，我们选择 p 到 r 之间的任意一个数据作为 pivot（分区点）。遍历 p 到 r 之间的数据，将小于 pivot 的放到左边，将大于 pivot 的放到右边，将 pivot 放到中间。经过这一步骤之后，数组 p 到 r 之间的数据就被分成了三个部分，前面 p 到 q-1 之间都是小于 pivot 的，中间是 pivot，后面的 q+1 到 r 之间是大于 pivot 的。

![](images/SJJG+SFZM-12-03.jpg)

根据分治、递归的处理思想，可以用递归排序下标从 p 到 q-1 之间的数据和下标从 q+1 到 r 之间的数据，直到区间缩小为 1，就说明所有的数据都有序了。

```

递推公式：
quick_sort(p…r) = quick_sort(p…q-1) + quick_sort(q+1… r)

终止条件：
p >= r
```

### 快速排序的性能分析

快排是一种原地、不稳定的排序算法。

## 13 线性排序：如何根据年龄给100万用户数据排序？

桶排序、计数排序、基数排序的时间复杂度是线性的（即时间复杂度是 O(n)），把这类排序算法叫作**线性排序（Linear sort）**。它们都是非基于比较的排序算法，都不涉及元素之间的比较操作。

### 桶排序（Bucket sort）

核心思想是将要排序的数据分到几个有序的桶里，每个桶里的数据再单独进行排序。桶内排完序之后，再把每个桶里的数据按照顺序依次取出，组成的序列就是有序的了。

![](images/SJJG+SFZM-13-01.jpg)

如果要排序的数据有 n 个，我们把它们均匀地划分到 m 个桶内，每个桶里就有 k=n/m 个元素。每个桶内部使用快速排序，时间复杂度为 O(k * logk)。m 个桶排序的时间复杂度就是 O(m * k * logk)，因为 k=n/m，所以整个桶排序的时间复杂度就是 O(n*log(n/m))。当桶的个数 m 接近数据个数 n 时，log(n/m) 就是一个非常小的常量，这个时候桶排序的时间复杂度接近 O(n)。

**桶排序对要排序数据的要求是非常苛刻的。**



桶排序比较适合用在外部排序中。所谓的==外部排序==就是数据存储在外部磁盘中，数据量比较大，内存有限，无法将数据全部加载到内存中。

🔖

### 计数排序（Counting sort）

计数排序其实是桶排序的一种特殊情况。

当要排序的 n 个数据，所处的范围并不大的时候，比如最大值是 k，我们就可以把数据划分成 k 个桶。每个桶内的数据值都是相同的，省掉了桶内排序的时间。



计数排序只能用在数据范围不大的场景中，如果数据范围 k 比要排序的数据 n 大很多，就不适合用计数排序了。而且，计数排序只能给非负整数排序，如果要排序的数据是其他类型的，要将其在不改变相对大小的情况下，转化为非负整数。



### 基数排序（Radix sort）



![](images/SJJG+SFZM-13-06.jpg)



基数排序对要排序的数据是有要求的，需要可以分割出独立的“位”来比较，而且位之间有递进的关系，如果 a 数据的高位比 b 数据大，那剩下的低位就不用比较了。除此之外，每一位的数据范围不能太大，要可以用线性排序算法来排序，否则，基数排序的时间复杂度就无法做到 O(n) 了。

## 14 排序优化：如何实现一个通用的、高性能的排序函数？

几乎所有的编程语言都会提供排序函数，比如 C 语言中 `qsort()`，C++ STL 中的 `sort()`、`stable_sort()`，还有 Java 语言中的 `Collections.sort()`。

![](images/SJJG+SFZM-14-01.jpg)

Java 语言采用堆排序实现排序函数，C 语言使用快速排序实现排序函数。

### 如何优化快速排序？

这种 O(n2) 时间复杂度出现的主要原因还是因为我们==分区点选得不够合理==。

最理想的分区点是：**被分区点分开的两个分区中，数据的数量差不多。**

#### 1.三数取中法

#### 2.随机法

## 15 二分查找（上）：如何用最省内存的方式实现快速查找功能？

二分查找（Binary Search）算法，也叫折半查找算法。（针对有序数据集合）

### 无处不在的二分思想

> 我随机写一个 0 到 99 之间的数字，然后你来猜我写的是什么。猜的过程中，你每猜一次，我就会告诉你猜的大了还是小了，直到猜中为止。你来想想，如何快速猜中我写的数字呢？（<u>如果猜测范围的数字有偶数个，中间数有两个，就选择较小的那个。</u>）

假设写的数字的23，过程大概为：

![](./images/SJJG+SFZM-15-01.jpg)

7次猜出来了。

> 实际的开发场景中。假设有 1000 条订单数据，已经按照订单金额从小到大排序，每个订单金额都不同，并且最小单位是元。我们现在想知道是否存在金额等于 19 元的订单。如果存在，则返回订单数据，如果不存在则返回 null。
>
> 简化方便说明。假设有 10 个订单，订单金额分别是：8，11，19，23，27，33，45，55，67，98（排过序），查找是否有19的订单。

low 和 high 表示待查找区间的下标，mid 表示待查找区间的中间元素下标。

![](./images/SJJG+SFZM-15-02.jpg)

二分查找针对的是一个==有序==的数据集合，查找思想有点类似分治思想。**每次都通过跟区间的中间元素对比，将待查找的区间缩小为之前的一半，直到找到要查找的元素，或者区间被缩小为 0**。

### O(logn) 惊人的查找速度

==对数时间复杂度==



### 二分查找的递归与非递归实现

==最简单==的情况就是有序数组中不存在==重复==元素。

```java
// 循环实现
public int bsearch(int[] a, int n, int value) {
  int low = 0;
  int high = n - 1;

  while (low <= high) {
//  	int mid = (low + high) / 2;
    int mid = low + ((high - low) >> 1); // 改进
    if (a[mid] == value) {
      return mid;
    } else if (a[mid] < value) {
      low = mid + 1;
    } else {
      high = mid - 1;
    }
  }
  return -1;
}
```

容易出错的地方：

1. 循环退出条件，是 low<=high，而不是 low<high

2. mid 的取值

   为了防止溢出和提升效率，改进为：`low + ((high - low) >> 1)`

3. low 和 high 的更新

```java
// 二分查找的递归实现
public int bsearch2(int a[], int n, int value) {
  return bsearchInternally(a, 0, n - 1, value);
}

private int bsearchInternally(int[] a, int low, int high, int value) {
  if (low > high) {
    return -1;
  }
  int mid = low + ((high - low) >> 1);
  if (a[mid] == value) {
    return mid;
  } else if (a[mid] < value) {
    return bsearchInternally(a, a[mid + 1], high, value);
  } else {
    return bsearchInternally(a, low, a[mid - 1], value);
  }
```



### 二分查找应用场景的局限性

- 首先，二分查找依赖的是==顺序==表结构，简单点说就是数组。(需要按照下标随机访问元素)

- 其次，二分查找针对的是==有序==数据。

  二分查找只能用在插入、删除操作不频繁（静态的数据），一次排序多次查找的场景中。针对动态变化的数据集合，二分查找将不再适用（动态的是使用二叉树）。

- 再次，数据量太小不适合二分查找。

  > 例外：如果数据之间的比较操作非常耗时，不管数据量大小，我都推荐使用二分查找。比如，数组中存储的都是长度超过 300 的字符串，如此长的两个字符串之间比对大小，就会非常耗时。我们需要尽可能地减少比较次数，而比较次数的减少会大大提高性能，这个时候二分查找就比顺序遍历更有优势。

- 最后，数据量太大也不适合二分查找。

  数组为了支持随机访问的特性，要求内存空间连续，对内存的要求比较苛刻。

### 思考题

> 假设我们有 1000 万个整数数据，每个数据占 8 个字节，如何设计数据结构和算法，快速判断某个整数是否出现在这 1000 万数据中？ 我们希望这个功能不要占用太多的内存空间，最多不要超过 100MB。

虽然大部分情况下，用二分查找可以解决的问题，用散列表、二叉树都可以解决。散列表和二叉树，都需要比较多的额外的内存空间，而二分查找底层依赖的是数组，除了数据本身之外，不需要额外存储其他信息，是最省内存空间的存储方式，所以刚好能在限定的内存大小下解决这个问题。

🔖

## 16 二分查找（下）：如何快速定位IP对应的省份地址？🔖

二分查找虽然原理极其简单，但是想要写出没有 Bug 的二分查找并不容易。

> 唐纳德·克努特（Donald E.Knuth）在《计算机程序设计艺术》的第 3 卷《排序和查找》中说到：<u>“尽管第一个二分查找算法于 1946 年出现，然而第一个完全正确的二分查找算法实现直到 1962 年才出现。”</u>

4中常见的二分查找变形问题：

### 查找第一个值等于给定值的元素



### 查找最后一个值等于给定值的元素



### 查找第一个大于等于给定值的元素



### 查找最后一个小于等于给定值的元素





## 17 跳表：为什么Redis一定要用跳表来实现有序集合？

二分查找底层依赖的是数组随机访问的特性，所以只能用数组来实现。

如果数据存储在链表中，对链表稍加改造，就可以支持类似“二分”的查找算法。这个改造之后的数据结构叫做**跳表（Skip list）**。

跳表是一种各方面性能都比较优秀的**==动态数据结构==**，可以支持快速地插入、删除、查找操作，写起来也不复杂，甚至可以替代红黑树（Red-black tree）。

### 如何理解“跳表”？

单链表，即便数据是有序的，查找时也是从头遍历链表，O(n)。

![](./images/SJJG+SFZM-17-01.jpg)

为了提高效率。像下图，对链表建立一级“索引”，每两个结点提取一个结点到上一级，down 表示 down 指针，指向下一级结点。

![](./images/SJJG+SFZM-17-02.jpg)

加来一层索引之后，查找一个结点需要遍历的结点个数减少了，也就是说查找效率提高了。

如果再建立一层索引，需要遍历的结点数会更少。

![](./images/SJJG+SFZM-17-03.jpg)

链表的长度越大，在构建索引之后，查找效率的提升就会非常明显。

![](./images/SJJG+SFZM-17-04.jpg)

**这种链表加多级索引的结构，就是跳表。**

### 用跳表查询到底有多快？



![](images/SJJG+SFZM-17-05.jpg)

### 跳表是不是很浪费内存？

![](images/SJJG+SFZM-17-06.jpg)

实际上，在软件开发中，我们不必太在意索引占用的额外空间。在讲数据结构和算法时，我们习惯性地把要处理的数据看成整数，但是在实际的软件开发中，原始链表中存储的有可能是很大的对象，而索引结点只需要存储关键值和几个指针，并不需要存储对象，所以当对象比索引结点大很多时，那索引占用的额外空间就可以忽略了。

### 高效的动态插入和删除

跳表不仅支持查找操作，还支持动态的插入、删除操作，而且插入、删除操作的时间复杂度也是 O(logn)。

![](images/SJJG+SFZM-17-09.jpg)

### 跳表索引动态更新

当我们不停地往跳表中插入数据时，如果我们不更新索引，就有可能出现某 2 个索引结点之间数据非常多的情况。极端情况下，跳表还会退化成单链表。



跳表是通过==随机函数==来维护前面提到的“平衡性”。

![](images/SJJG+SFZM-17-11.jpg)

### 思考

> 为什么 Redis 要用跳表来实现有序集合，而不是红黑树？



## 18 散列表（上）：Word文档中的单词拼写检查功能是如何实现的？

==散列表（Hash Table）==，“哈希表”或者“Hash 表”。

### 散列思想

散列表用的是数组支持按照下标随机访问数据的特性，所以散列表其实就是==数组的一种扩展，由数组演化而来==。可以说，如果没有数组，就没有散列表。

![](./images/SJJG+SFZM-18-01.jpg)

参赛选手的编号我们叫做**键（key）**或关键字。

参赛编号转化为数组下标的映射方法就叫作**==散列函数==**（或“Hash函数”“==哈希函数==”）。

散列函数计算得到的值就叫作**==散列值==**（或“Hash 值”“哈希值”）。

散列表用的就是数组支持按照下标随机访问的时候，时间复杂度是 O(1) 的特性。我们==通过散列函数把元素的键值映射为下标==，然后将数据存储在数组中对应下标的位置。当我们按照键值查询元素时，我们用同样的散列函数，将键值转化数组下标，从对应的数组下标的位置取数据。

### 散列函数

三点散列函数设计的基本要求：

1. 散列函数计算得到的散列值是一个非负整数；
2. 如果 key1 = key2，那 hash(key1) == hash(key2)；
3. 如果 key1 ≠ key2，那 hash(key1) ≠ hash(key2)。

真实的情况下，要想找到一个不同的 key 对应的散列值都不一样的散列函数，几乎是不可能的。即便像业界著名的MD5、SHA、CRC等哈希算法，也**无法完全避免**这种**==散列冲突==**。而且，因为数组的存储空间有限，也会加大散列冲突的概率。

### 散列冲突

常用的散列冲突解决方法有两类：

#### 1 开放寻址法（open addressing）

开放寻址法的核心思想是，**如果出现了散列冲突，我们就重新探测一个空闲位置，将其插入**。

重新探测新位置的方法：

##### **线性探测（Linear Probing）**

- 插入

如果某个数据经过散列函数散列之后，存储位置已经被占用了，我们就从当前位置开始，依次往后查找，看是否有空闲位置，直到找到为止。

![黄色表示空闲](./images/SJJG+SFZM-18-02.jpg)

- 查找

有点类似插入过程。

通过散列函数求出要查找元素的键值对应的散列值；

然后比较数组中下标为散列值的元素和要查找的元素；

如果相等，则说明就是我们要找的元素，否则就顺序往后依次查找；

如果遍历到数组中的空闲位置，还没有找到，就说明要查找的元素并没有在散列表中。

![](./images/SJJG+SFZM-18-03.jpg)

- 删除

不能单纯地把要删除的元素设置为空。因为影响查找操作。

将删除的元素，特殊标记为 **deleted**。当线性探测查找的时候，遇到标记为 deleted 的空间，并不是停下来，而是继续往下探测。

![](./images/SJJG+SFZM-18-04.jpg)

> 线性探测法的问题：
>
> 当散列表中插入的数据越来越多时，散列冲突发生的可能性就会越来越大，空闲位置会越来越少，线性探测的时间就会越来越久。极端情况下，我们可能需要探测整个散列表，所以最坏情况下的时间复杂度为 O(n)。
>
> 查找和删除操作情况类似。

##### 二次探测（Quadratic probing）

跟线性探测很像，线性探测每次探测的步长是 1，二次探测的下标序列就是hash(key)+0，hash(key)+1^2^，hash(key)+2^2^，hash(key)+3^2^……（步长变为原来的平方）。

##### 双重散列（Double hashing）

不仅要使用一个散列函数，而是使用一组散列函数 hash1(key)，hash2(key)，hash3(key)……

先用第一个散列函数，如果计算得到的存储位置已经被占用，再用第二个散列函数，依次类推，直到找到空闲的存储位置。



不管采用哪种探测方法，当散列表中空闲位置不多的时候，散列冲突的概率就会大大提高。为了尽可能保证散列表的操作效率，一般，会尽可能保证散列表中有一定比例的空闲槽位。用**==装载因子（load factor）==**来表示空位的多少。

> **==散列表的装载因子 = 填入表中的元素个数 / 散列表的长度==**

装载因子越大，说明空闲位置越少，冲突越多，散列表的性能会下降。

#### 2 链表法（chaining）

链表法是一种更加常用的散列冲突解决办法，相比开放寻址法，它要简单很多。

![](./images/SJJG+SFZM-18-05.jpg)

在散列表中，每个“桶（bucket）”或者“槽（slot）”会对应一条链表，所有散列值相同的元素我们都放到相同槽位对应的链表中。

插入，只需要通过散列函数计算出对应的散列槽位，将其插入到对应链表中即可，所以插入的时间复杂度是 O(1)。

查找、删除，通过散列函数计算出对应的槽，然后遍历链表查找或者删除。它们的时间复杂度跟链表的长度 k 成正比，也就是 O(k)。

对于散列比较均匀的散列函数来说，理论上讲，k=n/m，其中 n 表示散列中数据的个数，m 表示散列表中“槽”的个数。

### 思考题

- Word 文档中单词拼写检查功能是如何实现的？

  常用的英文单词有 20 万个左右，假设单词的平均长度是 10 个字母，平均一个单词占用 10 个字节的内存空间，那 20 万英文单词大约占 2MB 的存储空间，就算放大 10 倍也就是 20MB。对于现在的计算机来说，这个大小完全可以放在内存里面。所以我们可以用散列表来存储整个英文单词词典。

  当用户输入某个英文单词时，我们拿用户输入的单词去散列表中查找。如果查到，则说明拼写正确；如果没有查到，则说明拼写可能有误，给予提示。借助散列表这种数据结构，我们就可以轻松实现快速判断是否存在拼写错误。

- 假设我们有 10 万条 URL 访问日志，如何按照访问次数给 URL 排序？

  遍历 10 万条数据，以 URL 为 key，访问次数为 value，存入散列表，同时记录下访问次数的最大值 K，时间复杂度 O(N)。 

  如果 K 不是很大，可以使用桶排序，时间复杂度 O(N)。如果 K 非常大（比如大于 10 万），就使用快速排序，复杂度 O(NlogN)。

- 有两个字符串数组，每个数组大约有 10 万条字符串，如何快速找出两个数组中相同的字符串？

  以第一个字符串数组构建散列表，key 为字符串，value 为出现次数。再遍历第二个字符串数组，以字符串为 key 在散列表中查找，如果 value 大于零，说明存在相同字符串。时间复杂度 O(N)。



## 19 散列表（中）：如何打造一个工业级水平的散列表？

散列表的查询效率与<u>散列函数、装载因子、散列冲突</u>等都有关系。

在极端情况下，有些恶意的攻击者，还有可能通过精心构造的数据，使得所有的数据经过散列函数之后，都散列到同一个槽里。

**散列表碰撞攻击**

> 如何设计一个可以应对各种异常情况的工业级散列表，来避免在散列冲突的情况下，散列表性能的急剧下降，并且能抵抗散列碰撞攻击？

### 如何设计散列函数？

1. 散列函数的设计不能太复杂。
2. 散列函数生成的值要尽可能**==随机==**并且**==均匀==**分布

实际工作，还要考虑因素有**关键字的长度、特点、分布，散列表的大小**等。

常用的、简单的散列函数的设计方法：

- “数据分析法”。

  学生运动会，通过分析参赛编号的特征，把编号中的后两位作为散列值；

  手机号码前几位重复的可能性很大，后面几位就比较随机，可以取手机号的后四位作为散列值。

- 实现 Word 拼写检查功能：<u>将单词中每个字母的ASCll 码值“进位”相加，然后再跟散列表的大小求余、取模，作为散列值</u>。比如单词nice，转化成散列值：

  ```java
  hash("nice")=(("n" - "a") * 26*26*26 + ("i" - "a")*26*26 + ("c" - "a")*26+ ("e"-"a")) / 78978
  ```

- 其它方法有：<u>直接寻址法、平方取中法、折叠法、随机数法</u>等。

### 装载因子过大了怎么办？

装载因子越大，说明散列表中的元素越多，空闲位置越少，散列冲突的概率就越大。

- 对于没有频繁插入和删除的静态数据集合，容易设计散列函数。

- 对于动态散列表，数据集合是频繁变动的，事先无法预估加入的数据个数。**动态扩容**

  针对数组的扩容，数据搬移操作比较简单。但是，针对散列表的扩容，数据搬移操作要复杂很多。因为散列表的大小变了，数据的存储位置也变了，需要通过散列函数重新计算每个数据的存储位置。

  ![](./images/SJJG+SFZM-19-01.jpg)

一些数据删除后，如果对空间敏感，可通过**动态缩容**。

### 如何避免低效的扩容？

为了解决一次性扩容耗时过多的情况，可以将扩容操作穿插在插入操作的过程中，分批完成。

![](./images/SJJG+SFZM-19-02.jpg)

对于这期间的查询操作，为了兼容了新、老散列表中的数据，我们先从新散列表中查找，如果没有找到，再去老的散列表中查找。

### 如何选择冲突解决方法？

Java 中 LinkedHashMap 就采用了链表法解决冲突，ThreadLocalMap 是通过线性探测的开放寻址法来解决冲突。

|          | 开放寻址法                                                   | 链表法                                                       |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 优点     | 不需要很多链表，数据都存储在数组中，可以有效地利用 CPU 缓存加快查询速度；<br />序列化起来比较简单； | 对内存的利用率更高；<br />链表结点可以在需要的时候再创建，不要事先申请好；<br /><br />装载因子可很大； |
| 缺点     | 删除数据麻烦；<br />数据都存储在一个数组中，冲突代价更高；<br />装载因子的上限不能太大，更浪费内存空间； | 链表中的节点在内存中不是连续的，对CPU缓存不友好；<br />链表需要存储指针，比较小的对象比较消耗内存；（如果对象的代销远远大于一个指针的大小，就可以忽略） |
| 适用场景 | 当数据量比较小、装载因子小的时候，适合采用开放寻址法。       | 基于链表的散列冲突处理方法比较适合存储大对象、大数据量的散列表，而且，比起开放寻址法，它更加灵活，支持更多的优化策略，比如用红黑树代替链表。 |

![对链表改造](./images/SJJG+SFZM-19-03.jpg)

### 工业级散列表举例分析

HashMap

1. 初始大小

   16。可以根据数量大概量，修改初始大小，减少动态扩容的次数。

2. 装载因子和动态扩容

   0.75。每次扩容为原来的两倍。

3. 散列冲突解决方法

   HashMap 底层采用链表法来解决冲突。

   JDK8后，引入红黑树，优化链表过长的情况；当红黑树结点个数少于 8 个的时候，又会将红黑树转化为链表。

4. 散列函数

   追求的是简单高效、分布均匀。

   ```java
       static final int hash(Object key) {
           int h;
           return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
       }
   ```

   hashCode() 返回的是 Java 对象的 hash code。比如 String 类型的对象的 hashCode() 就是：

   ```java
       public int hashCode() {
           int h = hash;
           if (h == 0 && value.length > 0) {
               char val[] = value;
   
               for (int i = 0; i < value.length; i++) {
                   h = 31 * h + val[i];
               }
               hash = h;
           }
           return h;
       }
   ```

   

### 思考题

> 如何设计一个工业级的散列函数？

要求：

- 支持快速地查询、插入、删除操作；
- 内存占用合理，不能浪费过多的内存空间；
- 性能稳定，极端情况下，散列表的性能也不会退化到无法接受的情况。

实现：

- 设计一个合适的散列函数；
- 定义装载因子阈值，并且设计动态扩容策略；
- 选择合适的散列冲突解决方法。

> 在你熟悉的编程语言中，哪些数据类型底层是基于散列表实现的？散列函数是如何设计的？散列冲突是通过哪种方法解决的？是否支持动态扩容呢？



## 20 散列表（下）：为什么散列表和链表经常会一起使用？



### LRU缓存淘汰算法

借助散列表，可以把LRU缓存淘汰算法的时间复杂度降低为 O(1)。

![](images/SJJG+SFZM-20-01.jpg)

### Redis 有序集合

实际上，在有序集合中，每个成员对象有两个重要的属性，key（键值）和 score（分值）。我们不仅会通过 score 来查找数据，还会通过 key 来查找数据。

### Java的LinkedHashMap

LinkedHashMap 是通过**双向链表和散列表**这两种数据结构组合实现的。LinkedHashMap 中的“Linked”实际上是指的是双向链表，并非指用链表法解决散列冲突。

### 思考题

> 为什么散列表和链表会经常一块使用？

散列表这种数据结构虽然支持非常高效的数据插入、删除、查找操作，但是散列表中的数据都是通过散列函数打乱之后无规律存储的。也就说，它无法支持按照某种顺序快速地遍历数据。如果希望按照顺序遍历散列表中的数据，那我们需要将散列表中的数据拷贝到数组中，然后排序，再遍历。

因为散列表是动态数据结构，不停地有数据的插入、删除，所以每当我们希望按顺序遍历散列表中的数据的时候，都需要先排序，那效率势必会很低。为了解决这个问题，我们将散列表和链表（或者跳表）结合在一起使用。



>  上面的几个散列表和链表结合使用的例子里，用的都是双向链表。如果把双向链表改成单链表，还能否正常工作呢？为什么呢？



> 假设猎聘网有 10 万名猎头，每个猎头都可以通过做任务（比如发布职位）来积累积分，然后通过积分来下载简历。假设你是猎聘网的一名工程师，如何在内存中存储这 10 万个猎头 ID 和积分信息，让它能够支持这样几个操作：
>
> - 根据猎头的 ID 快速查找、删除、更新这个猎头的积分信息；
> - 查找积分在某个区间的猎头 ID 列表；
> - 查找按照积分从小到大排名在第 x 位到第 y 位之间的猎头 ID 列表。



> 总结：
>
> 两种数据结构，链表和数组。 
>
> 数组占据**随机访问**的优势，却有需要连续内存的缺点。 
>
> 链表具有**可不连续存储**的优势，但访问查找是线性的。 
>
> 散列表和链表、跳表的混合使用，是为了结合数组和链表的优势，规避它们的不足。
>
>  得出数据结构和算法的重要性排行榜：**==连续空间 > 时间 > 碎片空间==**。

## 21 哈希算法（上）：如何防止数据库中的用户信息被脱库？

哈希算法历史悠久，业界著名的哈希算法也有很多，比如 MD5、SHA 等。

如何用哈希算法解决问题。

### 什么是哈希算法？

> hash，翻译为哈希、散列

**将任意长度的二进制值串映射为固定长度的二进制值串**，这个**映射的规则**就是**==哈希算法==**，得到固定长度的二进制值串就是**哈希值**。

优秀哈希算法设计要点：

- 从哈希值不能**反向**推导出原始数据（所以哈希算法也叫单向哈希算法）；
- 对输入数据非常**敏感**，哪怕原始数据只修改了一个Bit，最后得到的哈希值也大不相同；
- 散列冲突的**概率**要很小，对于不同的原始数据，哈希值相同的概率非常小；
- 哈希算法的执行**效率**要尽量高效，针对较长的文本，也能快速地计算出哈希值。



哈希算法常见七个应用：安全加密、唯一标识、数据校验、散列函数、负载均衡、数据分片、分布式存储。

### 安全加密

 MD5（MD5 Message-Digest Algorithm，MD5 消息摘要算法），128bit二进制串，一般用32个16进制数表示。

SHA（Secure Hash Algorithm，安全散列算法）

DES（Data Encryption Standard，数据加密标准）

AES（Advanced Encryption Standard，高级加密标准）

> 组合数学（离散数学）一个非常基础的理论，鸽巢原理（也叫抽屉原理）：
>
> 如果有 10 个鸽巢，有 11 只鸽子，那肯定有 1 个鸽巢中的鸽子数量多于 1 个，换句话说就是，肯定有 2 只鸽子在 1 个鸽巢内。



### 唯一标识

图库

为每张图片去一个唯一标识（或者说信息摘要）。比如，我们可以从图片的二进制码串开头取 100 个字节，从中间取 100 个字节，从最后再取 100 个字节，然后将这 300 个字节放到一块，通过哈希算法（比如 MD5），得到一个哈希字符串，用它作为图片的唯一标识。



### 数据校验

BT下载，基于 P2P 协议的，文件被分割成很多块。



### 散列函数

相对哈希算法的其他应用，散列函数对于散列算法冲突的要求要低很多。即便出现个别散列冲突，只要不是过于严重，我们都可以通过开放寻址法或者链表法解决。

散列函数用的散列算法一般都比较简单，比较追求效率。



### 思考题

选择相对安全的加密算法，对用户密码进行加密之后再存储。

**字典攻击**：维护一个常用密码的字典表，把字典中的每个密码用哈希算法计算哈希值，然后拿哈希值跟脱库后的密文比对。

针对字典攻击，我们可以引入一个**盐（salt）**，跟用户的密码组合在一起，增加密码的复杂度。

> 区块链底层的实现原理并不复杂。其中，哈希算法就是它的一个非常重要的理论基础。
>
> 你能讲一讲区块链使用的是哪种哈希算法吗？是为了解决什么问题而使用的呢？

区块链是一块块区块组成的，每个区块分为两部分：区块头和区块体。 

区块头保存着 自己区块体 和 上一个区块头 的哈希值。 

因为这种链式关系和哈希值的唯一性，只要区块链上任意一个区块被修改过，后面所有区块保存的哈希值就不对了。 

区块链使用的是 SHA256 哈希算法，计算哈希值非常耗时，如果要篡改一个区块，就必须重新计算该区块后面所有的区块的哈希值，短时间内几乎不可能做到。

## 22 哈希算法（下）：哈希算法在分布式系统中有哪些应用？

### 负载均衡

负载均衡算法有**轮询、随机、加权轮询**等。



我们可以通过哈希算法，对客户端 IP 地址或者会话 ID 计算哈希值，将取得的哈希值与服务器列表的大小进行取模运算，最终得到的值就是应该被路由到的服务器编号。

### 数据分片

#### 1 如何统计“搜索关键词”出现的次数？



#### 2 如何快速判断图片是否在图库中？





### 分布式存储

该如何决定将哪个数据放到哪个机器上呢？

借用数据分片的思想，即通过哈希算法对数据取哈希值，然后对机器个数取模，这个最终值就是应该存储的缓存机器编号。

**==一致性哈希算法==**

[白话解析：一致性哈希算法 consistent hashing](https://www.zsythink.net/archives/1182)

### 思考题

哈希算法其它的应用：网络协议中的 CRC 校验、Git commit id等等。



## 23 二叉树基础（上）：什么样的二叉树适合用数组来存储？

> 二叉树有哪几种存储方式？什么样的二叉树适合用数组来存储？

### 树（Tree）

![](images/SJJG+SFZM-23-01.jpg)

**父节点**

**子节点**

**兄弟节点**

**叶子节点/叶节点**

节点的**==高度==**（Height） = 节点到叶子节点的**最长路径**（边数） 【树的高度 = 根节点的高度】

节点的**==深度==**（Depth） = 根节点到这个节点所经历的**边的个数**

节点的**==层==**（Level） = 节点的深度 + 1

树的高度 = 根节点的高度

![](images/SJJG+SFZM-23-04.jpg)

> “高度”就是从下往上度量；深度”是从上往下度量的；“层数”跟深度类似，不过计数起点是 1。

### 二叉树（Binary Tree）

每个节点**最多**有两个“叉”，分别是**左子节点**和**右子节点**。

![](./images/SJJG+SFZM-23-05.jpg)

编号 2 的二叉树中，叶子节点全都在<u>最底层</u>，除了叶子节点之外，每个节点都有左右两个子节点，这种二叉树就叫做**==满二叉树==**。

编号 3 的二叉树中，叶子节点都在<u>最底下两层</u>，最后一层的叶子节点都**靠左**排列，并且除了最后一层，其他层的节点个数都要达到最大，这种二叉树叫做**==完全二叉树==**。

![](images/18413c6597c2850b75367393b401ad60.jpg)

>  如何表示（或者存储）一棵二叉树？

1. 基于指针或者引用的二叉**==链式存储法==**

每个节点有三个字段，其中一个存储数据，另外两个是指向左右子节点的指针。

> 大部分二叉树代码都是通过这种结构来实现的。

![](./images/SJJG+SFZM-23-07.jpg)



2. 基于数组的**顺序存储法**

把根节点存储在下标 i = 1 的位置，那左子节点存储在下标 2 * i = 2 的位置，右子节点存储在 2 * i + 1 = 3 的位置。以此类推，B 节点的左子节点存储在 2 * i = 2 * 2 = 4 的位置，右子节点存储在 2 * i + 1 = 2 * 2 + 1 = 5 的位置。

![](images/SJJG+SFZM-23-08.jpg)

总结，如果节点 X 存储在数组中下标为 i 的位置，下标为 2 * i 的位置存储的就是左子节点，下标为 2 * i + 1 的位置存储的就是右子节点。反过来，下标为 i/2 的位置存储就是它的父节点。

通过这种方式，只要知道根节点存储的位置（一般情况下，为了方便计算子节点，根节点会存储在下标为 1 的位置），就可以通过下标计算把整棵树串起来。

完全二叉树只会“浪费”下标为0的存储位置，而非完全二叉树，会浪费较多空间：

![](images/SJJG+SFZM-23-09.jpg)

> 堆其实就是一种完全二叉树，最常用的存储方式就是数组。

### 二叉树的遍历

根据节点与它的左右子树节点遍历打印的先后顺序：

- **==前序遍历==**，对于树中的任意节点来说，先打印这个节点，然后再打印它的左子树，最后打印它的右子树。

- **==中序遍历==**，对于树中的任意节点来说，先打印它的左子树，然后再打印它本身，最后打印它的右子树。

- **==后序遍历==**，对于树中的任意节点来说，先打印它的左子树，然后再打印它的右子树，最后打印这个节点本身。

![前、中、后序遍历的顺序图](images/SJJG+SFZM-23-10.jpg)

实际上，**二叉树的前、中、后序遍历就是一个递归的过程**。很容易得出三种遍历的递推公式：

```java

前序遍历的递推公式：
preOrder(r) = print r->preOrder(r->left)->preOrder(r->right)

中序遍历的递推公式：
inOrder(r) = inOrder(r->left)->print r->inOrder(r->right)

后序遍历的递推公式：
postOrder(r) = postOrder(r->left)->postOrder(r->right)->print r
```

代码：

```java
void preOrder(Node* root) {
  if (root == null) return;
  print root // 此处为伪代码，表示打印root节点
  preOrder(root->left);
  preOrder(root->right);
}

void inOrder(Node* root) {
  if (root == null) return;
  inOrder(root->left);
  print root // 此处为伪代码，表示打印root节点
  inOrder(root->right);
}

void postOrder(Node* root) {
  if (root == null) return;
  postOrder(root->left);
  postOrder(root->right);
  print root // 此处为伪代码，表示打印root节点
}
```

从前、中、后序遍历的顺序图看来，每个节点最多会被访问两次，所以遍历操作的时间复杂度，跟节点的个数 n 成正比，也就是说二叉树遍历的时间复杂度是 O(n)。

### 思考🔖

二叉树既可以用链式存储，也可以用数组顺序存储。数组顺序存储的方式比较适合完全二叉树，其他类型的二叉树用数组存储会比较浪费存储空间。

> 给定一组数据，比如 1，3，5，6，9，10。可以构建出多少种不同的二叉树？



## 24 二叉树基础（下）：有了如此高效的散列表，为什么还需要二叉树？

二叉查找树最大的特点就是，支持==动态数据集合的快速插入、删除、查找==操作。

### 二叉查找树（Binary Search Tree）

**==二叉查找树==**是为了实现**快速查找**而生的。

二叉查找树要求，在树中的任意一个节点，==其左子树中的每个节点的值，都要小于这个节点的值，而右子树节点的值都大于这个节点的值==。

#### 1.查找

如果要查找的数据比根节点的值小，那就在左子树中递归查找；如果要查找的数据比根节点的值大，那就在右子树中递归查找。

![](images/SJJG+SFZM-24-02.jpg)

#### 2.插入

如果要插入的数据比节点的数据大，并且节点的右子树为空，就将新数据直接插到右子节点的位置；如果不为空，就再递归遍历右子树，查找插入位置。同理，如果要插入的数据比节点数值小，并且节点的左子树为空，就将新数据插入到左子节点的位置；如果不为空，就再递归遍历左子树，查找插入位置。

![](images/SJJG+SFZM-24-03.jpg)

#### 3.删除

- 第一种情况是，如果要删除的节点没有子节点，我们只需要直接将父节点中，指向要删除节点的指针置为 null。比如图中的删除节点 55。
- 第二种情况是，如果要删除的节点只有一个子节点（只有左子节点或者右子节点），我们只需要更新父节点中，指向要删除节点的指针，让它指向要删除节点的子节点就可以了。比如图中的删除节点 13。
- 第三种情况是，如果要删除的节点有两个子节点，这就比较复杂了。我们需要找到这个节点的右子树中的最小节点，把它替换到要删除的节点上。然后再删除掉这个最小节点，因为最小节点肯定没有左子节点（如果有左子结点，那就不是最小节点了），所以，我们可以应用上面两条规则来删除这个最小节点。比如图中的删除节点 18。

![](images/SJJG+SFZM-24-04.jpg)

#### 4.其它操作

快速地查找最大节点和最小节点、前驱节点和后继节点。

中序遍历二叉查找树，可以输出有序的数据序列，时间复杂度是 O(n)，非常高效。因此，二叉查找树也叫作==二叉排序树==。

### 支持重复数据的二叉查找树

实际开发中，会利用对象的某个字段作为键值（key）来构建二叉查找树。把对象中的其他字段叫作==卫星数据==。

如果存储的两个对象键值相同，这种情况该怎么处理呢？

1. 二叉查找树中每一个节点不仅会存储一个数据，因此我们通过链表和支持动态扩容的数组等数据结构，把值相同的数据都存储在同一个节点上。
2. 第二种方法比较不好理解，不过更加优雅。

### 二叉查找树的时间复杂度分析

🔖

### 思考

> 散列表的插入、删除、查找操作的时间复杂度可以做到常量级的 O(1)，非常高效。
>
> 而二叉查找树在比较平衡的情况下，插入、删除、查找操作时间复杂度才是 O(logn)，相对散列表，好像并没有什么优势，那我们为什么还要用二叉查找树呢？

1. 散列表中的数据是无序存储的，如果要输出有序的数据，需要先进行排序。而对于二叉查找树来说，我们只需要中序遍历，就可以在 O(n) 的时间复杂度内，输出有序的数据序列。
2. 散列表扩容耗时很多，而且当遇到散列冲突时，性能不稳定，尽管二叉查找树的性能不稳定，但是在工程中，我们**最常用的平衡二叉查找树的性能非常稳定，时间复杂度稳定在 O(logn)**。
3. 笼统地来说，尽管散列表的查找等操作的时间复杂度是常量级的，但因为哈希冲突的存在，这个常量不一定比 logn 小，所以实际的查找速度可能不一定比 O(logn) 快。加上哈希函数的耗时，也不一定就比平衡二叉查找树的效率高。
4. 散列表的构造比二叉查找树要复杂，需要考虑的东西很多。比如<u>散列函数的设计、冲突解决办法、扩容、缩容</u>等。平衡二叉查找树只需要考虑**平衡性**这一个问题，而且这个问题的解决方案比较成熟、固定。
5. 为了避免过多的散列冲突，散列表装载因子不能太大，特别是基于开放寻址法解决冲突的散列表，不然会浪费一定的存储空间。

> 如何通过编程，求出一棵给定二叉树的确切高度呢？



## 25 红黑树（上）：为什么工程中都用红黑树这种二叉树？

二叉查找树是最常用的一种二叉树，它支持快速插入、删除、查找操作，各个操作的时间复杂度跟树的高度成正比，理想情况下，时间复杂度是 O(logn)。

二叉查找树在<u>频繁的动态更新过程中</u>，可能会出现树的高度远大于 log~2~n 的情况，从而导致各个操作的效率下降。极端情况下，二叉树会退化为链表，时间复杂度会退化到 O(n)。

### 什么是“平衡二叉查找树”？

**==平衡二叉树==**的严格定义：**二叉树中任意一个节点的左右子树的高度相差不能大于 1**。

![](images/SJJG+SFZM-25-01.jpg)

完全二叉树、满二叉树其实都是平衡二叉树，但是非完全二叉树也有可能是平衡二叉树。

**==平衡二叉查找树==**既是平衡二叉树，又有二叉查找树的特点。最先被发明的平衡二叉查找树是**AVL树**。

> 我们学习数据结构和算法是为了应用到实际的开发中的，没必去死抠定义。

发明平衡二叉查找树这类数据结构的初衷是，**解决普通二叉查找树在频繁的插入、删除等动态更新的情况下，出现时间复杂度退化的问题**。

平衡二叉查找树中“**平衡**”的意思，其实就是让整棵树左右看起来比较“对称”、比较“平衡”，不要出现左子树很高、右子树很矮的情况。这样就能<u>让整棵树的高度相对来说低一些，相应的插入、删除、查找等操作的效率高一些</u>。

### 如何定义一棵“红黑树”？

平衡二叉查找树有很多，比如，<u>AVL树、Splay Tree（伸展树）、Treap（树堆）</u>等，但最出名的是红黑树。

红黑树，“Red-Black Tree”，简称 R-B Tree，是一种不严格的平衡二叉查找树。

- 红黑树中的节点，一类被标记为黑色，一类被标记为红色。
- 根节点是黑色的；
- 每个叶子节点都是黑色的空节点（NIL），也就是说，叶子节点不存储数据；
- 任何相邻的节点都不能同时为红色，也就是说，红色节点是被黑色节点隔开的；
- 每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点；

### 为什么说红黑树是“近似平衡”的？

“平衡”的意思可以等价为性能不退化。“近似平衡”就等价为性能不会退化得太严重。



### 思考

> 为什么工程中都喜欢用红黑树，而不是其他平衡二叉查找树呢？

Treap、Splay Tree，绝大部分情况下，它们操作的效率都很高，但是也无法避免极端情况下时间复杂度的退化。尽管这种情况出现的概率不大，但是对于单次操作时间非常敏感的场景来说，它们并不适用。

AVL 树是一种高度平衡的二叉树，所以查找的效率非常高，但是，有利就有弊，AVL 树为了维持这种高度的平衡，就要付出更多的代价。每次插入、删除都要做调整，就比较复杂、耗时。所以，对于有频繁的插入、删除操作的数据集合，使用 AVL 树的代价就有点高了。

红黑树只是做到了近似平衡，并不是严格的平衡，所以在维护平衡的成本上，要比 AVL 树要低。

> 动态数据结构支持动态的数据插入、删除、查找操作，除了红黑树，我们前面还学习过哪些呢？能对比一下各自的优势、劣势，以及应用场景吗？

散列表：插入删除查找都是O(1), 是最常用的，但其缺点是不能顺序遍历以及扩容缩容的性能损耗。适用于那些不需要顺序遍历，数据更新不那么频繁的。 

跳表：插入删除查找都是O(logn), 并且能顺序遍历。缺点是空间复杂度O(n)。适用于不那么在意内存空间的，其顺序遍历和区间查找非常方便。 

红黑树：插入删除查找都是O(logn), 中序遍历即是顺序遍历，稳定。缺点是难以实现，去查找不方便。其实跳表更佳，但红黑树已经用于很多地方了。



## 26 红黑树（下）：掌握这些技巧，你也可以实现一个红黑树

红黑树好的是因为它==稳定、高效==的性能，坏的是实现起来实在太==难==了。

### 实现红黑树的基本思想

魔方的复原解法是有固定算法的：遇到哪几面是什么样子，对应就怎么转几下。

红黑树的平衡过程跟魔方复原非常神似，大致过程就是：==遇到什么样的节点排布，我们就对应怎么去调整。==

左旋（rotate left）、右旋（rotate right）

![](images/SJJG+SFZM-26-01.jpg)

### 插入操作的平衡调整



### 删除操作的平衡调整



## 27 递归树：如何借助树来求解递归算法的时间复杂度？

借助递归树来分析递归算法的时间复杂度。

### 递归树与时间复杂度分析

递归的思想：**将大问题分解为小问题来求解，然后再将小问题分解为小小问题。**

如果我们把这个一层一层的分解过程画成图，它其实就是一棵树，叫作**==递归树==**。

![斐波那契数列的递归树](./images/SJJG+SFZM-27-01.jpg)

归并排序算法

![](./images/SJJG+SFZM-27-02.jpg)

归并算法中分解代价很低，比较耗时的是==归并==操作，也就是把两个子数组合并为大数组。

用高度 h 乘以每一层的时间消耗 n，就得到总的时间复杂度 O(n*h)。

归并排序递归树是一棵满二叉树，满二叉树的高度大约是 log~2~n，所以，归并排序递归实现的时间复杂度就是 O(nlogn)。

### 实战一：分析快速排序的时间复杂度



![](./images/SJJG+SFZM-27-03.jpg)



![](./images/SJJG+SFZM-27-04.jpg)



### 实战二：分析斐波那契数列的时间复杂度

![](images/SJJG+SFZM-27-05.jpg)

### 实战三：分析全排列的时间复杂度



### 思考题

> 1 个细胞的生命周期是 3 小时，1 小时分裂一次。求 n 小时后，容器内有多少细胞？请你用已经学过的递归时间复杂度的分析方法，分析一下这个递归问题的时间复杂度。

## 28 堆和堆排序：为什么说堆排序没有快速排序快？

堆排序是一种原地的、时间复杂度为 O(nlogn) 的排序算法。堆的应用场景非常多。

### 如何理解“堆”？

堆是一种特殊的树。

- 堆是一个完全二叉树；
- 堆中每一个节点的值都必须大于等于（或小于等于）其子树中每个节点的值。

“**==大顶堆==**”：每个节点的值都大于等于子树中每个节点值的堆。

“**==小顶堆==**”：每个节点的值都小于等于子树中每个节点值的堆。

![](images/SJJG+SFZM-28-01.jpg)

其中第 1 个和第 2 个是大顶堆，第 3 个是小顶堆，第 4 个不是堆。

### 如何实现一个堆？

> 堆都支持哪些操作？
>
> 如何存储一个堆？

完全二叉树比较适合用数组来存储。用数组来存储完全二叉树是非常节省存储空间的。

![](./images/SJJG+SFZM-28-02.jpg)

数组中下标为 i 的节点的左子节点，就是下标为 `i*2` 的节点，右子节点就是下标为 `i*2+1` 的节点，父节点就是下标为 `i/2` 的节点。

#### 1 往堆中插入一个元素

堆化（heapify）

![](./images/SJJG+SFZM-28-03.jpg)



![](./images/SJJG+SFZM-28-04.jpg)

#### 2 删除堆顶元素

![](./images/SJJG+SFZM-28-05.jpg)



![](./images/SJJG+SFZM-28-06.jpg)



### 如何基于堆实现排序？

借助于堆实现的排序算法，就叫做==堆排序==，它时间复杂度非常稳定，是 O(nlogn)，并且它还是原地排序算法。

#### 1 建堆

![](images/image-20230321193045098.png)

#### 2 排序

![](images/SJJG+SFZM-28-12.jpg)

### 思考题

> 两种排序算法的时间复杂度都是 O(nlogn)，甚至堆排序比快速排序的时间复杂度还要稳定，但是，在实际的软件开发中，快速排序的性能要比堆排序好，这是为什么呢？

1. 堆排序数据访问的方式没有快速排序友好。
2. 对于同样的数据，在排序过程中，堆排序算法的数据交换次数要多于快速排序。



> 在讲堆排序建堆的时候，我说到，对于完全二叉树来说，下标从 n/2+1 到 n 的都是叶子节点，这个结论是怎么推导出来的呢？



> 上面是堆的一种经典应用，堆排序。关于堆，你还能想到它的其他应用吗？

## 29 堆的应用：如何快速获取到Top 10最热门的搜索关键词？



### 堆的应用一：优先级队列



优先级队列的应用场景非常多。很多数据结构和算法都要依赖它，比如，<u>赫夫曼编码、图的最短路径、最小生成树算法</u>等等。

很多语言中，都提供了优先级队列的实现，比如，Java 的 PriorityQueue，C++ 的 priority_queue 等。

#### 1 合并有序小文件



#### 2 高性能定时器



### 堆的应用二：利用堆求 Top K



### 堆的应用三：利用堆求中位数



### 思考题

> 有一个包含 10 亿个搜索关键词的日志文件，如何快速获取到 Top 10 最热门的搜索关键词呢？



> 有一个访问量非常大的新闻网站，我们希望将点击量排名 Top 10 的新闻摘要，滚动显示在网站首页 banner 上，并且每隔 1 小时更新一次。如果你是负责开发这个功能的工程师，你会如何来实现呢？



## 30 图的表示：如何存储微博、微信等社交网络中的好友关系？

涉及图的算法：**图的搜索、最短路径、最小生成树、二分图**等等。

### 如何理解“图”？

图（Graph） ，比树更加复杂的非线性表结构。

==顶点（vertex）== 

==边（edge）==，图中的一个顶点可以与任意其他顶点建立连接关系。

![](images/SJJG+SFZM-30-01.jpg)

整个微信的好友关系就可以用一张图来表示，其中，每个用户有多少个好友，对应到图中，就叫做顶点的**==度（degree）==**，就是跟顶点相连接的边的条数。

微博的社交关系比微信更复杂一点，微博允许**单向关注**。

==“有向图”==  ==“无向图”==

![](images/SJJG+SFZM-30-02.jpg)



在有向图中，度分为**==入度==（In-degree）**和**==出度==（Out-degree）**。

> 对应微博，入度就表示有多少粉丝，出度就表示关注了多少人。

QQ的社交关系比微博更复杂一点，好有关系有**亲密度**。

**==带权图==（weighted graph）**，每条边都有一个权重（weight）（可以通过这个权重来表示 QQ 好友间的亲密度）。

![](images/SJJG+SFZM-30-03.jpg)



#### 邻接矩阵存储方法

图最直观的一种存储方法：**==邻接矩阵==**（Adjacency Matrix）。

邻接矩阵的底层依赖一个**二维数组**。对于无向图来说，如果顶点 i 与顶点 j 之间有边，我们就将 `A[i][j]`和 `A[j][i]`标记为 1；对于有向图来说，如果顶点 i 到顶点 j 之间，有一条箭头从顶点 i 指向顶点 j 的边，那我们就将 `A[i][j]`标记为 1。同理，如果有一条箭头从顶点 j 指向顶点 i 的边，我们就将 `A[j][i]`标记为 1。对于带权图，数组中就存储相应的权重。

![](images/SJJG+SFZM-30-04.jpg)

用邻接矩阵来表示一个图，虽然简单、直观，但是比较浪费存储空间。

对于无向图来说，如果 `A[i][j]`等于 1，那 `A[j][i]`也肯定等于 1。实际上，我们只需要存储一个就可以了。也就是说，无向图的二维数组中，如果我们将其用对角线划分为上下两部分，那我们只需要利用上面或者下面这样一半的空间就足够了，另外一半白白浪费掉了。

> ==稀疏图==（Sparse Matrix）：顶点很多，但每个顶点的边并不多。

对于稀疏图，邻接矩阵存储方式更加浪费空间。

#### 优点

- 存储方式简单、直接，因为基于数组，所以在获取两个顶点的关系时，就非常高效。

- 方便计算。

  用邻接矩阵的方式存储图，可以将很多图的运算转换成矩阵之间的运算。比如求解最短路径问题时会提到一个[Floyd-Warshall 算法]([Floyd-Warshall算法 - 维基百科，自由的百科全书 (wikipedia.org)](https://zh.wikipedia.org/wiki/Floyd-Warshall算法))🔖，就是利用矩阵循环相乘若干次得到结果。

#### 邻接表存储方法

**==邻接表==（Adjacency List）**

> 邻接矩阵空间大，耗时少；邻接表空间小，耗时多。【时间、空间复杂度互换的设计思想】

每个顶点对应一条链表，链表中存储的是与这个顶点相连接的其他顶点（有向图，存储的是指向的顶）。

有向图的邻接表存储方式：

![有向图的邻接表存储方式](images/SJJG+SFZM-30-05.jpg)

如果链表太长，可以像散列表那样，把链表改成其他更高效的数据结构，比如平衡二叉查找树、跳表等。

### 思考题

> 数据结构是为算法服务的，所以具体选择哪种存储方法，与期望支持的操作有关系。

> 如何存储微博、微信等这些社交网络的好友关系吗？🔖

逆邻接表

![](images/SJJG+SFZM-30-06.jpg)



## 31 深度和广度优先搜索：如何找出社交网络中的三度好友关系？

==六度分割理论==，你与世界上的另一个人间隔的关系不会超过六度，也就是说**平均只需要六步就可以联系到任何两个互不相识的人**。

### 什么是“搜索”算法？

**==算法是作用于具体数据结构之上的==**，深度优先搜索算法和广度优先搜索算法都是基于“图”这种数据结构的。这是因为，图这种数据结构的**==表达能力==**很强，<u>大部分涉及搜索的场景都可以抽象成“图”</u>。

图上的搜索算法，可以理解为**在图中找出从一个顶点出发，到另一个顶点的路径**。具体方法有很多，比如最简单、最“暴力”的深度优先、广度优先搜索，还有 `A*`、`IDA*` 等启发式搜索算法。

```java
public class Graph {
    /**
     * 顶点的个数
     */
    private int v;
    /**
     * 邻接表
     */
    private LinkedList<Integer> adj[];

    public Graph(int v) {
        this.v = v;
        adj = new LinkedList[v];
        for (int i = 0; i < v; ++i) {
            adj[i] = new LinkedList<>();
        }
    }

    /**
     * 无向图一条边存两次
     */
    public void addEdge(int s, int t) {
        adj[s].add(t);
        adj[t].add(s);
    }
}
```



### 广度优先搜索

==广度优先搜索（Breadth-First-Search，BFS）==，就是一种“地毯式”层层推进的搜索策略，即<u>先查找离起始顶点最近的，然后是次近的，依次往外搜索</u>。

![](./images/SJJG+SFZM-31-01.jpg)

```java
    /**
     *
     * @param s 起始顶点
     * @param t 终止顶点
     */
    public void bfs(int s, int t) {
        if (s == t) {
            return;
        }
        // 记录顶点是否已经被访问
        boolean[] visited = new boolean[v];
        visited[s] = true;
        // 已经被访问、但相连的顶点还没有被访问的顶点
        Queue<Integer> queue = new LinkedList<>();
        queue.add(s);
        // 记录搜索路径。反向存储，prev[w]存储的是，顶点w是从哪个前驱顶点遍历过来的。
        int[] prev = new int[v];
        int w = queue.poll();
        for (int i = 0; i < adj[w].size(); ++i) {
            int q = adj[w].get(i);
            if (!visited[q]) {
                prev[q] = w;
                if (q == t) {
                    print(prev, s, t);
                    return;
                }
                visited[q] = true;
                queue.add(q);
            }
        }

    }
    /**
     * 递归打印s->t路径
     * @param prev
     * @param s
     * @param t
     */
    private void print(int[] prev, int s, int t) {
        if (prev[t] != -1 && t != s) {
            print(prev, s, prev[t]);
        }
        System.out.print(t + "");
    }
```

三个辅助变量：

- visited 是用来记录已经被访问的顶点，用来避免顶点被重复访问。
- queue 是一个队列，用来存储已经被访问、但相连的顶点还没有被访问的顶点。
- prev 用来记录搜索路径。

广度优先搜索的分解图：

![](./images/SJJG+SFZM-31-02.jpg)

> 广度优先搜索的时间、空间复杂度是多少呢？

最坏情况下，终止顶点 t 离起始顶点 s 很远，需要遍历完整个图才能找到。这个时候，每个顶点都要进出一遍队列，每个边也都会被访问一次，所以，广度优先搜索的时间复杂度是 O(V+E)，其中，V 表示顶点的个数，E 表示边的个数。当然，对于一个==连通图==来说，也就是说一个图中的所有顶点都是连通的，E 肯定要大于等于 V-1，所以，广度优先搜索的时间复杂度也可以简写为 `O(E)`。

广度优先搜索的空间消耗主要在几个辅助变量 visited 数组、queue 队列、prev 数组上。这三个存储空间的大小都不会超过顶点的个数，所以空间复杂度是 `O(V)`。

### 深度优先搜索

==深度优先搜索（Depth-First-Search，DFS）== 。

最直观的例子就是“走迷宫”。

假设你站在迷宫的某个岔路口，然后想找到出口。你随意选择一个岔路口来走，走着走着发现走不通的时候，你就回退到上一个岔路口，重新选择一条路继续走，直到最终找到出口。这种走法就是一种深度优先搜索策略。

![](./images/SJJG+SFZM-31-03.jpg)

实线箭头表示遍历，虚线箭头表示回退。从图中我们可以看出，深度优先搜索找出来的路径，并不是顶点 s 到顶点 t 的最短路径。

深度优先搜索用的是一种比较著名的算法思想，**==回溯思想==**。这种思想解决问题的过程，非常适合用递归来实现。

```java
    /**
     * 全局变量或者类成员变量
     * 作用：当已经找到终止顶点 t 之后，就不再递归地继续查找了。
     */
    boolean found = false;

    /**
     * 深度优先搜索
     * @param s
     * @param t
     */
    public void dfs(int s, int t) {
        found = false;
        boolean[] visited = new boolean[v];
        int[] prev = new int[v];
        for (int i = 0; i < v; ++i) {
            prev[i] = -1;
        }
        recurDfs(s, t, visited, prev);
        print(prev, s, t);
    }

    private void recurDfs(int w, int t, boolean[] visited, int[] prev) {
        if (found == true) {
            return;
        }
        visited[w] = true;
        if (w == t) {
            found = true;
            return;
        }
        for (int i = 0; i < adj[w].size(); ++i) {
            int q = adj[w].get(i);
            if (!visited[q]) {
                prev[q] = w;
                recurDfs(q, t, visited, prev);
            }
        }
    }
```

> 深度优先搜索的时间、空间复杂度是多少呢？

每条边最多会被访问两次，一次是遍历，一次是回退。所以，图上的深度优先搜索算法的时间复杂度是 O(E)，E 表示边的个数。

深度优先搜索算法的消耗内存主要是 visited、prev 数组和递归调用栈。visited、prev 数组的大小跟顶点的个数 V 成正比，递归调用栈的最大深度不会超过顶点的个数，所以总的空间复杂度就是 O(V)。

### 小结

广度优先搜索和深度优先搜索是图上的两种最常用、最基本的搜索算法，也被叫作暴力搜索算法。这两种搜索算法仅适用于状态空间不大，也就是说图不大的搜索。

广度优先搜索，地毯式层层推进，从起始顶点开始，依次往外遍历；借助队列来实现，遍历得到的路径就是，起始顶点到终止顶点的最短路径。

深度优先搜索用的是回溯思想，非常适合用递归实现。换种说法，深度优先搜索是借助栈来实现的。

在执行效率方面，两者的时间复杂度都是 O(E)，空间复杂度是 O(V)。

### 思考题

> 如何找出社交网络中某个用户的三度好友关系？
>
> 适合用图的广度优先搜索算法来解决

🔖

## 32 字符串匹配基础（上）：如何借助哈希算法实现高效字符串匹配？

字符串查找函数

字符串匹配算法有很多。

- 单模式串匹配的算法
  - BF算法和RK算法  （简单、好理解）
  - BM算法和KMP算法  （难理解、高效）
- 多模式串匹配算法
  - Trie树
  - AC自动机

RK算法是 BF算法的改进，它借助了哈希算法，让匹配的效率有了很大的提升。

### BF算法

BF算法， Brute Force，中文叫作**暴力匹配算法** 或者 朴素匹配算法。

![](./images/SJJG+SFZM-32-01.jpg)

最坏情况时间复杂度是 O(n*m)，虽然时间复杂度很高，但实际开发中BF算法比较常用，因为：

1. 实际的软件开发中，大部分情况下，模式串和主串的长度都不会太长。
2. 朴素字符串匹配算法思想简单，代码实现也非常简单。

### RK算法

Rabin-Karp算法



RK算法是**借助哈希算法对BF算法进行改造**，即对每个子串分别求哈希值，然后拿子串的哈希值与模式串的哈希值比较，减少了比较的时间。理想情况下，RK 算法的时间复杂度是 O(n)。

## 33 字符串匹配基础（中）：如何实现文本编辑器中的查找功能？

BM（Boyer-Moore）算法，非常高效的字符串匹配算法，原理复杂难懂。

### BM算法的核心思想



### BM算法原理分析

坏字符规则（bad character rule）和好后缀规则（good suffix shift）



### BM算法代码实现



### BM算法的性能分析及优化



## 34 字符串匹配基础（下）：如何借助BM算法轻松理解KMP算法？

在所有的字符串匹配算法里，最知名的就是KMP算法

### KMP 算法基本原理

KMP 算法是根据三位作者（D.E.Knuth，J.H.Morris 和 V.R.Pratt）的名字来命名的。



### 失效函数计算方法



### KMP 算法复杂度分析



## 35 Trie树：如何实现搜索引擎的搜索关键词提示功能？

搜索引擎的搜索关键词提示功能，我们经常使用。Google、百度等搜索引擎，这个功能肯定辽很多优化，但基本原理还是：**Trie树**（发音类似 "try"）。

![](images/SJJG+SFZM-35-01.jpg)

### 什么是“Trie 树”？

==Trie树==，也叫“==字典树==”，是一个树形结构，是一种专门处理字符串匹配的数据结构，用来解决==在一组字符串集合中快速查找某个字符串==的问题。

有6个字符串，分别是：how，hi，her，hello，so，see。如何多次查找某个字符串是否在前面的几个字符串中？

一般情况下就是，拿要查找到的字符串依次和6个字符串匹配，这是可行的，但效率比较低下。

先对6个字符串预处理一下，组织成字典树的结构，然后在字典树中匹配查找。

Trie树的本质，就是**利用字符串之间的公共前缀，将重复的前缀合并在一起**。

![](images/SJJG+SFZM-35-02.jpg)

其中，字典树有几个特点：

- 根节点不包含任何信息
- 每个节点表示一个字符串中的一个字符

- 根节点到红色节点的一条路径表示一个字符串（注：红色节点不都是叶子节点）

字典树的具体构造过程：

![](images/SJJG+SFZM-35-03.jpg)

在字典树种查找字符串的过程，就很好理解了，比如查找“her”，先把它分割成三个字符h、e、r，然后从根节点依次匹配：

![](images/SJJG+SFZM-35-04.jpg)



### 如何实现一棵 Trie树？

上面的过程可以分为两步：

1. **将字符串集合构造成Trie树**。
2. **在Trie树中查询一个字符串**。



### Trie树真的很耗内存吗？





### Trie树与散列表、红黑树的比较



### 思考题

> 如何利用 Trie 树，实现搜索关键词的提示功能？



## 36 AC自动机：如何用多模式串匹配实现敏感词过滤功能？

> 敏感词过滤功能最基本的原理就是字符串匹配算法，也就是通过维护一个敏感词的字典，当用户输入一段文字内容之后，通过字符串匹配算法，来查找用户输入的这段文字，是否包含敏感词。如果有，就用“***”把它替代掉。

> 那如何才能实现一个高性能的敏感词过滤系统呢？

### 基于单模式串和Trie树实现的敏感词过滤

BF 算法、RK 算法、BM 算法、KMP 算法都是==单模式串匹配算法==；

Trie 树是==多模式串匹配算法==。



### 经典的多模式串匹配算法：AC自动机

Aho-Corasick算法

AC自动机实际上就是在Trie树之上，加了类似 KMP的 next数组，只不过此处的 next数组是构建在树上罢了。



## 37 贪心算法：如何用贪心算法实现Huffman压缩编码？

一些**==算法思想==**：==贪心算法、分治算法、回溯算法、动态规划==。

贪心算法（greedy algorithm）有很多经典的应用，比如霍夫曼编码（Huffman Coding）、Prim 和 Kruskal 最小生成树算法、还有 Dijkstra单源最短路径算法。

> 霍夫曼编码是如何利用贪心算法来实现对数据压缩编码，有效节省数据存储空间的？

### 如何理解“贪心算法”？

针对一组数据，我们定义了==限制值==和==期望值==，希望从中选出几个数据，在满足限制值的情况下，期望值最大。



实际上，用贪心算法解决问题的思路，并不总能给出最优解。

### 贪心算法实战分析

#### 1 分糖果



#### 2 钱币找零



#### 3 区间覆盖



贪心算法的最难的一块是==如何将要解决的问题抽象成贪心算法模型==。



### 如何用贪心算法实现霍夫曼编码



霍夫曼编码是一种十分有效的编码方法，广泛用于数据压缩中，其压缩率通常在 20%～90% 之间。

霍夫曼编码不仅会考察文本中有多少个不同字符，还会考察每个字符出现的==频率==，<u>根据频率的不同，选择不同长度的编码</u>。

霍夫曼编码试图用这种不等长的编码方法，来进一步增加压缩的效率。如何给不同频率的字符选择不同长度的编码呢？根据贪心的思想，我们可以把出现频率比较多的字符，用稍微短一些的编码；出现频率比较少的字符，用稍微长一些的编码。

### 思考题

> 在一个非负整数 a 中，我们希望从中移除 k 个数字，让剩下的数字值最小，如何选择移除哪 k 个数字呢？



> 假设有 n 个人等待被服务，但是服务窗口只有一个，每个人需要被服务的时间长度是不同的，如何安排被服务的先后顺序，才能让这 n 个人总的等待时间最短？



## 38 分治算法：谈一谈大规模计算框架MapReduce中的分治思想

**==MapReduce==** 是 Google 大数据处理的三驾马车之一，另外两个是 ==**GFS 和 Bigtable**==。它在**倒排索引、PageRank 计算、网页分析**等搜索引擎相关的技术中都有大量的应用。

MapRedue 的本质就是分治算法。

### 如何理解分治算法？

分治算法（divide and conquer）的核心思想其实就是四个字，**==分而治之==** ，也就是<u>将原问题划分成 n 个规模较小，并且结构与原问题相似的子问题，递归地解决这些子问题，然后再合并其结果，就得到原问题的解。</u>

**分治算法是一种处理问题的==思想==，递归是一种==编程技巧==。**

实际上，分治算法一般都比较适合用递归来实现。分治算法的递归实现中，每一层递归都会涉及这样三个操作：

- 分解：将原问题分解成一系列子问题；
- 解决：递归地求解各个子问题，若子问题足够小，则直接求解；
- 合并：将子问题的结果合并成原问题。

分治算法能解决的问题，一般需要满足下面这几个条件：

- 原问题与分解成的小问题具有**相同的模式**；
- 原问题分解成的子问题可以**独立求解**，子问题之间**没有相关性**，这一点是分治算法跟动态规划的明显区别，等我们讲到动态规划的时候，会详细对比这两种算法；
- 具有分解**终止条件**，也就是说，当问题足够小时，可以**直接求解**；
- 可以将子问题合并成原问题，而这个合并操作的复杂度不能太高，否则就起不到减小算法总体复杂度的效果了。

### 分治算法应用举例分析



### 分治思想在海量数据处理中的应用

分治算法思想的应用是非常广泛的，并不仅限于指导编程和算法设计。它还经常用在海量数据处理的场景中。

我们前面讲的数据结构和算法，大部分都是**基于内存存储和单机处理**。但是，如果要处理的数据量非常大，没法一次性放到内存中，这个时候，这些数据结构和算法就无法工作了。



可以将海量的数据集合根据某种方法，划分为几个小的数据集合，每个小的数据集合单独加载到内存来解决，然后再将小数据集合合并成大数据集合。



==创新并非离我们很远，创新的源泉来自对事物本质的认识。无数优秀架构设计的思想来源都是基础的数据结构和算法，这本身就是算法的一个魅力所在。==

### 思考题



> 为什么说 MapReduce 的本质就是分治思想？

实际上，MapReduce 框架只是一个任务调度器，底层依赖 GFS 来存储数据，依赖 Borg 管理机器。它从 GFS 中拿数据，交给 Borg 中的机器执行，并且时刻监控机器执行的进度，一旦出现机器宕机、进度卡壳等，就重新从 Borg 中调度一台机器执行。

## 39 回溯算法：从电影《蝴蝶效应》中学习回溯算法的核心思想

回溯算法思想的应用：

- 深度优先搜索算

- 实际的软件开发场景中，比如正则表达式匹配、编译原理中的语法分析等

- 很多经典的数学问题，比如数独、八皇后、0-1背包、图的着色、旅行商问题、全排列等等。

### 如何理解“回溯算法”？

笼统地讲，回溯算法很多时候都应用在<u>“搜索”这类问题</u>上。不过这里说的搜索，并不是狭义的指我们前面讲过的图的搜索算法，而是**在一组可能的解中，搜索满足期望的解**。

回溯的处理思想，有点类似**枚举搜索**。我们枚举所有的解，找到满足期望的解。为了有规律地枚举所有可能的解，避免遗漏和重复，我们把问题求解的过程分为多个阶段。每个阶段，我们都会面对一个岔路口，我们先随意选一条路走，当发现这条路走不通的时候（不符合期望的解），就回退到上一个岔路口，另选一种走法继续走。

### 两个回溯算法的经典应用

#### 0-1背包

0-1 背包是非常经典的算法问题，很多场景都可以抽象成这个问题模型。这个问题的经典解法是动态规划，不过还有一种简单但没有那么高效的解法，那就是今天讲的回溯算法。



#### 正则表达式

如何用回溯算法，判断一个给定的文本，能否跟给定的正则表达式匹配？

依次考察正则表达式中的每个字符，当是非通配符时，我们就直接跟文本的字符进行匹配，如果相同，则继续往下处理；如果不同，则回溯。



回溯算法非常适合用递归来实现，在实现的过程中，==剪枝==操作是提高回溯效率的一种技巧。

## 40 初识动态规划：如何巧妙解决“双十一”购物时的凑单问题？

> 淘宝的“双十一”购物节有各种促销活动，比如“满 200 元减 50 元”。假设你女朋友的购物车中有 n 个（n>100）想买的商品，她希望从里面选几个，在凑够满减条件的前提下，让选出来的商品价格总和最大程度地接近满减条件（200 元）。



## 41 动态规划理论：一篇文章带你彻底搞懂最优子结构、无后效性和重复子问题

### “一个模型三个特征”理论讲解

什么样的问题适合用动态规划来解决呢？/动态规划能解决的问题有什么规律可循呢？

**多阶段决策最优解模型**

一般是用动态规划来解决==最优问题==。而解决问题的过程，需要经历多个==决策阶段==。每个决策阶段都对应着==一组状态==。然后我们寻找一组==决策序列==，经过这组决策序列，能够产生最终期望求解的最优值。

"三个特征"：

#### 1.最优子结构

最优子结构指的是，**问题的最优解包含子问题的最优解**。反过来说就是，我们可以**通过子问题的最优解，推导出问题的最优解**。如果我们把最优子结构，对应到我们前面定义的动态规划问题模型上，那我们也可以理解为，后面阶段的状态可以通过前面阶段的状态推导出来。

#### 2.无后效性

无后效性有两层含义：

- 第一层含义是，在推导后面阶段的状态的时候，我们只关心前面阶段的状态值，不关心这个状态是怎么一步一步推导出来的。
- 第二层含义是，某阶段状态一旦确定，就不受之后阶段的决策影响。

无后效性是一个非常“宽松”的要求。只要满足前面提到的动态规划问题模型，其实基本上都会满足无后效性。

#### 3.重复子问题

不同的决策序列，到达某个相同的阶段时，可能会产生重复的状态。

### “一个模型三个特征”实例剖析

假设我们有一个 n 乘以 n 的矩阵 `w[n][n]`。矩阵存储的都是正整数。棋子起始位置在左上角，终止位置在右下角。我们将棋子从左上角移动到右下角。每次只能向右或者向下移动一位。从左上角到右下角，会有很多不同的路径可以走。我们把每条路径经过的数字加起来看作路径的长度。那从左上角移动到右下角的最短路径长度是多少呢？

![](images/652dff86c5dcc6a0e2a0de9a814b079f.jpg)



![](images/b0da245a38fafbfcc590782486b85269.jpg)



### 两种动态规划解题思路总结

#### 1.状态转移表法



#### 2.状态转移方程法



### 四种算法思想比较分析

贪心、回溯、动态规划可以归为一类，而分治单独可以作为一类。前三个算法解决问题的模型，都可以抽象成我们今天讲的那个多阶段决策最优解模型，而分治算法解决的问题尽管大部分也是最优解问题，但是，大部分都不能抽象成多阶段决策模型。

回溯算法是个“万金油”。基本上能用的动态规划、贪心解决的问题，我们都可以用回溯算法解决。回溯算法相当于穷举搜索。穷举所有的情况，然后对比得到最优解。不过，回溯算法的时间复杂度非常高，是指数级别的，只能用来解决小规模数据的问题。

尽管动态规划比回溯算法高效，但是，并不是所有问题，都可以用动态规划来解决。能用动态规划解决的问题，需要满足三个特征，最优子结构、无后效性和重复子问题。在重复子问题这一点上，动态规划和分治算法的区分非常明显。分治算法要求分割成的子问题，不能有重复子问题，而动态规划正好相反，动态规划之所以高效，就是因为回溯算法实现中存在大量的重复子问题。

贪心算法实际上是动态规划算法的一种特殊情况。它解决问题起来更加高效，代码实现也更加简洁。不过，它可以解决的问题也更加有限。它能解决的问题需要满足三个条件，最优子结构、无后效性和贪心选择性（这里我们不怎么强调重复子问题）。

其中，最优子结构、无后效性跟动态规划中的无异。“贪心选择性”的意思是，通过局部最优的选择，能产生全局的最优选择。每一个阶段，我们都选择当前看起来最优的决策，所有阶段的决策完成之后，最终由这些局部最优解构成全局最优解。







## 42 动态规划实战：如何实现搜索引擎中的拼写纠错功能？

### 如何量化两个字符串的相似度？

莱文斯坦距离（Levenshtein distance）和最长公共子串长度（Longest common substring length）

![](images/f0e72008ce8451609abed7e368ac420f.jpg)



### 如何编程计算莱文斯坦距离？

![](images/864f25506eb3db427377bde7bb4c9589.jpg)



### 如何编程计算最长公共子串长度？



# 高级篇

## 43 拓扑排序：如何确定代码源文件的编译依赖关系？

编译器通过分析源文件或者程序员事先写好的编译配置文件（比如 Makefile 文件），来获取这种局部的依赖关系。<u>那编译器又该如何通过源文件两两之间的局部依赖关系，确定一个全局的编译顺序呢？</u>

![](images/5247b6639e98419a1963cecd8f12713b.jpg)



### 算法解析

> 穿衣服时有一定的顺序，衣服与衣服之间有一定的依赖关系。
>
> 你必须先穿袜子才能穿鞋，先穿内裤才能穿秋裤。假设我们现在有八件衣服要穿，它们之间的两两依赖关系我们已经很清楚了，那如何安排一个穿衣序列，能够满足所有的两两之间的依赖关系？

![](images/c26d0f472d9a607c0c4eb688c01959bd.jpg)

算法是构建在具体的数据结构之上的。针对这个问题，我们先来看下，**如何将问题背景抽象成具体的数据结构**？

拓扑排序本身就是基于有向无环图的一个算法。

如何在这个有向无环图上，实现拓扑排序？

#### 1 Kahn 算法



#### 2 DFS 算法



## 44 最短路径：地图软件是如何计算出最优出行路径的？

深度优先搜索和广度优先搜索这两种算法主要是针对无权图的搜索算法。

最短路径算法（Shortest Path Algorithm）

### 算法解析

最优问题包含三个：最短路线、最少用时和最少红绿灯。

解决软件开发中的实际问题，最重要的一点就是==建模==，也就是==将复杂的场景抽象成具体的数据结构==。

> 把地图抽象成有向有权图。
>
> 每个岔路口看作一个顶点，岔路口与岔路口之间的路看作一条边，路的长度就是边的权重。如果路是单行道，我们就在两个顶点之间画一条有向边；如果路是双行道，我们就在两个顶点之间画两条方向不同的边。

![](images/e20907173c458fac741e556c947bb9a9.jpg)

Dijkstra 最短路径算法

### 总结引申



## 45 位图：如何实现网页爬虫中的URL去重功能？

爬虫的工作原理是，通过解析已经爬取页面中的网页链接，然后再爬取这些链接对应的网页。而同一个网页链接有可能被包含在多个页面中，这就会导致爬虫在爬取的过程中，重复爬取相同的网页。<u>如何避免这些重复的爬取呢？</u>

### 算法解析

添加一个 URL 和查询一个 URL

内存消耗方面的优化，==布隆过滤器（Bloom Filter）==

存储结构，==位图（BitMap）==



![](images/94630c1c3b7657f560a1825bd9d02cae.jpg)



![](images/d0a3326ef0037f64102163209301aa1a.jpg)



布隆过滤器用多个哈希函数对同一个网页链接进行处理，CPU 只需要将网页链接从内存中读取一次，进行多次哈希计算，理论上讲这组操作是 CPU 密集型的。而在散列表的处理方式中，需要读取散列值相同（散列冲突）的多个网页链接，分别跟待判重的网页链接，进行字符串匹配。这个操作涉及很多内存数据的读取，所以是内存密集型的。我们知道 CPU 计算可能是要比内存访问更快速的，所以，理论上讲，布隆过滤器的判重方式，更加快速。



布隆过滤器非常适合这种不需要 100% 准确的、允许存在小概率误判的大规模判重场景。除了爬虫网页去重这个例子，还有比如统计一个大型网站的每天的 UV 数，也就是每天有多少用户访问了网站，我们就可以使用布隆过滤器，对重复访问的用户进行去重。

布隆过滤器最大的特点就是比较==省存储空间==。

## 46 概率统计：如何利用朴素贝叶斯算法过滤垃圾短信？

用位图、布隆过滤器，来过滤重复的数据。



### 算法解析

#### 1 基于黑名单的过滤器

维护一个骚扰电话号码和垃圾短信发送号码的黑名单。

#### 2 基于规则的过滤器

通过短信的内容，来判断某条短信是否是垃圾短信。



去掉“的、和、是”等没有意义的==停用词（Stop words）==，得到 n 个不同的单词。针对每个单词，我们统计有多少个垃圾短信出现了这个单词，有多少个非垃圾短信会出现这个单词，进而求出==每个单词出现在垃圾短信中的概率==，以及出现在非垃圾短信中的概率。**如果某个单词出现在垃圾短信中的概率，远大于出现在非垃圾短信中的概率，那我们就把这个单词作为特殊单词，用来过滤垃圾短信。**

#### 3 基于概率统计的过滤器



![](images/fbef6a760f916941bc3128c2d32540cc.jpg)



使用概率，来表征一个短信是垃圾短信的可信程度。







在实际的工程中，我们还需要结合具体的场景，以及大量的实验，不断去调整策略，权衡垃圾短信判定的==准确率==（是否会把不是垃圾的短信错判为垃圾短信）和==召回率==（是否能把所有的垃圾短信都找到），来实现我们的需求。

## 47 向量空间：如何实现一个简单的音乐推荐系统？

### 算法解析

核心思想：

- 找到跟你口味偏好相似的用户，把他们爱听的歌曲推荐给你；
- 找出跟你喜爱的歌曲特征相似的歌曲，把这些歌曲推荐给你。

#### 1 基于相似用户做推荐

如何定义口味偏好相似呢？

把跟你听类似歌曲的人，看作口味相似的用户。用“1”表示“喜爱”，用“0”笼统地表示“不发表意见”：

![](images/cc24a9c98a93795c75d8ef7a5892c406.jpg)

如何定义用户对某首歌曲的喜爱程度呢？

通过用户的行为，来定义这个喜爱程度。给每个行为定义一个得分，得分越高表示喜爱程度越高。

![](images/93c26a89303a748199528fdd998ebba6.jpg)

![](images/056552502f1cf4fdf331488e0eed5fa9.jpg)



==欧几里得距离（Euclidean distance）==是用来计算两个==向量==之间的==距离==的。

向量（vector）

![](images/f4d1d906c076688a43380f82e47dce12.jpg)

把每个用户对所有歌曲的喜爱程度，都用一个向量表示。计算出两个向量之间的欧几里得距离，作为两个用户的口味相似程度的度量。

![](images/3e145a3054c1abdea5d3f207d13e9b89.jpg)

#### 2 基于相似歌曲做推荐

计算机通过什么数据来量化两个歌曲之间的相似程度呢？



特征项



对于两首歌，如果喜欢听的人群都是差不多的，那侧面就可以反映出，这两首歌比较相似。



## 48 B+树：MySQL数据库索引是如何实现的？

### 解决问题的前提是定义清楚问题

> 如何定义清楚问题呢？

- 对问题进行详细的调研
- 通过对一些模糊的需求进行假设，来限定要解决的问题的范围。

功能性需求，关注执行效率和存储空间。

非功能性需求(比如安全、性能、用户体验等等)，关注性能。 

### 尝试用学过的数据结构解决这个问题



### 改造二叉查找树来解决这个问题

为了让二叉查找树支持按照区间来查找数据，对它进行这改造：树中的节点并不存储数据本身，而是只是作为索引。另外把每个叶子节点串在一条链表上，链表中的数据是从小到大有序的。

![](images/25700c1dc28ce094eed3ffac394531f4.jpg)

改造之后，要求某个区间的数据，只需要拿区间的起始值，在树中进行查找，当查找到某个叶子节点之后，再顺着链表往后遍历，直到链表中的结点数据值大于区间的终止值为止。所有遍历到的数据，就是符合区间值的所有数据。

![](images/1cf179c03c702a6ef5b9336f5b1eaecc.jpg)



> 通常内存的访问速度是==纳秒==级别的，而磁盘访问的速度是==毫秒==级别的。读取同样大小的数据，从磁盘中读取花费的时间，是从内存中读取所花费时间的上万倍，甚至几十万倍。

树的高度就等于每次查询数据时磁盘 IO 操作的次数。

优化：减少树的高度。

![](images/ea4472fd7bb7fa948532c8c8ba334430.jpg)

**索引可以提高数据库的查询效率，也会让写入数据的效率下降。**因为数据的写入过程，会涉及索引的更新。

![](images/1800bc80e1e05b32a042ff6873e6c2e0.jpg)



![](images/1730e34450dad29f062e76536622c918.jpg)



### B+树和B树

B树（B-Tree）有时也写成B-树，这里-只是一个连接符不是减号。

B 树实际上是低级版的 B+ 树，或者说**B+树是 B 树的改进版**。不同点：

- B+树中的节点不存储数据，只是索引，而B树中的节点存储数据；
- B中的叶子节点并不需要链表来串联。

也就是说，B树只是一个每个节点的子节点个数不能小于 m/2 的 m 叉树。



### 思考题

> B+ 树中，将叶子节点串起来的链表，是单链表还是双向链表？为什么？





## 49 搜索：如何用A*搜索算法实现游戏中的寻路功能？

> 魔兽世界、仙剑奇侠传这类==MMRPG==（大型多人在线角色扮演游戏）游戏，有一个非常重要的功能，那就是人物角色**自动寻路**。<u>当人物处于游戏地图中的某个位置的时候，我们用鼠标点击另外一个相对较远的位置，人物就会自动地绕过障碍物走过去。</u>
> 这个功能是怎么实现的呢？

出行路线规划、游戏寻路

搜索问题

次优解



**==A*算法==**是对Dijkstra算法的优化和改造。



启发函数（heuristic function）

曼哈顿距离（Manhattan distance）是两点之间横纵坐标的距离之和。

估价函数（evaluation function）



启发式搜索算法（Heuristically Search Algorithm）

## 50 索引：如何在海量数据中快速查找某个数据？

### 为什么需要索引？

在实际的软件开发中，业务纷繁复杂，功能千变万化，但是，万变不离其宗。如果抛开这些业务和功能的外壳，其实它们的本质都可以抽象为**==“对数据的存储和计算”==**。对应到数据结构和算法中，那==“存储”需要的就是数据结构，“计算”需要的就是算法==。

对于存储的需求，功能上无外乎**增删改查**。这其实并不复杂。但是，一旦存储的数据很多，那==性能==就成了这些系统要关注的重点，特别是在一些跟存储相关的基础系统（比如 MySQL 数据库、分布式文件系统等）、中间件（比如消息中间件 RocketMQ 等）中。

“如何节省存储空间、如何提高数据增删改查的执行效率”，这样的问题就成了设计的重点。而这些系统的实现，都离不开一个东西，那就是**索引**。不夸张地说，索引设计得好坏，直接决定了这些系统是否优秀。

### 索引的需求定义

#### 1 功能性需求

- 数据是格式化数据还是非格式化数据？

- 数据是静态数据还是动态数据？

- 索引存储在内存还是硬盘？

- 单值查找还是区间查找？

- 单关键词查找还是多关键词组合查找？

#### 2 非功能性需求

- 不管是存储在内存中还是磁盘中，索引对存储空间的消耗不能过大。

- 在考虑索引查询效率的同时，我们还要考虑索引的维护成本。



### 构建索引常用的数据结构有哪些？

常用来构建索引的数据结构，就是几种支持**==动态数据集合==**的数据结构。比如，**散列表、红黑树、跳表、B+ 树**。除此之外，**位图、布隆过滤器**可以作为辅助索引，**有序数组**可以用来对静态数据构建索引。



## 51 并行算法：如何利用并行处理提高算法的执行效率？

算法的目的就是为了提高代码执行的效率。那当算法无法再继续优化的情况下，可以借助并行计算的处理思想对算法进行改造，从而进一步提高执行效率。

### 并行排序



### 并行查找



### 并行字符串匹配



### 并行搜索



# 实战篇





# 更多

## 书单

![](images/1e306ffd0d56facbda45f413bc27a4b8.jpg)
