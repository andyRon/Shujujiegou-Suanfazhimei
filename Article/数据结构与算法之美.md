# 入门篇

https://github.com/wangzheng0822/algo

## 1 为什么要学习数据结构和算法？

### 1. 想要通关大厂面试，千万别让数据结构和算法拖了后腿

很多大公司，比如 BAT、Google、Facebook，面试的时候都喜欢考算法、让人现场写代码。越是厉害的公司，越是注重考察数据结构与算法这类基础知识。相比短期能力，他们更看中你的==长期潜力==。 

我们学任何知识都是为了“**用**”的，是<font color=#FF8C00>**为了解决实际工作问题**</font>的，学习数据结构和算法自然也不例外。 

### 2. 业务开发工程师，你真的愿意做一辈子CRUD boy吗？

对于大部分业务开发来说，我们平时可能更多的是利用已经封装好的现成的接口、类库来堆砌、翻译业务逻辑，很少需要自己实现数据结构和算法。但是，**不需要自己实现，并不代表什么都不需要了解。** 

如果不知道这些类库背后的原理，不懂得时间、空间复杂度分析，你如何能用好、用对它们？存储某个业务数据的时候，你如何知道应该用 **ArrayList**，还是 **LinkedList** 呢？调用了某个函数之后，你又该<font color=#FF8C00>**如何评估代码的性能和资源的消耗呢？**</font> 

作为业务开发，我们会用到各种**框架、中间件和底层系统**，比如 Spring、RPC 框架、消息中间件、Redis 等等。**在这些==基础框架==中，一般都揉和了很多基础数据结构和算法的设计思想。** 

> Redis 中的有序集合为什么要用跳表来实现呢？为什么不用二叉树呢？ 🔖
> 
> 因为它可以实现快速的查找数据，其数据查找的时间复杂度为O(logn)。此时可以对比一般的链表查询，时间复杂度O(n)，当数据规模n越大时跳表的查询效率优势就会越明显。

**掌握数据结构和算法，不管对于阅读框架源码，还是理解其背后的设计思想，都是非常有用的。** 

> **如何实时地统计业务接口的99%响应时间？** 
> 
> 两个堆-> [29堆的应用：利用堆求中位数](#堆的应用三：利用堆求中位数)

### 3. 基础架构研发工程师，写出达到开源水平的框架才是你的目标！

现在互联网上的技术文章、架构分享、开源项目满天飞，照猫画虎做一套基础框架并不难。我就拿RPC框架举例。 

不同的公司、不同的人做出的RPC框架，架构设计思路都差不多，最后实现的功能也都差不多。但是有的人做出来的框架，Bug很多、性能一般、扩展性也不好，只能在自己公司仅有的几个项目里面用一下。而有的人做的框架可以开源到 GitHub 上给很多人用，甚至被Apache收录。为什么会有这么大的差距呢？ 

高手之间的竞争其实就在<font color=#FF8C00>**细节**</font>。这些细节包括：**==算法是不是够优化，数据存取的效率是不是够高，内存是不是够节省==**等等。这些累积起来，决定了一个框架是不是优秀。 

### 4.对编程还有追求？不想被行业淘汰？那就不要只会写凑合能用的代码！

**性能好坏起码是其中一个非常重要的评判标准**。

有的人写代码的时候，从来都不考虑**==非功能性的需求==**，只是完成功能，凑合能用就好；做事情的时候，也从来没有长远规划，只把眼前事情做好就满足了。  

如果你在一家成熟的公司，或者BAT这样的大公司，面对的是千万级甚至亿级的用户，开发的是TB、PB级别数据的处理系统。性能几乎是开发过程中时刻都要考虑的问题。一个简单的 ArrayList、LinkedList 的选择问题，就可能会产生成千上万倍的性能差别。这个时候，数据结构和算法的意义就完全凸显出来了。 

其实，我觉得，**数据结构和算法这个东西，如果你不去学，可能真的这辈子都用不到，也感受不到它的好。但是一旦掌握，你就会常常被它的强大威力所折服**。之前你可能需要费很大劲儿来优化的代码，需要花很多心思来设计的架构，用了数据结构和算法之后，很容易就可以解决了。 

### **内容小结**

目的是**==建立时间复杂度、空间复杂度意识，写出高质量的代码，能够设计基础架构，提升编程技能，训练逻辑思维，积攒人生经验，以此获得工作回报，实现你的价值，完善你的人生==**。 

不管你是**业务开发工程师**，还是**基础架构工程师**；不管你是初入职场的初级工程师，还是工作多年的资深架构师，又或者是想转人工智能、区块链这些热门领域的程序员，数据结构与算法作为计算机的基础知识、核心知识，都是必须要掌握的。 

**掌握了数据结构与算法，你看待问题的==深度==，解决问题的角度就会完全不一样。**

### **课后思考**

- 你为什么要学习数据结构和算法呢？ 
  
  写出高质量的代码
  
  解决问题的新思路
  
  锻炼大脑🧠
  
  不当菜鸟

- 在过去的软件开发中，数据结构和算法在哪些地方帮到了你？

## 2 如何抓住重点，系统高效地学习数据结构与算法？

**没有找到好的学习方法，没有抓住学习的重点**。

### 什么是数据结构？什么是算法？

> 现在学习，并不是为了考试。

**虽然我们说没必要深挖严格的定义，但是这并不等于不需要理解概念。** 

广义上，**数据结构**是一组==数据==的==存储结构==。**算法**是**==操作==数据的一组==方法==**。 

图书馆储藏书籍。图书**按照一定规律编号**，就是书籍这种“数据”的存储结构。**查找书籍方法**都是算法。 

**经典数据结构和算法**，都是前人从很多实际操作场景中**抽象**出来的，经过非常多的**求证和检验**，可以高效地帮助我们解决很多实际的开发问题。 

**数据结构是为算法服务的，算法要作用在特定的数据结构之上**。它们是**相辅相成**的。

数据结构是**静态**的，它只是组织数据的一种方式。如果不在它的基础上操作、构建算法，**孤立存在的数据结构就是没用的**。 

### 学习的重点在什么地方？

**掌握一个数据结构与算法中最重要的概念**——<font color=#FF8C00>**复杂度分析**</font>。<u>考量效率和资源消耗的方法</u>。

一张图了解所有数据结构和算法的知识点。 

<img src="images/SJJG+SFZM-1-0.jpg" style="zoom: 25%;" />

**20 个最常用的、最基础**数据结构与算法，10 个数据结构：<font color=#FF8C00>数组、链表、栈、队列、散列表、二叉树、堆、跳表、图、Trie树</font>；10 个算法：<font color=#FF8C00>递归、排序、二分查找、搜索、哈希算法、贪心算法、分治算法、回溯算法、动态规划、字符串匹配算法</font>。 

要学习它的**“来历”，“自身的特点”，“适合解决的问题”以及“实际的应用场景”**。 

不要被动地记忆，要多辩证地思考，多问为什么。

### 一些可以让你事半功倍的学习技巧

#### 1.边学边练，适度刷题

**可以“适度”刷题，但一定不要浪费太多时间在刷题上**。

#### 2.多问、多思考、多互动

#### 3.打怪升级学习法

在枯燥的学习过程中，也可以给自己设立一个切实可行的目标

#### 4.知识需要沉淀，不要想试图一下子掌握所有

**学习知识的过程是==反复迭代、不断沉淀==的过程。** 



## 3 复杂度分析（上）：如何分析、统计算法的执行效率和资源消耗？

数据结构和算法本身解决的是“**快**”和“**省**”的问题，即如何让代码运行得更快，如何让代码更省存储空间。

**复杂度分析是整个算法学习的精髓，只要掌握了它，数据结构和算法的内容基本上就掌握了一半。** 

### 为什么需要复杂度分析？

**==事后统计法==**的局限性：

#### **1 测试结果非常依赖==测试环境==**

#### **2 测试结果受==数据规模==的影响很大**

对同一个排序算法，待排序数据的有序度不一样，排序的执行时间就会有很大的差别。极端情况下，如果数据已经是有序的，那排序算法不需要做任何操作，执行时间就会非常短。除此之外，如果测试数据规模太小，测试结果可能无法真实地反应算法的性能。比如，对于小规模的数据排序，插入排序可能反倒会比快速排序要快！ 

所以，**我们需要一个不用具体的测试数据来测试，就可以==粗略地估计算法的执行效率的方法==。**

### 大O复杂度表示法

算法的**执行效率**，粗略地讲，就是算法代码**执行的时间**。

如何在不运行代码的情况下，估算一下这段代码的执行时间。 

```c
int cal(int n) { 
   int sum = 0; 
   int i = 1; 
   for (; i <= n; ++i) { 
      sum = sum + i; 
   } 
   return sum; 
} 
```

从CPU的角度来看，这段代码的每一行都执行着类似的操作：**==读数据-运算-写数据==**。尽管每行代码对应的CPU执行的个数、执行的时间都不一样，但是，我们这里只是粗略估计，所以可以**假设每行代码执行的时间都一样**，为 **==unit_time==**。在这个假设的基础之上，这段代码的总执行时间是多少呢？ 

第 2、3 行代码分别需要 1 个 unit_time 的执行时间，第 4、5 行都运行了 n 遍，所以需要 **2n*unit_time**的执行时间，所以这段代码总的执行时间就是 **(2n+2)*unit_time**。可以看出来，**所有代码的执行时间 T(n) 与每行代码的执行次数成正比。** 

按照这个分析思路，我们再来看这段代码。 

```c
 int cal(int n) {
   int sum = 0;
   int i = 1;
   int j = 1;
   for (; i <= n; ++i) {
     j = 1;
     for (; j <= n; ++j) {
       sum = sum +  i * j;
     }
   }
 }
```

我们依旧假设每个语句的执行时间是 unit_time。那这段代码的总执行时间 T(n) 是多少呢？ 

第 2、3、4 行代码，每行都需要 1 个 unit_time 的执行时间，第 5、6 行代码循环执行了 n 遍，需要 **2n * unit_time** 的执行时间，第 7、8 行代码循环执行了 n<sup>2</sup>遍，所以需要 **2n<sup>2</sup> * unit_time** 的执行时间。所以，整段代码总的执行时间 **T(n) = (2n<sup>2</sup>+2n+3)*unit_time**。 

尽管我们不知道 unit_time 的具体值，但是通过这两段代码执行时间的推导过程，我们可以得到一个非常重要的规律，那就是，**所有代码的执行时间 T(n) 与每行代码的执行次数 n 成==正比==。** 

![](images/SJJG+SFZM-03-01.jpg)

其中，<u>T(n) 表示代码执行的时间；n 表示数据规模的大小；f(n) 表示每行代码执行的次数总和</u>。因为这是一个公式，所以用 f(n) 来表示。公式中的 O，表示代码的执行时间 T(n) 与 f(n) 表达式成**正比**。 

所以，第一个例子中的 T(n) = O(2n+2)，第二个例子中的 T(n) = O(2n<sup>2</sup>+2n+3)。这就是**大O时间复杂度表示法**。大O时间复杂度实际上并不具体表示代码真正的执行时间，而是表示**==代码执行时间随数据规模增长的变化趋势==**，所以，也叫作**渐进时间复杂度**（asymptotic time complexity），简称**==时间复杂度==**。 

当 n 很大时，你可以把它想象成 10000、100000。而公式中的**低阶、常量、系数**三部分并不左右增长趋势，所以都可以忽略。我们只需要记录一个最大量级就可以了，如果用大O表示法表示刚讲的那两段代码的时间复杂度，就可以记为：T(n) = O(n)； T(n) = O(n<sup>2</sup>)。 

### 时间复杂度分析

如何分析一段代码的时间复杂度？三个比较实用的方法。 

#### 1 只关注循环执行次数最多的一段代码

**在分析一个算法、一段代码的时间复杂度的时候，只关注循环执行次数最多的那一段代码就可以了。**这段核心代码执行次数的n的量级，就是整段要分析代码的时间复杂度。 

```c
 int cal(int n) {
   int sum = 0;
   int i = 1;
   for (; i <= n; ++i) {
     sum = sum + i;
   }
   return sum;
 }
```

其中第 2、3 行代码都是常量级的执行时间，与 n 的大小无关，所以对于复杂度并没有影响。循环执行次数最多的是第 4、5 行代码，所以这块代码要重点分析。这两行代码被执行了 n 次，所以总的时间复杂度就是 O(n)。 

#### 2 加法法则：总复杂度等于量级最大的那段代码的复杂度

```c
int cal(int n) {
   int sum_1 = 0;
   int p = 1;
   for (; p < 100; ++p) {
     sum_1 = sum_1 + p;
   }

   int sum_2 = 0;
   int q = 1;
   for (; q < n; ++q) {
     sum_2 = sum_2 + q;
   }

   int sum_3 = 0;
   int i = 1;
   int j = 1;
   for (; i <= n; ++i) {
     j = 1; 
     for (; j <= n; ++j) {
       sum_3 = sum_3 +  i * j;
     }
   }

   return sum_1 + sum_2 + sum_3;
 }
```

这个代码分为三部分，分别是求 sum_1、sum_2、sum_3。我们可以分别分析每一部分的时间复杂度，然后把它们放到一块儿，再取一个量级最大的作为整段代码的复杂度。 

第一段的时间复杂度是多少呢？这段代码循环执行了 100 次，所以是一个常量的执行时间，跟 n 的规模无关。 

第二段代码和第三段代码的时间复杂度是多少呢？答案是 O(n) 和 O(n<sup>2</sup>)。 

综合这三段代码的时间复杂度，我们取其中最大的量级。所以，整段代码的时间复杂度就为 O(n<sup>2</sup>)。也就是说：**总的时间复杂度就等于量级最大的那段代码的时间复杂度**。

将这个规律抽象成公式就是： 

如果 T1(n)=O(f(n))，T2(n)=O(g(n))；那么 T(n)=T1(n)+T2(n)=max(O(f(n)), O(g(n))) =**O(max(f(n), g(n)))**. 

#### 3 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积

还有一个**乘法法则**。如果 T1(n)=O(f(n))，T2(n)=O(g(n))；那么`T(n)=T1(n)*T2(n)=O(f(n))*O(g(n))=O(f(n)*g(n))`. 

也就是说，假设 T1(n) = O(n)，T2(n) = O(n<sup>2</sup>)，则 T1(n) * T2(n) = O(n<sup>3</sup>)。落实到具体的代码上，我们可以把乘法法则看成是**==嵌套循环==**，我举个例子给你解释一下。 

```c
int cal(int n) {
   int ret = 0; 
   int i = 1;
   for (; i < n; ++i) {
     ret = ret + f(i);
   } 
 } 

 int f(int n) {
  int sum = 0;
  int i = 1;
  for (; i < n; ++i) {
    sum = sum + i;
  } 
  return sum;
 }
```

我们单独看 cal() 函数。假设 f() 只是一个普通的操作，那第 4～6 行的时间复杂度就是，T1(n) = O(n)。但 f() 函数本身不是一个简单的操作，它的时间复杂度是 T2(n) = O(n)，所以，整个 cal() 函数的时间复杂度就是，T(n) = T1(n) * T2(n) = O(n*n) = O(n<sup>2</sup>)。 

### 几种常见时间复杂度实例分析

虽然代码千差万别，但是常见的复杂度量级并不多。下面👇复杂度量级几乎涵盖了你今后可以接触的所有代码的复杂度量级。 

![](images/SJJG+SFZM-03-02.jpg)

对于上面罗列的复杂度量级，我们可以粗略地分为两类，**==多项式量级==**和**==非多项式量级==**。其中，非多项式量级只有两个：O(2<sup>n</sup>) 和 O(n!)，也叫[**==NP问题==**](https://zh.wikipedia.org/wiki/P/NP问题)(Non-Deterministic Polynomial, 非确定多项式)。 

当数据规模n越来越大时，非多项式量级算法的执行时间会**急剧增加**，求解问题的执行时间会无限增长。所以，非多项式时间复杂度的算法其实是非常低效的算法。我们主要关注几种常见的**多项式时间复杂度**。 

![](images/1678135-514c3ef55574c28b.jpg)

#### O(1)

```c
int i = 8; 
int j = 6; 
int sum = i + j; 
```

**一般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万行的代码，其时间复杂度也是Ο(1)。** 

#### O(logn)、O(nlogn)

对数阶时间复杂度非常常见，同时也是**最难分析**的一种时间复杂度。 

```c
i=1; 
while (i <= n)  { 
   i = i * 2; 
} 
```

第三行代码是循环执行次数最多的。 

从代码中可以看出，变量 i 的值从 1 开始取，每循环一次就乘以 2。当大于 n 时，循环结束。实际上，变量 i 的取值就是一个等比数列。如果我把它一个一个列出来，就应该是这个样子的： 

![](images/SJJG+SFZM-03-03.jpg)

通过 2<sup>x</sup>=n 求解 x， 得x=log<sub>2</sub>n，所以，这段代码的时间复杂度就是 O(log<sub>2</sub>n)。 

把代码稍微改下的时间复杂度是多少？ 

```c
i=1; 
while (i <= n)  { 
   i = i * 3; 
} 
```

同上思路，这段代码的时间复杂度为 O(log<sub>3</sub>n)。 

我们知道，对数之间是可以互相转换的，log<sub>3</sub>n 就等于 log<sub>3</sub>2 * log<sub>2</sub>n，所以 O(log<sub>3</sub>n) = O(C * log<sub>2</sub>n)，其中 C=log<sub>3</sub>2 是一个常量。基于我们前面的一个理论：**在采用大 O 标记复杂度的时候，可以忽略系数，即 O(Cf(n)) = O(f(n))**。所以，O(log<sub>2</sub>n) 就等于 O(log<sub>3</sub>n)。因此，在对数阶时间复杂度的表示方法里，我们**忽略对数的“底”**，统一表示为 `O(logn)`。 

如果你理解了我前面讲的 O(logn)，那 O(nlogn) 就很容易理解了。还记得我们刚讲的乘法法则吗？如果一段代码的时间复杂度是 O(logn)，我们循环执行 n 遍，时间复杂度就是 O(nlogn) 了。而且，O(nlogn) 也是一种非常常见的算法时间复杂度。比如，归并排序、快速排序的时间复杂度都是 O(nlogn)。 

#### O(m+n)、O(m*n)

有的代码的复杂度**由两个数据的规模**来决定。 

```c++
int cal(int m, int n) {
  int sum_1 = 0;
  int i = 1;
  for (; i < m; ++i) {
    sum_1 = sum_1 + i;
  }

  int sum_2 = 0;
  int j = 1;
  for (; j < n; ++j) {
    sum_2 = sum_2 + j;
  }

  return sum_1 + sum_2;
}
```

从代码中可以看出，m 和 n 是表示两个数据规模。我们无法事先评估 m 和 n 谁的量级大，所以我们在表示复杂度的时候，就不能简单地利用加法法则，省略掉其中一个。所以，上面代码的时间复杂度就是 O(m+n)。 

针对这种情况，原来的加法法则就不正确了，我们需要将加法规则改为：**T1(m) + T2(n) = O(f(m) + g(n))**。但是乘法法则继续有效：**T1(m)*T2(n) = O(f(m) * f(n))**。 

### 空间复杂度分析

相对而言，空间复杂度分析方法学起来就简单些。 

时间复杂度的全称是**渐进时间复杂度，表示算法的执行时间与数据规模之间的增长关系**。类比一下，空间复杂度全称就是渐进空间复杂度（asymptotic space complexity），**表示算法的==存储空间与数据规模之间的增长关系==。** 

具体的例子。（这段代码有点“傻”，一般没人会这么写，我这么写只是为了方便给你解释。） 

```c++
void print(int n) {
  int i = 0;
  int[] a = new int[n];
  for (i; i < n; ++i) {
    a[i] = i * i;
  }

  for (i = n-1; i >= 0; --i) {
    print out a[i]
  }
}
```

跟时间复杂度分析一样，我们可以看到，第 2 行代码中，我们申请了一个空间存储变量 i，但是它是常量阶的，跟数据规模 n 没有关系，所以我们可以忽略。第 3 行申请了一个大小为 n 的 int 类型数组，除此之外，剩下的代码都没有占用更多的空间，所以整段代码的空间复杂度就是 O(n)。 

常见的空间复杂度就是 **O(1)、O(n)、O(n^2^)**，像O(logn)、O(nlogn)这样的对数阶复杂度平时都用不到。而且，空间复杂度分析比时间复杂度分析要简单很多。 

### 小结

复杂度也叫**渐进复杂度**，包括时间复杂度和空间复杂度，用来分析算法执行效率与数据规模之间的增长关系，可以粗略地表示，越高阶复杂度的算法，执行效率越低。常见的复杂度并不多，从低阶到高阶有：O(1)、O(logn)、O(n)、O(nlogn)、O(n<sup>2</sup> )。几乎所有的数据结构和算法的复杂度都跑不出这几个。 

![](images/SJJG+SFZM-03-04.jpg)

**复杂度分析并不难，关键在于多练**。  

### 课后思考

> 有人说，我们项目之前都会进行性能测试，再做代码的时间复杂度、空间复杂度分析，是不是多此一举呢？而且，每段代码都分析一下时间复杂度、空间复杂度，是不是很浪费时间呢？你怎么看待这个问题呢？ 

我不认为是多此一举，渐进时间、空间复杂度分析为我们提供了一个很好的**==理论分析的方向==**，并且它是宿主平台无关的，能够让我们对我们的程序或算法有一个大致的认识，让我们知道，比如在最坏的情况下程序的执行效率如何，同时也为我们交流提供了一个不错的桥梁，我们可以说，算法1的时间复杂度是O(n)，算法2的时间复杂度是O(logN)，这样我们立刻就对不同的算法有了一个“效率”上的**感性认识**。

当然，渐进式时间、空间复杂度分析只是一个理论模型，只能提供给粗略的估计分析，我们不能直接断定就觉得O(logN)的算法一定优于O(n), 针对不同的宿主环境，不同的数据集，不同的数据量的大小，在实际应用上面可能真正的性能会不同，个人觉得，针对不同的实际情况，进而进行一定的性能基准测试是很有必要的，比如在统一一批手机上(同样的硬件，系统等等)进行横向基准测试，进而选择适合特定应用场景下的最有算法。

综上所述，渐进式时间，==空间复杂度分析与性能基准测试并不冲突，而是相辅相成的==，但是一个低阶的时间复杂度程序有极大的可能性会优于一个高阶的时间复杂度程序，所以在实际编程中，时刻关心理论时间、空间度模型是有助于产出效率高的程序的，同时，因为渐进式时间、空间复杂度分析只是提供一个粗略的分析模型，因此也不会浪费太多时间，重点在于在编程时，要具有这种复杂度分析的思维。

## 4 复杂度分析（下）：浅析最好、最坏、平均、均摊时间复杂度

四个复杂度分析：<font color=#FF8C00>最好情况时间复杂度</font>（best case time complexity）、<font color=#FF8C00>最坏情况时间复杂度</font>（worst case time complexity）、<font color=#FF8C00>平均情况时间复杂度</font>（average case time complexity）、<font color=#FF8C00>均摊时间复杂度</font>（amortized time complexity）。

### **1.最好、最坏情况时间复杂度**

分析一下这段代码的时间复杂度。 

```c
// n 表示数组 array 的长度
int find(int[] array, int n, int x) {
  int i = 0;
  int pos = -1;
  for (; i < n; ++i) {
    if (array[i] == x) pos = i;
  }
  return pos;
}
```

这段代码要实现的功能是，在一个无序的数组（array）中，查找变量 x 出现的位置。如果没有找到，就返回 -1。按照上节课讲的分析方法，这段代码的复杂度是 O(n)，其中，n 代表数组的长度。 

在数组中查找一个数据，并不需要每次都把整个数组都遍历一遍，因为有可能中途找到就可以提前结束循环了。但是，这段代码写得不够高效。我们可以这样优化一下这段查找代码。 

```c
// n 表示数组 array 的长度
int find(int[] array, int n, int x) {
  int i = 0;
  int pos = -1;
  for (; i < n; ++i) {
    if (array[i] == x) {
       pos = i;
       break;
    }
  }
  return pos;
} 
```

这个时候，问题就来了。我们优化完之后，这段代码的时间复杂度还是 O(n) 吗？很显然，咱们上一节讲的分析方法，解决不了这个问题。 

因为，要查找的变量 x 可能出现在数组的任意位置。如果数组中第一个元素正好是要查找的变量 x，那就不需要继续遍历剩下的 n-1 个数据了，那时间复杂度就是 O(1)。但如果数组中不存在变量 x，那我们就需要把整个数组都遍历一遍，时间复杂度就成了 O(n)。所以，不同的情况下，这段代码的时间复杂度是不一样的。 

为了表示代码在不同情况下的不同时间复杂度，我们需要引入三个概念：最好情况时间复杂度、最坏情况时间复杂度和平均情况时间复杂度。 

顾名思义，**最好情况时间复杂度就是，在最理想的情况下，执行这段代码的时间复杂度**。就像我们刚刚讲到的，在最理想的情况下，要查找的变量 x 正好是数组的第一个元素，这个时候对应的时间复杂度就是最好情况时间复杂度。 

同理，**最坏情况时间复杂度就是，在最糟糕的情况下，执行这段代码的时间复杂度**。就像刚举的那个例子，如果数组中没有要查找的变量 x，我们需要把整个数组都遍历一遍才行，所以这种最糟糕情况下对应的时间复杂度就是最坏情况时间复杂度。 

### **2.平均情况时间复杂度**

上面两种情况都是极端情况下的代码复杂度，引入一个更贴合实际情况的概念：平均情况时间复杂度，后面我简称为**==平均时间复杂度==**。

要查找的变量 x 在数组中的位置，有 n+1 种情况：**在数组的 0～n-1 位置中和不在数组中**。我们把每种情况下，查找需要遍历的元素个数累加起来，然后再除以 n+1，就可以得到需要遍历的元素个数的平均值，即： 

![](images/SJJG+SFZM-04-01.jpg)

根据大O标记法，省略掉系数、低阶、常量，简化后得到平均时间复杂度就是 O(n)。 

这个结论虽然是正确的，但是计算过程稍微有点儿问题。究竟是什么问题呢？我们刚讲的这 n+1 种情况，出现的概率并不是一样的。我带你具体分析一下。

我们知道，要查找的变量 x，要么在数组里，要么就不在数组里。这两种情况对应的概率统计起来很麻烦，为了方便你理解，我们假设在数组中与不在数组中的概率都为 1/2。另外，要查找的数据出现在 0～n-1 这 n 个位置的概率也是一样的，为 1/n。所以，根据**概率乘法法则**，要查找的数据出现在 0～n-1 中任意位置的概率就是 1/(2n)。 

因此，前面的推导过程中存在的最大问题就是，没有将各种情况发生的概率考虑进去。如果我们把每种情况发生的概率也考虑进去，那平均时间复杂度的计算过程就变成了这样： 

![](images/SJJG+SFZM-04-02.jpg)

🔖这个值就是概率论中的**==加权平均值==**，也叫作**==期望值==**，所以平均时间复杂度的全称应该叫**加权平均时间复杂度**或者**期望时间复杂度**。 

引入概率之后，前面那段代码的加权平均值为 (3n+1)/4。用大 O 表示法来表示，去掉系数和常量，这段代码的加权平均时间复杂度仍然是 O(n)。 

你可能会说，平均时间复杂度分析好复杂啊，还要涉及概率论的知识。**实际上，在大多数情况下，我们并不需要区分最好、最坏、平均情况时间复杂度三种情况。**像我们上一节课举的那些例子那样，很多时候，我们使用一个复杂度就可以满足需求了。**只有同一块代码在不同的情况下，时间复杂度有量级的差距，我们才会使用这三种复杂度表示法来区分。** 

### **3.均摊时间复杂度**

均摊时间复杂度，以及它对应的分析方法，摊还分析（或者叫**平摊分析**）。 

均摊时间复杂度，听起来跟平均时间复杂度有点儿像。对于初学者来说，这两个概念确实非常容易弄混。大部分情况下，并不需要区分最好、最坏、平均三种复杂度。平均复杂度只在某些特殊情况下才会用到，而均摊时间复杂度应用的场景比它更加特殊、更加有限。 

借助一个具体的例子理解。（当然，这个例子只是我为了方便讲解想出来的，实际上没人会这么写。） 

```c
 // array 表示一个长度为 n 的数组
 // 代码中的 array.length 就等于 n
 int[] array = new int[n];
 int count = 0;

 void insert(int val) {
    if (count == array.length) {
       int sum = 0;
       for (int i = 0; i < array.length; ++i) {
          sum = sum + array[i];
       }
       array[0] = sum;
       count = 1;
    }

    array[count] = val;
    ++count;
 }
```

这段代码实现了一个往数组中插入数据的功能。当数组满了之后，也就是代码中的 count == array.length 时，我们用 for 循环遍历数组求和，并清空数组，将求和之后的 sum 值放到数组的第一个位置，然后再将新的数据插入。但如果数组一开始就有空闲空间，则直接将数据插入数组。 

那这段代码的时间复杂度是多少呢？你可以先用我们刚讲到的三种时间复杂度的分析方法来分析一下。 

最理想的情况下，数组中有空闲空间，我们只需要将数据插入到数组下标为 count 的位置就可以了，所以最好情况时间复杂度为 O(1)。最坏的情况下，数组中没有空闲空间了，我们需要先做一次数组的遍历求和，然后再将数据插入，所以最坏情况时间复杂度为 O(n)。 

那平均时间复杂度是多少呢？答案是 O(1)。我们还是可以通过前面讲的概率论的方法来分析。 

假设数组的长度是 n，根据数据插入的位置的不同，我们可以分为 n 种情况，每种情况的时间复杂度是 O(1)。除此之外，还有一种“额外”的情况，就是在数组没有空闲空间时插入一个数据，这个时候的时间复杂度是 O(n)。而且，这 n+1 种情况发生的概率一样，都是 1/(n+1)。所以，根据加权平均的计算方法，我们求得的平均时间复杂度就是： 

![](images/SJJG+SFZM-04-03.jpg)

至此为止，前面的最好、最坏、平均时间复杂度的计算，理解起来应该都没有问题。但是这个例子里的平均复杂度分析其实并不需要这么复杂，不需要引入概率论的知识。这是为什么呢？我们先来对比一下这个 insert() 的例子和前面那个 find() 的例子，你就会发现这两者有很大差别。 

首先，find() 函数在极端情况下，复杂度才为 O(1)。但 insert() 在大部分情况下，时间复杂度都为 O(1)。只有个别情况下，复杂度才比较高，为 O(n)。这是 insert()**第一个**区别于 find() 的地方。 

我们再来看**第二个**不同的地方。对于 insert() 函数来说，O(1) 时间复杂度的插入和 O(n) 时间复杂度的插入，出现的频率是非常有规律的，而且有一定的前后时序关系，一般都是一个 O(n) 插入之后，紧跟着 n-1 个 O(1) 的插入操作，循环往复。 

所以，针对这样一种特殊场景的复杂度分析，我们并不需要像之前讲平均复杂度分析方法那样，找出所有的输入情况及相应的发生概率，然后再计算加权平均值。 

针对这种特殊的场景，我们引入了一种更加简单的分析方法：<font color=#FF8C00>**摊还分析法**</font>，通过摊还分析得到的时间复杂度我们起了一个名字，叫**均摊时间复杂度**。 

<u>那究竟如何使用摊还分析法来分析算法的均摊时间复杂度呢？</u> 

我们还是继续看在数组中插入数据的这个例子。每一次 O(n) 的插入操作，都会跟着 n-1 次 O(1) 的插入操作，所以把耗时多的那次操作均摊到接下来的 n-1 次耗时少的操作上，均摊下来，这一组连续的操作的均摊时间复杂度就是 O(1)。这就是均摊分析的大致思路。你都理解了吗？ 

均摊时间复杂度和摊还分析应用场景比较特殊，所以我们并不会经常用到。为了方便你理解、记忆，我这里简单总结一下它们的应用场景。如果你遇到了，知道是怎么回事儿就行了。 

对一个数据结构进行一组连续操作中，大部分情况下时间复杂度都很低，只有个别情况下时间复杂度比较高，而且这些操作之间存在前后连贯的时序关系，这个时候，我们就可以将这一组操作放在一块儿分析，看是否能**将较高时间复杂度那次操作的耗时，平摊到其他那些时间复杂度比较低的操作上**。而且，在能够应用均摊时间复杂度分析的场合，一般均摊时间复杂度就等于最好情况时间复杂度。 

尽管很多数据结构和算法书籍都花了很大力气来区分平均时间复杂度和均摊时间复杂度，但其实我个人认为，**均摊时间复杂度就是一种特殊的平均时间复杂度**，我们没必要花太多精力去区分它们。你最应该掌握的是它的分析方法，摊还分析。至于分析出来的结果是叫平均还是叫均摊，这只是个说法，并不重要。 

### **4.内容小结**

最好、最坏情况下的时间复杂度分析起来比较简单，但平均、均摊两个复杂度分析相对比较复杂。如果你觉得理解得还不是很深入，不用担心，在后续具体的数据结构和算法学习中，我们可以继续慢慢实践！ 

### **5.课后思考**

分析一下下面这个 add()函数的时间复杂度。 

```c
// 全局变量，大小为 10 的数组 array，长度 len，下标 i。
int array[] = new int[10]; 
int len = 10;
int i = 0;

// 往数组中添加一个元素
void add(int element) {
   if (i >= len) { // 数组空间不够了
     // 重新申请一个 2 倍大小的数组空间
     int new_array[] = new int[len*2];
     // 把原来 array 数组中的数据依次 copy 到 new_array
     for (int j = 0; j < len; ++j) {
       new_array[j] = array[j];
     }
     // new_array 复制给 array，array 现在大小就是 2 倍 len 了
     array = new_array;
     len = 2 * len;
   }
   // 将 element 放到下标为 i 的位置，下标 i 加一
   array[i] = element;
   ++i;
}
```

最好是O(1)，最差是O(n), 均摊是O(1)



# 基础篇

## 5 数组：为什么很多编程语言中数组都从0开始编号？

### 如何实现随机访问？

**数组（Array）是一种<font color=#FF8C00>线性表</font>数据结构。它用一组<font color=#FF8C00>连续的内存空间</font>，来存储一组具有<font color=#FF8C00>相同类型</font>的数据。** 

1. **<font color=#FF8C00>线性表（Linear List）</font>**就是数据排成像一条线一样的结构。每个线性表上的数据最多只有**前和后**两个方向。其实除了<font color=#FF8C00>数组，链表、队列、栈</font>等也是线性表结构。 

<img src="images/SJJG+SFZM-05-01.jpg"  />

**<font color=#FF8C00>非线性表</font>**，比如<font color=#FF8C00>二叉树、堆、图</font>等。在非线性表中，数据之间并不是简单的**前后关系**。

<img src="images/SJJG+SFZM-05-02.jpg"  />

2. <font color=#FF8C00>连续的内存空间和相同类型的数据</font>。

​    利：“**随机访问**”。

​    弊：很多操作变得非常低效，比如要想在数组中删除、插入一个数据，为了保证连续性，就需要做大量的数据搬移工作。 

#### 数组是如何实现根据下标随机访问数组元素的

以长度为 10 的 int 类型的数组 `int[] a = new int[10]` 为例。

计算机给数组 a[10]，分配了一块连续内存空间 1000～1039，其中，内存块的首地址为 base_address = 1000。 

<img src="images/SJJG+SFZM-05-03.jpg"  />

计算机会给每个内存单元分配一个地址，计算机通过地址来访问内存中的数据。当计算机需要随机访问数组中的某个元素时，它会首先通过下面的**==寻址公式==**，计算出该元素存储的内存地址： 

```java
a[i]_address = base_address + i * data_type_size 
```

其中**data_type_size** 表示数组中每个元素的大小。这个例子数组中存储的是 int 类型数据，所以 data_type_size 就为 4 个字节。 

#### 数组和链表的区别

不准确表达，“链表适合插入、删除，时间复杂度 O(1)；数组适合查找，查找时间复杂度为 O(1)”。 

数组是适合查找操作，但是查找的时间复杂度并不为 O(1)。即便是排好序的数组，你用二分查找，时间复杂度也是 O(logn)。所以，正确的表述应该是，**<font color=#FF8C00>数组支持随机访问，根据下标随机访问的时间复杂度为 O(1)</font>**。 

### 低效的“插入”和“删除”

数组为了保持内存数据的连续性，会导致插入、删除比较低效。

#### **插入操作**

假设数组的长度为 n，现在，如果我们需要将一个数据插入到数组中的第 k 个位置。为了把第 k 个位置腾出来，给新来的数据，我们需要将第 k～n 这部分的元素都顺序地往后挪一位。那插入操作的时间复杂度是多少呢？

如果在数组的末尾插入元素，那就不需要移动数据了，这时的时间复杂度为 O(1)。但如果在数组的开头插入元素，那所有的数据都需要依次往后移动一位，所以最坏时间复杂度是 O(n)。 因为我们在每个位置插入元素的概率是一样的，所以平均情况时间复杂度为 <u>(1+2+…n)/n=O(n)</u>。 

如果数组中的数据是有序的，我们在某个位置插入一个新的元素时，就必须按照刚才的方法搬移 k 之后的数据。但是，如果数组中存储的数据并没有任何规律，数组只是被当作一个存储数据的集合。在这种情况下，如果要将某个数组插入到第 k 个位置，为了避免大规模的数据搬移，我们还有一个简单的办法就是，==直接将第k位的数据搬移到数组元素的最后，把新的元素直接放入第k个位置。== 

为了更好地理解，我们举一个例子。假设数组 a[10] 中存储了如下 5 个元素：a，b，c，d，e。 

我们现在需要将元素 x 插入到第 3 个位置。我们只需要将 c 放入到 a[5]，将 a[2] 赋值为 x 即可。最后，数组中的元素如下： a，b，x，d，e，c。 

![](images/SJJG+SFZM-05-04.jpg)

利用这种处理技巧，在特定场景下，在第k个位置插入一个元素的时间复杂度就会降为O(1)。这个处理思想在快排中也会用到。 

#### 删除操作

跟插入数据类似，如果要删除第k个位置的数据，为了内存的连续性，也需要搬移数据，不然中间就会出现空洞，内存就不连续了。 

和插入类似，如果删除数组末尾的数据，则最好情况时间复杂度为 O(1)；如果删除开头的数据，则最坏情况时间复杂度为 O(n)；平均情况时间复杂度也为 O(n)。 

实际上，在某些特殊场景下，我们并不一定非得追求数组中数据的连续性。如果我们**将多次删除操作集中在一起执行**，删除的效率是不是会提高很多呢？ 

例子。数组 a[10] 中存储了 8 个元素：a，b，c，d，e，f，g，h。现在，我们要依次删除 a，b，c 三个元素。 

![](images/SJJG+SFZM-05-05.jpg)

为了避免 d，e，f，g，h 这几个数据会被搬移三次，可以先记录下已经删除的数据。<u>每次的删除操作并不是真正地搬移数据，只是记录数据已经被删除。当数组没有更多空间存储数据时，再触发执行一次真正的删除操作，这样就大大减少了删除操作导致的数据搬移。</u> 

如果你了解**JVM**(JAVAV虚拟机)，你会发现，这不就是**==JVM标记清除垃圾回收算法==**的核心思想吗？没错，数据结构和算法的魅力就在于此，<font color=#FF8C00>很多时候我们并不是要去死记硬背某个数据结构或者算法，而是要学习它背后的思想和处理技巧，这些东西才是最有价值的</font>。如果你细心留意，不管是在软件开发还是架构设计中，总能找到某些算法和数据结构的影子。 

### 警惕数组的访问越界问题

> 在C语言中，只要不是访问受限的内存，所有的内存空间都是可以自由访问的。

数组越界在C语言中是一种**==未决行为==**，并没有规定数组访问越界时编译器应该如何处理。因为，**访问数组的本质就是访问一段连续内存，只要数组通过偏移计算得到的内存地址是可用的，那么程序就可能不会报任何错误**。

很多**计算机病毒**也正是利用到了代码中的数组越界可以访问非法地址的漏洞，来攻击系统。

Java有越界检查，`java.lang.ArrayIndexOutOfBoundsException`。

### 容器能否完全替代数组？

针对数组类型，很多语言都提供了容器类，比如 Java 中的 ArrayList、C++ STL中的vector。

Java中`ArrayList`的优势：**将很多数组操作（比如插入、删除等）的细节封装起来**；**支持动态扩容**。

最好在创建ArrayList 的时候**事先指定数据大小**。事先指定数据大小可以省掉很多次内存申请和数据搬移操作。

数组更适合的情况：

1. ArrayList无法存储基本类型，需要封装为包装类；
2. 如果数据大小事先已知，并且对数据的操作非常简单，用不到 ArrayList 提供的大部分方法，也可以直接使用数组。
3. 多维数组时，用数组往往会更加直观

```java
Object[][] array;

ArrayList<ArrayList<Object>> array;
```

总结：对于**业务开发**，直接使用容器就足够了。一些非常**底层的开发**，比如开发网络框架，性能的优化需要做到极致，这个时候数组就会优于容器。

### 思考题

> 为什么数组要从 0 开始编号，而不是从 1 开始呢？

原因一：”下标“理解为”偏移（offset）“，从1开始，对CPU来说，就多了一次减法指令。

```c
a[k]_address = base_address + k * type_size

a[k]_address = base_address + (k-1)*type_size
```

原因二：历史原因。C语言设计者用 0 开始计数数组下标。

> 前面我基于数组的原理引出 **JVM的标记清除垃圾回收算法**的核心理念。那怎么理解的标记清除垃圾回收算法。 

大多数主流虚拟机采用可达性分析算法来判断对象是否存活，在标记阶段，会遍历所有 GC ROOTS，将所有 GC ROOTS 可达的对象标记为存活。只有当标记工作完成后，清理工作才会开始。

不足：

1. 效率问题。标记和清理效率都不高，但是当知道只有少量垃圾产生时会很高效。
2. 空间问题。会产生不连续的内存空间碎片。

> 前面我们讲到一维数组的内存寻址公式，那你可以思考一下，类比一下，二维数组的内存寻址公式是怎样的呢？ 

对于 m * n 的数组，`a [i][j]` (i < m,j < n)的地址为：`address = base_address + ( i * n + j) * type_size` 

🔖 m和n怎么区分先后？



## 06 链表（上）：如何实现LRU缓存淘汰算法?

缓存是一种提高数据读取性能的技术，在硬件设计、软件开发中都有着非常广泛的应用，比如常见的 ==CPU缓存==、==数据库缓存==、==浏览器缓存==等等。

> 当缓存被用满时，哪些数据应该被清理出去，哪些数据应该被保留？

这就需要==缓存淘汰策略==来决定。常见的策略有三种：

- 先进先出策略 FIFO（First In，First Out）
- 最少使用策略 LFU（Least Frequently Used）
- 最近最少使用策略 LRU（Least Recently Used）

> 数组和链表是两个**非常基础、非常常用**的数据结构。
> 
> 某种意义上是仅有的两种数据存储结构。 因为使用内存空间时，只有两种方式：要么是==连续==的内存空间——数组，要么是==不连续==的内存空间——链表。 ==其它复杂的数据结构，其实都是在数组和链表的基础上形成的==。

### 五花八门的链表结构

链表并不需要一块连续的内存空间，它通过“指针”将一组零散的内存块串联起来使用。

![](images/SJJG+SFZM-06-01.jpg)

三种最常见的链表结构：

#### 1 单链表

![](images/SJJG+SFZM-06-02.jpg)

>  结点
> 
>  后继指针next
> 
>  头结点
> 
>  尾结点         
> 
>  空地址NULL

链表也支持数据的查找、插入和删除操作。

![](images/SJJG+SFZM-06-03.jpg)

单链表的插入、删除操作的时间复杂度都是 O(1) 。

#### 2 循环链表

循环链表是一种特殊的单链表。

![](images/SJJG+SFZM-06-04.jpg)

和单链表相比，循环链表的优点是从链尾到链头比较方便。当要处理的数据具有**环型结构**特点时，就特别适合采用循环链表。比如著名的**约瑟夫问题**。

#### 3 双向链表

![](images/SJJG+SFZM-06-05.jpg)

双向链表需要额外的两个空间来存储<u>后继结点和前驱结点的地址</u>。

> 前驱指针prev
> 
> 双向遍历

从链表中**删除**一个数据无外乎这两种情况：

- 删除结点中“值等于某个给定值”的结点；
- 删除给定指针指向的结点。

第一种情况，单链表和双向链表，为了查找到值等于给定值的结点，都需要从头结点开始一个一个依次遍历对比，直到找到值等于给定值的结点，然后再通过我前面讲的指针操作将其删除。（删除之前先要找到要删除的节点）

第二种情况，已经找到了要删除的结点，但是删除某个结点 q 需要知道其前驱结点，而单链表并不支持直接获取前驱结点，所以，为了找到前驱结点，我们还是要从头结点开始遍历链表，直到 p->next=q，说明 p 是 q 的前驱结点。而双向链表中的结点已经保持前驱结点的指针了，不需要遍历。

**插入**的情况类似。

除了插入、删除操作有优势之外，对于一个==有序链表==，双向链表的按值查询的效率也要比单链表高一些。因为，我们可以记录上次查找的位置 p，每次查询时，根据要查找的值与 p 的大小关系，决定是往前还是往后查找，所以平均只需要查找一半的数据。

> Java中的`LinkedHashMap`用到双向链表。

> 💡用**==空间换时间==**的设计思想：当内存空间充足的时候，如果我们更加追求代码的执行速度，我们就可以选择空间复杂度相对较高、但时间复杂度相对很低的算法或者数据结构。
> 
> 相反，如果内存比较紧缺，比如代码跑在手机或者单片机上，这个时候，就要反过来用**==时间换空间==**的设计思路。

双向循环链表

![](images/d1665043b283ecdf79b157cfc9e5ed91.jpg)

### 链表 VS 数组性能大比拼

![](images/4f63e92598ec2551069a0eef69db7168.jpg)

数组和链表的对比，并不能局限于时间复杂度。

数组简单易用，在实现上使用的是连续的内存空间，可以借助**==CPU的缓存机制==**，预读数组中的数据，所以访问效率更高。而链表在内存中并不是连续存储，所以对 CPU缓存不友好，没办法有效预读。

数组的缺点是==大小固定==，一经声明就要占用整块连续内存空间。如果声明的数组过大，系统可能没有足够的连续内存空间分配给它，导致“内存不足（out of memory）”。如果声明的数组过小，则可能出现不够用的情况。这时只能再申请一个更大的内存空间，把原数组拷贝进去，非常费时。链表本身没有大小的限制，天然地支持==动态扩容==，我觉得这也是它与数组最大的区别。

### 小结

和数组相比，链表更适合插入、删除操作频繁的场景，查询的时间复杂度较高。

![](images/image-20231113194037994.png)

### 思考题

> 如何基于链表实现 LRU 缓存淘汰算法？

思路：维护一个有序单链表，越靠近链表尾部的结点是越早之前访问的。当有一个新的数据被访问时，我们从链表头开始顺序遍历链表。

1. 如果此数据之前已经被缓存在链表中了，我们遍历得到这个数据对应的结点，并将其从原来的位置删除，然后再插入到链表的头部。
2. 如果此数据没有在缓存链表中，又可以分为两种情况：
   - 如果此时缓存未满，则将此结点直接插入到链表的头部；
   - 如果此时缓存已满，则链表尾结点删除，将新的数据结点插入链表的头部。

> 如何利用数组实现 LRU 缓存淘汰策略呢？🔖

> 如果字符串是通过单链表来存储的，那该如何来判断是一个回文串呢？你有什么好的解决思路呢？相应的时间空间复杂度又是多少呢？🔖

> “数组简单易用，在实现上使用的是连续的内存空间，可以借助 CPU 的缓存机制，预读数组中的数据，所以访问效率更高。而链表在内存中并不是连续存储，所以对 CPU 缓存不友好，没办法有效预读。” 
> 
> 这里的**CPU缓存机制**指的是什么？为什么就数组更好了？

CPU在从内存读取数据的时候，会先把读取到的数据加载到CPU的缓存中。而CPU每次从内存读取数据并不是只读取那个特定要访问的地址，而是读取一个数据块并保存到CPU缓存中，然后下次访问内存数据的时候就会先从CPU缓存开始查找，如果找到就不需要再从内存中取。

这样就实现了比内存访问速度更快的机制，也就是CPU缓存存在的意义:<u>为了弥补内存访问速度过慢与CPU执行速度快之间的差异而引入</u>。

 对于数组来说，存储空间是连续的，所以在加载某个下标的时候可以把以后的几个下标元素也加载到CPU缓存这样执行速度会快于存储空间不连续的链表存储。



## 07 链表（下）：如何轻松写出正确的链表代码？

复杂的链表操作，比如**==链表反转、有序链表合并==**等。

> 自己有决心并且付出精力是成功的先决条件，除此之外，我们还需要一些方法和技巧。

### 技巧一：理解指针或引用的含义

有些语言有“指针”的概念，比如C语言；有些语言没有指针，取而代之的是“引用”，比如 Java、Python。不管是“**指针**”还是“**引用**”，实际上，它们的意思都是一样的，都是存储所指**==对象的内存地址==**。

> 将某个变量赋值给指针，实际上就是将这个变量的地址赋值给指针，或者反过来说，指针中存储了这个变量的内存地址，指向了这个变量，通过指针就能找到这个变量。

`p->next = q`：p结点中的next指针存储了q结点的内存地址。

`p->next = p->next->next`：p结点的next指针存储了p结点的下下一个结点的内存地址。

### 技巧二：警惕指针丢失和内存泄漏

以单链表的插入操作为例：

![](images/SJJG+SFZM-07-01.jpg)

插入结点时，一定要注意操作的顺序。要先将结点 x 的 next 指针指向结点 b，再把结点 a 的 next 指针指向结点 x，这样才不会丢失指针，导致内存泄漏。

```java
x->next = p->next;  // 将x的结点的next指针指向b结点；
p->next = x;  // 将p的next指针指向x结点；
```

删除链表结点时，也一定要记得手动释放内存空间。

### 技巧三：利用哨兵简化实现难度

上面的插入操作，也就是：

```java
new_node->next = p->next;
p->next = new_node;
```

如果向一个空链表中插入第一个结点，就不一样，需要：

```java
if (head == null) {
  head = new_node;
}
```

单链表结点删除操作为：

```java
p->next = p->next->next;
```

删除链表中的最后一个结点，也要特殊处理：

```java
if (head->next == null) {
   head = null;
}
```

总的来说，**针对链表的插入、删除操作，需要对插入第一个结点和删除最后一个结点的情况进行特殊处理**。这些代码比较繁琐。

哨兵，解决的是国家之间的边界问题。同理，这里说的哨兵也是解决“边**界问题**”的，不直接参与业务逻辑。

如果我们引入哨兵结点，在任何时候，不管链表是不是空，head 指针都会一直指向这个哨兵结点。我们也把这种有哨兵结点的链表叫**带头链表**。相反，没有哨兵结点的链表就叫作**不带头链表**。

哨兵结点是不存储数据的。因为哨兵结点一直存在，所以插入第一个结点和插入其他结点，删除最后一个结点和删除其他结点，都可以统一为相同的代码实现逻辑了。

![](images/SJJG+SFZM-07-02.jpg)

代码一：

```c
// 在数组a中，查找key，返回key所在的位置
// 其中，n表示数组a的长度
int find(char* a, int n, char key) {
  // 边界条件处理，如果a为空，或者n<=0，说明数组中没有数据，就不用while循环比较了
  if(a == null || n <= 0) {
    return -1;
  }

  int i = 0;
  // 这里有两个比较操作：i<n和a[i]==key.
  while (i < n) {
    if (a[i] == key) {
      return i;
    }
    ++i;
  }

  return -1;
}
```

代码二：

```c
// 在数组a中，查找key，返回key所在的位置
// 其中，n表示数组a的长度
// 我举2个例子，你可以拿例子走一下代码
// a = {4, 2, 3, 5, 9, 6}  n=6 key = 7
// a = {4, 2, 3, 5, 9, 6}  n=6 key = 6
int find(char* a, int n, char key) {
  if(a == null || n <= 0) {
    return -1;
  }

  // 这里因为要将a[n-1]的值替换成key，所以要特殊处理这个值
  if (a[n-1] == key) {
    return n-1;
  }

  // 把a[n-1]的值临时保存在变量tmp中，以便之后恢复。tmp=6。
  // 之所以这样做的目的是：希望find()代码不要改变a数组中的内容
  char tmp = a[n-1];
  // 把key的值放到a[n-1]中，此时a = {4, 2, 3, 5, 9, 7}
  a[n-1] = key;

  int i = 0;
  // while 循环比起代码一，少了i<n这个比较操作
  while (a[i] != key) {
    ++i;
  }

  // 恢复a[n-1]原来的值,此时a= {4, 2, 3, 5, 9, 6}
  a[n-1] = tmp;

  if (i == n-1) {
    // 如果i == n-1说明，在0...n-2之间都没有key，所以返回-1
    return -1;
  } else {
    // 否则，返回i，就是等于key值的元素的下标
    return i;
  }
}
```

🔖

#### 技巧四：重点留意边界条件处理

> 软件开发中，代码在一些边界或者异常情况下，最容易产生 Bug。

经常用来检查链表代码是否正确的边界条件有这样几个：

- 如果链表为空时，代码是否能正常工作？
- 如果链表只包含一个结点时，代码是否能正常工作？
- 如果链表只包含两个结点时，代码是否能正常工作？
- 代码逻辑在处理头结点和尾结点的时候，是否能正常工作？

### 技巧五：举例画图，辅助思考

例如往单链表中插入一个数据这样一个操作：

![](images/SJJG+SFZM-07-03.jpg)

### 技巧六：多写多练，没有捷径

5个常见的链表操作：🔖🔖

- 单链表反转
- 链表中环的检测
- 两个有序的链表合并
- 删除链表倒数第 n 个结点
- 求链表的中间结点

### 小结

**写链表代码是最考验逻辑思维能力的**。因为，链表代码到处都是指针的操作、边界条件的处理，稍有不慎就容易产生 Bug。链表代码写得好坏，可以看出一个人写代码是否够细心，考虑问题是否全面，思维是否缜密。所以，这也是很多面试官喜欢让人手写链表代码的原因。

### 思考题

> 有哪些其他场景，利用哨兵可以大大地简化编码难度？



## 08 栈：如何实现浏览器的前进和后退功能？

### 如何理解“栈”？

后进者先出，先进者后出，这就是典型的“栈”结构。

![](images/SJJG+SFZM-08-01.jpg)

栈是**一种“操作受限”的线性表**，只允许在一端插入和删除数据。

从功能上来说，数组或链表确实可以替代栈，但==特定的数据结构是对特定场景的抽象==，而且，数组或链表暴露了太多的操作接口，操作上的确灵活自由，但使用时就比较不可控，自然也就更容易出错。

> 当某个数据集合只涉及在一端插入和删除数据，并且满足后进先出、先进后出的特性，这时就应该首选“栈”这种数据结构。

### 如何实现一个“栈”？

栈既可以用数组来实现（==顺序栈==），也可以用链表来实现（==链式栈==）。

### 支持动态扩容的顺序栈

![](images/SJJG+SFZM-08-02.jpg)

![](images/SJJG+SFZM-08-03.jpg)

### 栈在函数调用中的应用

```java
int main() {
   int a = 1; 
   int ret = 0;
   int res = 0;
   ret = add(3, 5);
   res = a + ret;
   printf("%d", res);
   reuturn 0;
}

int add(int x, int y) {
   int sum = 0;
   sum = x + y;
   return sum;
}
```

![](images/SJJG+SFZM-08-04.jpg)

### 栈在表达式求值中的应用

编译器如何利用栈来实现表达式求值?

```
34+13*9+44-12/3
```

编译器就是通过两个栈来实现的:

1. 一个保存操作数的栈
2. 另一个是保存运算符的栈。

从左向右遍历表达式，当遇到数字，我们就直接压入操作数栈；当遇到运算符，就与运算符栈的栈顶元素进行比较。如果比运算符栈顶元素的优先级高，就将当前运算符压入栈；如果比运算符栈顶元素的优先级低或者相同，从运算符栈中取栈顶运算符，从操作数栈的栈顶取 2 个操作数，然后进行计算，再把计算完的结果压入操作数栈，继续比较。

![](images/SJJG+SFZM-08-05.jpg)

### 栈在括号匹配中的应用

借助栈来检查表达式中的括号是否匹配。

用栈来保存未匹配的左括号，从左到右依次扫描字符串。当扫描到左括号时，则将其压入栈中；当扫描到右括号时，从栈顶取出一个左括号。如果能够匹配，比如“(”跟“)”匹配，“[”跟“]”匹配，“{”跟“}”匹配，则继续扫描剩下的字符串。如果扫描的过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为非法格式。当所有的括号都扫描完成之后，如果栈为空，则说明字符串为合法格式；否则，说明有未匹配的左括号，为非法格式。

### 思考题

> 1 如何实现浏览器的前进、后退功能？

使用两个栈，X 和 Y，把首次浏览的页面依次压入栈 X，当点击后退按钮时，再依次从栈 X 中出栈，并将出栈的数据依次放入栈 Y。当我们点击前进按钮时，我们依次从栈 Y 中取出数据，放入栈 X 中。当栈 X 中没有数据时，那就说明没有页面可以继续后退浏览了。当栈 Y 中没有数据，那就说明没有页面可以点击前进按钮浏览了。

![](images/SJJG+SFZM-08-06.jpg)

![](images/SJJG+SFZM-08-07.jpg)

![](images/SJJG+SFZM-08-08.jpg)

这个时候，你通过页面 b 又跳转到新的页面 d 了，页面 c 就无法再通过前进、后退按钮重复查看了，所以需要清空栈 Y。此时两个栈的数据这个样子：

![](images/SJJG+SFZM-08-09.jpg)

> 2 为什么函数调用要用“栈”来保存临时变量呢？用其他数据结构不行吗？



### 补充：内存中的堆栈和数据结构中堆栈的区别

#### 内存中的堆栈

内存空间在逻辑上分为三部分：代码区、静态数据区和动态数据区，动态数据区又分为栈区和堆区。

代码区：存储方法体的二进制代码。高级调度（作业调度）、中级调度（内存调度）、低级调度（进程调度）控制代码区执行代码的切换。

静态数据区：存储全局变量、静态变量、常量，常量包括final修饰的常量和String常量。系统自动分配和回收。

栈区：存储运行方法的形参、局部变量、返回值。由系统自动分配和回收。

#### 数据结构中的堆栈

- 栈：是一种连续存储的数据结构，特点是存储的数据先进后出。

- 堆：是一棵完全二叉树结构，特点是父节点的值大于（小于）两个子节点的值（分别称为大顶堆和小顶堆）。它常用于管理算法执行过程中的信息，应用场景包括堆排序，优先队列等。

- 在数据结构中，有时”堆栈“，这是堆是动词，表示把数据往栈里放。



## 09 队列：队列在线程池等有限资源池中的应用

CPU资源是有限的，任务的处理速度与线程个数并不是线性正相关。相反，过多的线程反而会导致CPU频繁切换，处理性能下降。所以，线程池的大小一般都是综合考虑要处理**任务的特点和硬件环境**，来事先设置的。

当我们向固定大小的线程池中请求一个线程时，如果线程池中没有空闲资源了，这个时候线程池如何处理这个请求？是拒绝请求还是排队请求？各种处理策略又是怎么实现的呢？

### 如何理解“队列”？

先进者先出，这就是典型的==“队列”（queue）==。

![](images/SJJG+SFZM-09-01.jpg)

栈只支持两个基本操作：**入栈 push()，出栈 pop()**。

队列最基本的两个操作：**入队 enqueue()，出队 dequeue()**。

队列跟栈一样，也是一种==操作受限的线性表数据结构==。

作为一种非常基础的数据结构，队列的应用也非常广泛，特别是一些具有某些额外特性的队列，比如循环队列、阻塞队列、并发队列。它们在很多偏底层系统、框架、中间件的开发中，起着关键性的作用。比如[高性能队列 Disruptor](https://github.com/LMAX-Exchange/disruptor)、Linux 环形缓存，都用到了循环并发队列；Java concurrent 并发包利用 `ArrayBlockingQueue` 来实现公平锁等。

### 顺序队列和链式队列

跟栈一样，队列可以用数组来实现（==顺序队列==），也可以用链表来实现（==链式队列==）。

栈只需要一个==栈顶指针==。但是队列需要两个：一个是 ==head 指针==，指向队头；一个是 ==tail 指针==，指向队尾。

```java
public class ArrayQueue {

    private String[] items;
    /**
     * 数组大小
     */
    private int n = 0;
    /**
     * 队头下标
     */
    private int head = 0;
    /**
     * 队尾下标
     */
    private int tail = 0;

    /**
     * 申请一个大小为capacity的数组
     */
    public ArrayQueue(int capacity) {
        items = new String[capacity];
        n = capacity;
    }
    /**
     * 入队
     */
    public boolean enqueue(String item) {
        if (tail == n) {
            return false;
        }
        items[tail] = item;
        ++tail;
        return true;
    }
    /**
     * 出队
     */
    public String  dequeue() {
        if (head == tail) {
            return null;
        }
        String ret = items[head];
        ++head;
        return ret;
    }
}
```

当 a、b、c、d 依次入队之后，队列中的 head 指针指向下标为 0 的位置，tail 指针指向下标为 4 的位置：

![](images/SJJG+SFZM-09-02.jpg)

当调用两次出队操作之后，队列中 head 指针指向下标为 2 的位置，tail 指针仍然指向下标为 4 的位置：

![](images/SJJG+SFZM-09-03.jpg)

为了避免每一次出队、入队操作都进行**数据搬移**，在入队时进行判断，如果队尾没有空间了，再触发一次数据搬移，改进入队操作：

```java
    public boolean enqueue(String item) {
        // tail为n时表示队尾没有空间了
        if (tail == n) {
            // head为0同时tail为n，表示队列满了
            if (head == 0 ){
                return false;
            }
            // 队前有空间，进行数据搬移
            for (int i = head; i < tail; ++i) {
                items[i - head] = items[i];
            }
            // 搬移完之后，更新head和tail
            tail -= head;
            head = 0;
        }
        items[tail] = item;
        ++tail;
        return true;
    }
```

代码中，当队列的 tail 指针移动到数组的最右边后，如果有新的数据入队，我们可以将 head 到 tail 之间的数据，整体搬移到数组中 0 到 tail-head 的位置。

![](images/094ba7722eeec46ead58b40c097353c7.jpg)



基于链表的实现：入队时，`tail->next = new_node`, `tail = tail->next`；出队时，`head = head->next`。

![](images/SJJG+SFZM-09-04.jpg)

### 循环队列

刚才用数组来实现队列的时候，在 tail==n 时，会有数据搬移操作，这样入队操作性能就会受到影响。

原本数组是有头有尾的，是一条直线。现在我们把首尾相连，扳成了一个环。

![](images/SJJG+SFZM-09-06.jpg)

图中这个队列的大小为 8，当前 head=4，tail=7。当有一个新的元素 a 入队时，我们放入下标为 7 的位置。但这个时候，我们并不把 tail 更新为 8，而是将其在环中后移一位，到下标为 0 的位置。当再有一个元素 b 入队时，我们将 b 放入下标为 0 的位置，然后 tail 加 1 更新为 1。所以，在 a，b 依次入队之后，循环队列中的元素就变成了下面的样子：

![](images/SJJG+SFZM-09-07.jpg)

这样就避免了数据搬移操作。循环队列的代码实现比较复杂，最关键的是，==确定好队空和队满的判定条件==。

在用数组实现的非循环队列中，队满的判断条件是 tail == n，队空的判断条件是 head == tail。

循环队列为空的判断条件仍然是 head == tail。但队列满的判断条件就稍微有点复杂了。

![](images/SJJG+SFZM-09-08.jpg)

tail=3，head=4，n=8，所以总结一下规律就是：(3+1)%8=4。多画几张队满的图，你就会发现，当队满时，**==(tail+1)%n=head==**。

当队列满时，图中的 tail 指向的位置实际上是没有存储数据的。所以，**循环队列会浪费一个数组的存储空间**。

```java
public class CircularQueue {
  // 数组：items，数组大小：n
  private String[] items;
  private int n = 0;
  // head表示队头下标，tail表示队尾下标
  private int head = 0;
  private int tail = 0;

  // 申请一个大小为capacity的数组
  public CircularQueue(int capacity) {
    items = new String[capacity];
    n = capacity;
  }

  // 入队
  public boolean enqueue(String item) {
    // 队列满了
    if ((tail + 1) % n == head) return false;
    items[tail] = item;
    tail = (tail + 1) % n;
    return true;
  }

  // 出队
  public String dequeue() {
    // 如果head == tail 表示队列为空
    if (head == tail) return null;
    String ret = items[head];
    head = (head + 1) % n;
    return ret;
  }
}
```



### 阻塞队列和并发队列

平时的业务开发不大可能从零实现一个队列，甚至都不会直接用到。而一些具有特殊特性的队列应用却比较广泛，比如阻塞队列和并发队列。

==阻塞队列==就是在队列为空的时候，从队头取数据会被阻塞。因为此时还没有数据可取，直到队列中有了数据才能返回；如果队列已经满了，那么插入数据的操作就会被阻塞，直到队列中有空闲位置后再插入数据，然后再返回。

![](images/SJJG+SFZM-09-09.jpg)

可以使用可以使用阻塞队列实现一个==生产者-消费者模型==，它可以有效地协调生产和消费的速度。当“生产者”生产数据的速度过快，“消费者”来不及消费时，存储数据的队列很快就会满了。这个时候，生产者就阻塞等待，直到“消费者”消费了数据，“生产者”才会被唤醒继续“生产”。

基于阻塞队列，还可以通过协调“生产者”和“消费者”的个数，来提高数据的处理效率。比如前面的例子，我们可以多配置几个“消费者”，来应对一个“生产者”。

![](images/9f539cc0f1edc20e7fa6559193898067.jpg)

在多线程情况下，会有多个线程同时操作队列，这个时候就会存在线程安全问题，那如何实现一个线程安全的队列呢？

线程安全的队列叫作==并发队列==。

最简单直接的实现方式是直接在 enqueue()、dequeue() 方法上加锁，但是锁粒度大并发度会比较低，同一时刻仅允许一个存或者取操作。实际上，基于数组的循环队列，利用 CAS 原子操作，可以实现非常高效的并发队列。这也是循环队列比链式队列应用更加广泛的原因。在实战篇讲 Disruptor 的时候，再详细讲并发队列的应用。

### 小结

![](images/image-20231118210220122.png)

### 思考题

> 线程池没有空闲线程时，新的任务请求线程资源时，线程池该如何处理？各种处理策略又是如何实现的呢？

一般有两种处理策略。

- 第一种是非阻塞的处理方式，直接拒绝任务请求；
- 另一种是阻塞的处理方式，将请求排队，等到有空闲线程时，取出排队的请求继续处理。

那如何存储排队的请求呢？

希望公平地处理每个排队的请求，先进者先服务，所以队列这种数据结构很适合来存储排队请求。我们前面说过，队列有基于链表和基于数组这两种实现方式。这两种实现方式对于排队请求又有什么区别呢？

基于链表的实现方式，可以实现一个支持无限排队的无界队列（unbounded queue），但是可能会导致过多的请求排队等待，请求处理的响应时间过长。所以，针对响应时间比较敏感的系统，基于链表实现的无限排队的线程池是不合适的。

而基于数组实现的有界队列（bounded queue），队列的大小有限，所以线程池中排队的请求超过队列大小时，接下来的请求就会被拒绝，这种方式对响应时间敏感的系统来说，就相对更加合理。不过，设置一个合理的队列大小，也是非常有讲究的。队列太大导致等待的请求太多，队列太小会导致无法充分利用系统资源、发挥最大性能。

除了前面讲到队列应用在线程池请求排队的场景之外，队列可以应用在任何==有限资源池==中，用于排队请求，比如==数据库连接池==等。实际上，对于大部分资源有限的场景，当没有空闲资源时，基本上都可以通过“队列”这种数据结构来实现请求排队。

> 除了线程池这种池结构会用到队列排队请求，你还知道有哪些类似的池结构或者场景中会用到队列的排队请求呢？

分布式消息队列，如 kafka 也是一种队列。

> 今天讲到并发队列，关于如何实现无锁并发队列，网上有非常多的讨论。对这个问题，你怎么看呢？

可以使用 cas + 数组的方式实现。

考虑使用CAS实现无锁队列，则在入队前，获取tail位置，入队时比较tail是否发生变化，如果否，则允许入队，反之，本次入队失败。出队则是获取head位置，进行cas。





## 10 递归：如何用三行代码找到“最终推荐人”？

> 推荐注册返佣金：用户 A 推荐用户 B 来注册，用户 B 又推荐了用户 C 来注册。我们可以说，用户 C 的“最终推荐人”为用户 A，用户 B 的“最终推荐人”也为用户 A，而用户 A 没有“最终推荐人”。
> 
> <u>给定一个用户 ID，如何查找这个用户的“最终推荐人”？</u>

### 如何理解“递归”？

==递归（Recursion）==是一种应用非常广泛的算法（或者编程技巧）。很多数据结构和算法的编码实现都要用到递归，比如 <u>DFS 深度优先搜索、前中后序二叉树遍历</u>等等。

==去的过程叫“递”，回来的过程叫“归”==。基本上，所有的递归问题都可以用递推公式来表示。

递推公式：

```
f(n)=f(n-1)+1     其中，f(1)=1
```

### 递归需要满足的三个条件

#### 1 一个问题的解可以分解为几个子问题的解

子问题就是数据规模更小的问题

#### 2 这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样

#### 3 存在递归终止条件



### 如何编写递归代码？

写递归代码最关键的是**==写出递推公式，找到终止条件==**。

> 写递归代码的关键就是找到==如何将大问题分解为小问题的规律==，并且基于此写出递推公式，然后再推敲终止条件，最后将递推公式和终止条件翻译成代码。

计算机擅长做重复的事情，所以递归正合它的胃口。而我们人脑更喜欢==平铺直叙的思维方式==。当我们看到递归时，我们总想把递归平铺展开，脑子里就会循环，一层一层往下调，然后再一层一层返回，试图想搞清楚计算机每一步都是怎么执行的，这样就很容易被绕进去。

对于递归代码真确的思维方式是：**如果一个问题 A 可以分解为若干子问题 B、C、D，你可以假设子问题 B、C、D 已经解决，在此基础上思考如何解决问题 A。**而且，你只需要思考问题 A 与子问题 B、C、D 两层之间的关系即可，不需要一层一层往下思考子问题与子子问题，子子问题与子子子问题之间的关系。屏蔽掉递归细节，这样子理解起来就简单多了。

因此，**编写递归代码的关键是，只要遇到递归，我们就把它抽象成一个递推公式，不用想一层层的调用关系，==不要试图用人脑去分解递归的每个步骤==。**



### 递归代码要警惕堆栈溢出

> 为什么递归代码容易造成堆栈溢出呢？我们又该如何预防堆栈溢出呢？

通过在代码中限制递归调用的==最大深度==的方式来解决。

因为最大允许的递归深度跟当前线程剩余的栈空间大小有关，事先无法计算。如果实时计算，代码过于复杂，就会影响代码的可读性。所以，如果最大深度比较小，比如 10、50，就可以用这种方法，否则这种方法并不是很实用。

### 递归代码要警惕重复计算

`f(n) = f(n-1)+f(n-2)`：

![](images/SJJG+SFZM-10-02.jpg)

想要计算 f(5)，需要先计算 f(4) 和 f(3)，而计算 f(4) 还需要计算 f(3)，因此，f(3) 就被计算了很多次，这就是重复计算问题。

可以通过一个数据结构（比如散列表）来保存已经求解过的 f(k)。当递归调用到 f(k) 时，先看下是否已经求解过了。如果是，则直接从散列表中取值返回，不需要重复计算，这样就能避免刚讲的问题了。

```java
public int f(int n) {
  if (n == 1) return 1;
  if (n == 2) return 2;
  
  // hasSolvedList可以理解成一个Map，key是n，value是f(n)
  if (hasSolvedList.containsKey(n)) {
    return hasSolvedList.get(n);
  }
  
  int ret = f(n-1) + f(n-2);
  hasSolvedList.put(n, ret);
  return ret;
}
```

除了堆栈溢出、重复计算这两个常见的问题，递归代码还有很多别的问题：

在时间效率上，递归代码里多了很多函数调用，当这些函数调用的数量较大时，就会积聚成一个可观的时间成本。在空间复杂度上，因为递归调用一次就会在内存栈中保存一次**现场数据**，所以在分析递归代码空间复杂度时，需要额外考虑这部分的开销，比如我们前面讲到的电影院递归代码，空间复杂度并不是 O(1)，而是 O(n)。

### 怎么将递归代码改写为非递归代码？

递归有利有弊，利是递归代码的==表达力很强，写起来非常简洁==；而弊就是==空间复杂度高、有堆栈溢出的风险、存在重复计算、过多的函数调用会耗时较多==等问题。

```java
int f(int n) {
  int ret = 1;
  for (int i = 2; i <= n; ++i) {
    ret = ret + 1;
  }
  return ret;
}
```

```java
int f(int n) {
  if (n == 1) return 1;
  if (n == 2) return 2;
  
  int ret = 0;
  int pre = 2;
  int prepre = 1;
  for (int i = 3; i <= n; ++i) {
    ret = pre + prepre;
    prepre = pre;
    pre = ret;
  }
  return ret;
}
```

笼统地讲，所有的递归代码都可以改为这种迭代循环的非递归写法的。因为递归本身就是借助栈来实现的，只不过我们使用的栈是系统或者虚拟机本身提供的，我们没有感知罢了。如果我们自己在内存堆上实现栈，手动模拟入栈、出栈过程，这样任何递归代码都可以改写成看上去不是递归代码的样子。

### 思考题

> 如何找到“最终推荐人”？

```java
long findRootReferrerId(long actorId) {
  Long referrerId = select referrer_id from [table] where actor_id = actorId;
  if (referrerId == null) return actorId;
  return findRootReferrerId(referrerId);
}
```

两个问题：

第一，如果递归很深，可能会有堆栈溢出的问题。

第二，如果数据库里存在脏数据，我们还需要处理由此产生的无限递归问题。比如 demo 环境下数据库中，测试工程师为了方便测试，会人为地插入一些数据，就会出现脏数据。如果 A 的推荐人是 B，B 的推荐人是 C，C 的推荐人是 A，这样就会发生死循环。🔖

> 我们平时调试代码喜欢使用 IDE 的单步跟踪功能，像规模比较大、递归层次很深的递归代码，几乎无法使用这种调试方式。对于递归代码，你有什么好的调试方法呢？

调试递归: 

1. 打印日志发现，递归值。 
2. 结合条件断点进行调试。



## 11 排序（上）：为什么插入排序比冒泡排序更受欢迎？

排序算法有很多，大部分编程语言中，也都提供了排序函数。最经典、最常用的有：

冒泡排序、插入排序、选择排序、归并排序、快速排序、计数排序、基数排序、桶排序。

![](images/SJJG+SFZM-11-01.jpg)

> ==带着问题去学习，是最有效的学习方法。==
>
> 插入排序和冒泡排序的时间复杂度相同，都是 O(n^2^)，在实际的软件开发里，为什么我们更倾向于使用插入排序算法而不是冒泡排序算法呢？

### 如何分析一个“排序算法”？

学习排序算法，我们除了学习它的==算法原理、代码实现==之外，更重要的是要学会==如何评价、分析==一个排序算法。

#### 排序算法的执行效率

##### 1 最好情况、最坏情况、平均情况时间复杂度

既要给出最好情况、最坏情况、平均情况下的时间复杂度，也要说出最好、最坏时间复杂度对应的要排序的原始数据是什么样的。

为什么要区分这三种时间复杂度呢？

第一，有些排序算法会区分，为了好对比，所以我们最好都做一下区分。

第二，对于要排序的数据，有的接近有序，有的完全无序。==有序度==不同的数据，对于排序的执行时间肯定是有影响的，我们要知道排序算法在不同数据下的性能表现。

##### 2 时间复杂度的系数、常数 、低阶

时间复杂度反映的是数据规模 n 很大的时候的一个增长趋势，所以它表示的时候会忽略系数、常数、低阶。

但实际开发中，小规模排序，需要考虑系数、常熟、低阶。

##### 3 比较次数和交换（或移动）次数

基于==比较==的排序算法的执行过程，会涉及两种操作，==一种是元素比较大小，另一种是元素交换或移动==。

#### 排序算法的内存消耗

==原地排序（Sorted in place）==，就是特指空间复杂度是 O(1) 的排序算法。

#### 排序算法的稳定性

==稳定性==，如果待排序的序列中存在值==相等==的元素，经过排序之后，相等元素之间原有的先后顺序不变。

**稳定的排序算法**，**不稳定的排序算法**

> 实际开发中排序的可能不是单纯的整数，而是一组对象，需要按照对象的某个 key 来排序，比如：
> 
> 要给电商交易系统中的“订单”排序。订单有两个属性，一个是下单时间，另一个是订单金额。如果我们现在有 10 万条订单数据，我们希望按照金额从小到大对订单数据排序。对于金额相同的订单，我们希望按照下单时间从早到晚有序。
> 
> **稳定排序算法可以保持金额相同的两个对象，在排序之后的前后顺序不变。**
> 
> ![](images/SJJG+SFZM-11-02.jpg)

### 冒泡排序

冒泡排序（Bubble Sort）**只会操作相邻的两个数据**。每次冒泡操作都会对相邻的两个元素进行比较，看是否满足大小关系要求。如果不满足就让它俩互换。一次冒泡会让至少一个元素移动到它应该在的位置，重复 n 次，就完成了 n 个数据的排序工作。

一组数据 4，5，6，3，2，1，从小到大进行排序，第一次冒泡操作过程：

![](images/SJJG+SFZM-11-03.jpg)

6次冒泡：

![](images/SJJG+SFZM-11-04.jpg)

优化：当某次冒泡操作已经没有数据交换时，说明已经达到完全有序，不用再继续执行后续的冒泡操作。

![](images/SJJG+SFZM-11-05.jpg)

```java
    /**
     * 冒泡排序
     * @param a
     * @param n 数组大小
     */
    public void bubbleSort(int[] a, int n) {
        if (n <= 1) {
            return;
        }
        for (int j = 0; j < n; ++j) {
            // 提前退出冒泡循环的标志位
            boolean flag = false;
            for (int i = 0; i < n - j - 1; ++i) {
                if (a[i] > a[i + 1]) {
                    int tmp = a[i + 1];
                    a[i + 1] = a[i];
                    a[i] = tmp;
                    // 表示有数据交换
                    flag = true;
                }
            }
            // 没有数据交换提前退出
            if (!flag) {
                break;
            }
        }
    }
```

#### 第一，冒泡排序是原地排序算法吗？

是，只涉及相邻数据的交换操作，只需要常量级的临时空间

#### 第二，冒泡排序是稳定的排序算法吗？

是，只有交换才可以改变两个元素的前后顺序。为了保证冒泡排序算法的稳定性，当有相邻的两个元素大小相等的时候，我们不做交换，相同大小的数据在排序前后不会改变顺序。

#### 第三，冒泡排序的时间复杂度是多少？

最好情况是数据有序，只需一次冒泡操作；最坏情况是数据倒序，需要n次冒泡操作。

![](images/SJJG+SFZM-11-06.jpg)

平均时间复杂度就是加权平均期望时间复杂度，分析的时候要结合概率论的知识。

> 对于包含 n 个数据的数组，这 n 个数据就有 `n!` 种排列方式。不同的排列方式，冒泡排序执行的时间肯定是不同的。比如我们前面举的那两个例子，其中一个要进行 6 次冒泡，而另一个只需要 4 次。如果用概率论方法定量分析平均时间复杂度，涉及的数学推理和计算就会很复杂。我这里还有一种思路，通过“有序度”和“逆序度”这两个概念来进行分析。

==有序度==是数组中具有有序关系的元素对的个数。

![](images/SJJG+SFZM-11-07.jpg)

同理，对于一个倒序排列的数组，比如 6，5，4，3，2，1，有序度是 0；对于一个完全有序的数组，比如 1，2，3，4，5，6，有序度就是 `n*(n-1)/2`，也就是 15。我们把这种完全有序的数组的有序度叫作==满有序度==。

==逆序度==的定义正好跟有序度相反（默认从小到大为有序）。

公式：**==逆序度 = 满有序度 - 有序度==**

<u>排序的过程就是一种增加有序度，减少逆序度的过程，最后达到满有序度。</u>

![](images/SJJG+SFZM-11-08.jpg)

冒泡排序包含两个操作原子，==比较==和==交换==。每交换一次，有序度就加 1。不管算法怎么改进，交换次数总是确定的，即为逆序度，也就是`n*(n-1)/2 – 初始有序度`。此例中就是 15–3=12，要进行 12 次交换操作。

对于包含 n 个数据的数组进行冒泡排序，平均交换次数是多少呢？最坏情况下，初始状态的有序度是 0，所以要进行 `n*(n-1)/2` 次交换。最好情况下，初始状态的有序度是 `n*(n-1)/2`，就不需要进行交换。我们可以取个中间值 `n*(n-1)/4`，来表示初始有序度既不是很高也不是很低的平均情况。

换句话说，平均情况下，需要 `n*(n-1)/4` 次交换操作，比较操作肯定要比交换操作多，而复杂度的上限是 O(n^2^)，所以平均情况下的时间复杂度就是 O(n^2^)。🔖

这个平均时间复杂度推导过程其实并不严格，但是很多时候很实用，毕竟<u>概率论的定量分析太复杂，不太好用</u>。等我们讲到快排的时候，我还会再次用这种“不严格”的方法来分析平均时间复杂度。

### 插入排序（Insertion Sort）

一个有序的数组，往里面添加一个新的数据后，如何继续保持数据有序呢？

很简单，只要遍历数组，找到数据应该插入的位置将其插入即可。

![](images/SJJG+SFZM-11-09.jpg)

**动态排序的过程**，即动态地往有序集合中添加数据，可以通过这种方法保持集合中的数据一直有序。

首先，我们将数组中的数据分为两个区间，==已排序区间==和==未排序区间==。初始已排序区间只有一个元素，就是数组的第一个元素。插入算法的核心思想是==取未排序区间中的元素，在已排序区间中找到合适的插入位置将其插入，并保证已排序区间数据一直有序==。重复这个过程，直到未排序区间中元素为空，算法结束。

![](images/SJJG+SFZM-11-10.jpg)

插入排序也包含两种操作，一种是**元素的比较**，一种是**元素的移动**。当我们需要将一个数据 a 插入到已排序区间时，需要拿 a 与已排序区间的元素依次比较大小，找到合适的插入位置。找到插入点之后，我们还需要将插入点之后的元素顺序往后移动一位，这样才能腾出位置给元素 a 插入。

对于不同的查找插入点方法（从头到尾、从尾到头），元素的比较次数是有区别的。但对于一个给定的初始序列，==移动操作的次数总是固定的，就等于逆序度==。

为什么说移动次数就等于逆序度呢？

上面的例子满有序度是 `n*(n-1)/2=15`，初始序列的有序度是 5，所以逆序度是 10。插入排序中，数据移动的个数总和也等于 10=3+3+4。

![](images/SJJG+SFZM-11-11.jpg)

```java
    public void insertionSort(int[] a, int n) {
        if (n <= 1) {
            return;
        }
        // 初始，第一个元素作为已排序区间，后面的元素作为未排序区
        // 从未排序区第一个元素开始遍历
        for (int i = 1; i < n; ++i) {
            int value = a[i];
            int j = i - 1;
            // 查找插入的位置
            for (; j >= 0; --j) {
                if (a[j] > value) {
                    // 数据移动
                    a[j + 1] = a[j];
                } else {
                    break;
                }
            }
            // 插入数据
            a[j + 1] = value;
        }
    }
```

#### 第一，插入排序是原地排序算法吗？

是

#### 第二，插入排序是稳定的排序算法吗？

可以

#### 第三，插入排序的时间复杂度是多少？

最好情况是数据已经有序，时间复杂度为 O(n)。注意，这里是从尾到头遍历已经有序的数据。

如果数组是倒序的，每次插入都相当于在数组的第一个位置插入新的数据，所以需要移动大量的数据，所以最坏情况时间复杂度为 O(n^2^)。

对于插入排序来说，每次插入操作都相当于在数组中插入一个数据，循环执行 n 次插入操作，所以平均时间复杂度为 O(n^2^)。🔖

### 选择排序（Selection Sort）

类似插入排序，也分已排序区间和未排序区间。但是选择排序<u>每次会从未排序区间中找到最小的元素，将其放到已排序区间的末尾(未排序区间的第一个元素)</u>。

> 插入排序，是找到插入点之后，原本插入点及其之后的元素后移； 选择排序，是找到插入点之后，待插入的最小元素与原本插入点位置的元素做交换。

![](images/SJJG+SFZM-11-12.jpg)

选择排序空间复杂度为 O(1)，是一种原地排序算法，最好情况时间复杂度、最坏情况和平均情况时间复杂度都为 O(n^2^)。

选择排序是一种不稳定的排序算法。选择排序每次都要找剩余未排序元素中的最小值，并和前面的元素交换位置，这样破坏了稳定性。

比如 5，8，5，2，9 这样一组数据，使用选择排序算法来排序的话，第一次找到最小元素 2，与第一个 5 交换位置，那第一个 5 和中间的 5 顺序就变了，所以就不稳定了。正是因此，相对于冒泡排序和插入排序，选择排序就稍微逊色了。



### 小结

要想分析、评价一个排序算法，需要从执行效率、内存消耗和稳定性三个方面来看。

![](images/SJJG+SFZM-11-13.jpg)

这三种时间复杂度为 O(n^2^) 的排序算法中，冒泡排序、选择排序，可能就纯粹停留在理论的层面了，学习的目的也只是为了开拓思维，实际开发中应用并不多，但是插入排序还是挺有用的。

特定算法是依赖特定的数据结构的。这三种排序算法，都是基于数组实现的。

### 思考题

冒泡排序不管怎么优化，元素交换的次数是一个固定值，是原始数据的逆序度。插入排序是同样的，不管怎么优化，元素移动的次数也等于原始数据的逆序度。

但是，从代码实现上来看，冒泡排序的数据交换要比插入排序的数据移动要复杂，冒泡排序需要 3 个赋值操作，而插入排序只需要 1 个。

```java
冒泡排序中数据的交换操作：
if (a[j] > a[j+1]) { // 交换
   int tmp = a[j];
   a[j] = a[j+1];
   a[j+1] = tmp;
   flag = true;
}

插入排序中数据的移动操作：
if (a[j] > value) {
  a[j+1] = a[j];  // 数据移动
} else {
  break;
}
```



> 如果数据存储在链表中，这三种排序算法还能工作吗？如果能，那相应的时间、空间复杂度又是多少呢？

应该有个前提，是否允许修改链表的节点value值，还是只能改变节点的位置。一般而言，考虑只能改变节点位置，冒泡排序相比于数组实现，比较次数一致，但交换时操作更复杂；插入排序，比较次数一致，不需要再有后移操作，找到位置后可以直接插入，但排序完毕后可能需要倒置链表；选择排序比较次数一致，交换操作同样比较麻烦。综上，时间复杂度和空间复杂度并无明显变化，若追求极致性能，冒泡排序的时间复杂度系数会变大，插入排序系数会减小，选择排序无明显变化。



## 12 排序（下）：如何用快排思想在O(n)内查找第K大元素？

冒泡排序、插入排序、选择排序时间复杂度都为 O(n^2^)，适合小规模数据排序。

归并排序和快速排序时间复杂度为 O(nlogn)，适合大规模的数据排序，它们都用到了==分治思想==。

### 归并排序的原理

归并排序（Merge Sort）

如果要排序一个数组，先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组就都有序了。

![](images/SJJG+SFZM-12-01.jpg)

分治思想跟我们前面讲的递归思想很像。是的，分治算法一般都是用递归来实现的。==分治是一种解决问题的处理思想，递归是一种编程技巧==。

如何用递归代码来实现归并排序。写递归代码的技巧就是，**分析得出递推公式，然后找到终止条件，最后将递推公式翻译成递归代码**。

```
递推公式：
merge_sort(p…r) = merge(merge_sort(p…q), merge_sort(q+1…r))

终止条件：
p >= r 不用再继续分解
```

`merge_sort(p…r)` 表示，给下标从 p 到 r 之间的数组排序，下标q是p和r的中间位置。

```java
    /**
     * 归并排序
     * @param a 要排序数组
     * @param n 数组长度
     */
    public static void mergeSort(int[] a, int n) {
        merge_sort_c(a, 0, n - 1);
    }

    /**
     * 给数组a从下标p到r之间的数组进行归并排序
     * @param a
     * @param p
     * @param r
     */
    private static void merge_sort_c(int[] a, int p, int r) {
        // 递归终止条件
        if (p >= r) {
            return;
        }
        // 取p到r之间的位置q
        int q = (p + r) / 2;
        // 分治递归
        merge_sort_c(a, p, q);
        merge_sort_c(a, q + 1, r);

        // 将A[p...q]和A[q+1...r]合并为A[p...r]
        merge(a, p, q, r);
    }

    private static void merge(int[] a, int p, int q, int r) {
        int i = p;
        int j = q + 1;
        int k = 0;
        // 申请一个大小跟a[p...r]一样的临时数组
        int[] tmp = new int[r - p + 1];

        while (i <= q && j <= r) {
            if (a[i] <= a[j]) {
                tmp[k++] = a[i++];
            } else {
                tmp[k++] = a[j++];
            }
        }

        // 判断哪个子数组中有剩余的数据
        int start = i;
        int end = q;
        if (j <= r) {
            start = j;
            end = r;
        }

        // 将剩余的数据拷贝到临时数组tmp
        while (start <= end) {
            tmp[k++] = a[start++];
        }

        // 将tmp中数组拷贝回a[p...r]
        for (i = 0; i <= r - p; ++i) {
            a[p + i] = tmp[i];
        }
    }
```

`merge(a, p, q, r)` 中用两个游标 i 和 j，分别指向 A[p...q]和 A[q+1...r]的第一个元素。比较这两个元素 A[i]和 A[j]，如果 A[i]<=A[j]，我们就把 A[i]放入到临时数组 tmp，并且 i 后移一位，否则将 A[j]放入到数组 tmp，j 后移一位。

![](images/SJJG+SFZM-12-02.jpg)

### 归并排序的性能分析

#### 稳定

归并排序稳不稳定关键要看 merge() 函数。在合并的过程中，如果 A[p...q]和 A[q+1...r]之间有值相同的元素，可以先把 A[p...q]中的元素放入 tmp 数组。这样就保证了值相同的元素，在合并前后的先后顺序不变。

#### 归并排序的时间复杂度

🔖

归并排序的执行效率与要排序的原始数组的有序程度无关，所以其时间复杂度是非常稳定的，不管是最好情况、最坏情况，还是平均情况，时间复杂度都是 `O(nlogn)`。

#### 归并排序的空间复杂度

尽管每次合并操作都需要申请额外的内存空间，但在合并完成之后，临时开辟的内存空间就被释放掉了。在任意时刻，CPU 只会有一个函数在执行，也就只会有一个临时的内存空间在使用。临时内存空间最大也不会超过 n 个数据的大小，所以空间复杂度是 `O(n)`。

### 快速排序的原理

快速排序算法（Quicksort，快排），也利用分治思想，看着想归并排序，但思路完全不一样。

快排的思想：

如果要排序数组中下标从 p 到 r 之间的一组数据，我们选择 p 到 r 之间的任意一个数据作为 **pivot（分区点）**。

遍历 p 到 r 之间的数据，将小于 pivot 的放到左边，将大于 pivot 的放到右边，将 pivot 放到中间。经过这一步骤之后，数组 p 到 r 之间的数据就被分成了三个部分，前面 p 到 q-1 之间都是小于 pivot 的，中间是 pivot，后面的 q+1 到 r 之间是大于 pivot 的。

![](images/SJJG+SFZM-12-03.jpg)

根据分治、递归的处理思想，可以用递归排序下标从 p 到 q-1 之间的数据和下标从 q+1 到 r 之间的数据，直到区间缩小为 1，就说明所有的数据都有序了。

```
递推公式：
quick_sort(p…r) = quick_sort(p…q-1) + quick_sort(q+1… r)

终止条件：
p >= r
```

```java
    public static void quick_sort(int[] a, int n) {
        quick_sort_c(a, 0, n - 1);
    }

    private static void quick_sort_c(int[] a, int p, int r) {
        if (p >= r) {
            return;
        }

        int q = partition(a, p, r);
        quick_sort_c(a, p, q - 1);
        quick_sort_c(a, q + 1, r);
    }
```

分区函数partition()需要考虑使用原地排序：

```java
   /**
     * 分区函数，获取分区点
     */
    private static int partition(int[] a, int p, int r) {
        int pivot = a[r];
        int i = p;
        for (int j = p; j < r; ++j) {
            if (a[j] < pivot) {
                if (i == j) {
                    ++i;
                } else {
                    int tmp = a[i];
                    a[i++] = a[j];
                    a[j] = tmp;
                }
            }
        }
        int tmp = a[i];
        a[i] = a[r];
        a[r] = tmp;

        System.out.println("i=" + i);
        return i;
    }
```

🔖

![](images/SJJG+SFZM-12-05.jpg)

为分区的过程涉及交换操作，如果数组中有两个相同的元素，比如序列 6，8，7，6，3，5，9，4，在经过第一次分区操作之后，两个 6 的相对先后顺序就会改变。所以，**快速排序并不是一个稳定的排序算**法。

![](images/SJJG+SFZM-12-06.jpg)

归并排序的处理过程是==由下到上的==，先处理子问题，然后再合并。而快排正好相反，它的处理过程是==由上到下的==，先分区，然后再处理子问题。归并排序虽然是稳定的、时间复杂度为 O(nlogn) 的排序算法，但是它是非原地排序算法。我们前面讲过，归并之所以是非原地排序算法，主要原因是合并函数无法在原地执行。快速排序通过设计巧妙的原地分区函数，可以实现原地排序，解决了归并排序占用太多内存的问题。

### 快速排序的性能分析

快排是一种原地、不稳定的排序算法。

🔖

T(n) 在大部分情况下的时间复杂度都可以做到 O(nlogn)，只有在极端情况下，才会退化到 O(n^2^)。

### 小结

归并排序和快速排序是两种稍微复杂的排序算法，它们用的都是**分治的思想，代码都通过递归来实现**，过程非常相似。

理解归并排序的重点是理解==递推公式和 merge()合并函数==。

同理，理解快排的重点也是理解==递推公式和partition()分区函数==。

### 思考题

快排核心思想就是==分治==和==分区==。

> O(n) 时间复杂度内求无序数组中的第 K 大元素。

🔖

> 现在你有 10 个接口访问日志文件，每个日志文件大小约 300MB，每个文件里的日志都是按照时间戳从小到大排序的。你希望将这 10 个较小的日志文件，合并为 1 个日志文件，合并之后的日志仍然按照时间戳从小到大排列。如果处理上述排序任务的机器内存只有 1GB，你有什么好的解决思路，能“快速”地将这 10 个日志文件合并吗？

先构建十条io流，分别指向十个文件，每条io流读取对应文件的第一条数据，然后比较时间戳，选择出时间戳最小的那条数据，将其写入一个新的文件，然后指向该时间戳的io流读取下一行数据，然后继续刚才的操作，比较选出最小的时间戳数据，写入新文件，io流读取下一行数据，以此类推，完成文件的合并， 这种处理方式，日志文件有n个数据就要比较n次，每次比较选出一条数据来写入，时间复杂度是O（n），空间复杂度是O（1）,几乎不占用内存



## 13 线性排序：如何根据年龄给100万用户数据排序？

桶排序、计数排序、基数排序的时间复杂度是线性的（即时间复杂度是 O(n)），把这类排序算法叫作**线性排序（Linear sort）**。它们都是**非基于比较的排序算法**，都不涉及元素之间的比较操作。

重点是掌握这些排序算法的==适用场景==。

### 桶排序（Bucket sort）

核心思想是**将要排序的数据分到几个有序的桶里，每个桶里的数据再单独进行排序。桶内排完序之后，再把每个桶里的数据按照顺序依次取出，组成的序列就是有序的了**。

![](images/SJJG+SFZM-13-01.jpg)

如果要排序的数据有 n 个，我们把它们均匀地划分到 m 个桶内，每个桶里就有 k=n/m 个元素。每个桶内部使用**快速排序**，时间复杂度为 `O(k * logk)`。m 个桶排序的时间复杂度就是 O(m * k * logk)，因为 k=n/m，所以整个桶排序的时间复杂度就是 `O(n*log(n/m))`。当桶的个数 m 接近数据个数 n 时，log(n/m) 就是一个非常小的常量，这个时候桶排序的时间复杂度接近 O(n)。

**桶排序对要排序数据的要求是非常苛刻的。**

- 首先，要排序的数据需要**很容易就能划分成 m 个桶**，并且，桶与桶之间有着天然的大小顺序。这样每个桶内的数据都排序完之后，桶与桶之间的数据不需要再进行排序。
- 其次，数据在各个桶之间的分布是比较**均匀**的。如果数据经过桶的划分之后，有些桶里的数据非常多，有些非常少，很不平均，那桶内数据排序的时间复杂度就不是常量级了。在极端情况下，如果数据都被划分到一个桶里，那就退化为 O(nlogn) 的排序算法了。

桶排序比较适合用在外部排序中。所谓的==外部排序==就是数据存储在外部磁盘中，数据量比较大，内存有限，无法将数据全部加载到内存中。

> 比如说我们有 10GB 的订单数据，我们希望按订单金额（假设金额都是正整数）进行排序，但是我们的内存有限，只有几百 MB，没办法一次性把 10GB 的数据都加载到内存中。这个时候该怎么办呢？

我们可以先扫描一遍文件，看订单金额所处的数据范围。假设经过扫描之后我们得到，订单金额最小是 1 元，最大是 10 万元。我们将所有订单根据金额划分到 100 个桶里，第一个桶我们存储金额在 1 元到 1000 元之内的订单，第二桶存储金额在 1001 元到 2000 元之内的订单，以此类推。每一个桶对应一个文件，并且按照金额范围的大小顺序编号命名（00，01，02...99）。

理想的情况下，如果订单金额在 1 到 10 万之间均匀分布，那订单会被均匀划分到 100 个文件中，每个小文件中存储大约 100MB 的订单数据，我们就可以将这 100 个小文件依次放到内存中，用快排来排序。等所有文件都排好序之后，我们只需要按照文件编号，从小到大依次读取每个小文件中的订单数据，并将其写入到一个文件中，那这个文件中存储的就是按照金额从小到大排序的订单数据了。

不过，你可能也发现了，订单按照金额在 1 元到 10 万元之间并不一定是均匀分布的 ，所以 10GB 订单数据是无法均匀地被划分到 100 个文件中的。有可能某个金额区间的数据特别多，划分之后对应的文件就会很大，没法一次性读入内存。这又该怎么办呢？

针对这些划分之后还是比较大的文件，我们可以继续划分，比如，订单金额在 1 元到 1000 元之间的比较多，我们就将这个区间继续划分为 10 个小区间，1 元到 100 元，101 元到 200 元，201 元到 300 元....901 元到 1000 元。如果划分之后，101 元到 200 元之间的订单还是太多，无法一次性读入内存，那就继续再划分，<u>直到所有的文件都能读入内存为止。</u>

### 计数排序（Counting sort）

计数排序其实是**桶排序的一种特殊情况**。

当要排序的 n 个数据，所处的范围并不大的时候，比如最大值是 k，我们就可以把数据划分成 k 个桶。每个桶内的数据值都是相同的，省掉了桶内排序的时间。

> 高考查分数系统，查分数的时候，系统会显示我们的成绩以及所在省的排名。如果你所在的省有 50 万考生，如何通过成绩快速排序得出名次呢？

考生的满分是 900 分，最小是 0 分，这个数据的范围很小，所以我们可以分成 901 个桶，对应分数从 0 分到 900 分。根据考生的成绩，我们将这 50 万考生划分到这 901 个桶里。桶内的数据都是分数相同的考生，所以并不需要再进行排序。我们只需要依次扫描每个桶，将桶内的考生依次输出到一个数组中，就实现了 50 万考生的排序。因为只涉及扫描遍历操作，所以时间复杂度是 O(n)。

🔖

计数排序只能用在数据范围不大的场景中，如果数据范围 k 比要排序的数据 n 大很多，就不适合用计数排序了。而且，计数排序只能给非负整数排序，如果要排序的数据是其他类型的，要将其在不改变相对大小的情况下，转化为非负整数。

### 基数排序（Radix sort）

> 假设有 10 万个手机号码，希望将这 10 万个手机号码从小到大排序，有什么比较快速的排序方法呢？

手机号码有 11 位，范围太大，显然不适合用桶排序、计数排序这两种排序算法。

🔖

![](images/SJJG+SFZM-13-06.jpg)

基数排序对要排序的数据是有要求的，需要**可以分割出独立的“位”来比较，而且位之间有递进的关系**，如果 a 数据的高位比 b 数据大，那剩下的低位就不用比较了。除此之外，**每一位的数据范围不能太大**，要可以用线性排序算法来排序，否则，基数排序的时间复杂度就无法做到 O(n) 了。

### 小结

3种线性时间复杂度的排序算法，桶排序、计数排序、基数排序，对要排序的数据都有比较苛刻的要求，应用不是非常广泛。数据特征符合的，应用这些排序会非常高效。

桶排序和计数排序的排序思想是非常相似的，都是针对范围不大的数据，将数据划分成不同的桶来实现排序。基数排序要求数据可以划分成高低位，位之间有递进关系。比较两个数，我们只需要比较高位，高位相同的再比较低位。而且每一位的数据范围不能太大，因为基数排序算法需要借助桶排序或者计数排序来完成每一个位的排序工作。

![](images/image-20231125225847102.png)

### 思考题

> 如何根据年龄给 100 万用户排序？

根据年龄给 100 万用户排序，就类似按照成绩给 50 万考生排序。我们假设年龄的范围最小 1 岁，最大不超过 120 岁。我们可以遍历这 100 万用户，根据年龄将其划分到这 120 个桶里，然后依次顺序遍历这 120 个桶中的元素。这样就得到了按照年龄排序的 100 万用户数据。

实际上，还有很多看似是排序但又不需要使用排序算法就能处理的排序问题。

> 假设我们现在需要对 D，a，F，B，c，A，z 这个字符串进行排序，要求将其中所有小写字母都排在大写字母的前面，但小写字母内部和大写字母内部不要求有序。比如经过排序之后为 a，c，z，D，F，B，A，这个如何来实现呢？如果字符串中存储的不仅有大小写字母，还有数字。要将小写字母的放到前面，大写字母放在最后，数字放在中间，不用排序算法，又该怎么解决呢？

用两个指针a、b：a指针从头开始往后遍历，遇到大写字母就停下，b从后往前遍历，遇到小写字母就停下，交换a、b指针对应的元素；重复如上过程，直到a、b指针相交。 对于小写字母放前面，数字放中间，大写字母放后面，可以先将数据分为小写字母和非小写字母两大类，进行如上交换后再在非小写字母区间内分为数字和大写字母做同样处理



## 14 排序优化：如何实现一个通用的、高性能的排序函数？

几乎所有的编程语言都会提供排序函数，比如 C 语言中 `qsort()`，C++ STL 中的 `sort()`、`stable_sort()`，还有 Java 语言中的 `Collections.sort()`（`Arrays.mergeSort()`）。

### 如何选择合适的排序算法？

![](images/SJJG+SFZM-14-01.jpg)

Java 语言采用堆排序实现排序函数，C 语言使用快速排序实现排序函数。

快速排序比较适合来实现排序函数，但是，快速排序在最坏情况下的时间复杂度是 O(n2)，如何来解决这个“复杂度恶化”的问题呢？

### 如何优化快速排序？🔖

这种 O(n^2^) 时间复杂度出现的主要原因还是因为我们==分区点选得不够合理==。

最理想的分区点是：**被分区点分开的两个分区中，数据的数量差不多。**

#### 1.三数取中法

#### 2.随机法

### 举例分析排序函数

C语言的Glibc中qsort()函数。

qsort() 会优先使用归并排序来排序输入数据，要排序的数据量比较大的时候，qsort() 会改为用快速排序算法来排序。

### 思考 🔖

查看了下Arrays.sort的源码，主要采用`TimSort`算法, 大致思路是这样的： 

1 元素个数 < 32, 采用二分查找插入排序(Binary Sort) 

2 元素个数 >= 32, 采用归并排序，归并的核心是分区(Run) 

3 找连续升或降的序列作为分区，分区最终被调整为升序后压入栈 

4 如果分区长度太小，通过二分插入排序扩充分区长度到分区最小阙值 

5 每次压入栈，都要检查栈内已存在的分区是否满足合并条件，满足则进行合并 

6 最终栈内的分区被全部合并，得到一个排序好的数组 

Timsort的合并算法非常巧妙： 

1 找出左分区最后一个元素(最大)及在右分区的位置 

2 找出右分区第一个元素(最小)及在左分区的位置 

3 仅对这两个位置之间的元素进行合并，之外的元素本身就是有序的

`DualPivotQuicksort`





## 15 二分查找（上）：如何用最省内存的方式实现快速查找功能？

二分查找（Binary Search）算法，也叫折半查找算法。（针对有序数据集合）

### 无处不在的二分思想

> 我随机写一个 0 到 99 之间的数字，然后你来猜我写的是什么。猜的过程中，你每猜一次，我就会告诉你猜的大了还是小了，直到猜中为止。你来想想，如何快速猜中我写的数字呢？（<u>如果猜测范围的数字有偶数个，中间数有两个，就选择较小的那个。</u>）

假设写的数字的23，过程大概为：

![](./images/SJJG+SFZM-15-01.jpg)

7次猜出来了。

> 实际的开发场景中。假设有 1000 条订单数据，已经按照订单金额从小到大排序，每个订单金额都不同，并且最小单位是元。我们现在想知道是否存在金额等于 19 元的订单。如果存在，则返回订单数据，如果不存在则返回 null。
> 
> 简化方便说明。假设有 10 个订单，订单金额分别是：8，11，19，23，27，33，45，55，67，98（排过序），查找是否有19的订单。

low 和 high 表示待查找区间的下标，mid 表示待查找区间的中间元素下标。

![](./images/SJJG+SFZM-15-02.jpg)

二分查找针对的是一个==有序==的数据集合，查找思想有点类似分治思想。**每次都通过跟区间的中间元素对比，将待查找的区间缩小为之前的一半，直到找到要查找的元素，或者区间被缩小为 0**。

### O(logn) 惊人的查找速度

假设数据大小是 n，每次查找后数据都会缩小为原来的一半，也就是会除以 2。最坏情况下，直到查找区间被缩小为空，才停止。

![](./images/SJJG+SFZM-15-03.jpg)

这是一个等比数列。其中 n/2^k^=1 时，k 的值就是总共缩小的次数。而每一次缩小操作只涉及两个数据的大小比较，所以，经过了 k 次区间缩小操作，时间复杂度就是 O(k)。通过 n/2^k^=1，我们可以求得 k=log~2~n，所以时间复杂度就是 O(logn)。

==对数时间复杂度==

logn 是一个非常“恐怖”的数量级，即便 n 非常非常大，对应的 logn 也很小。比如 n 等于 2 的 32 次方，这个数很大了吧？大约是 42 亿。也就是说，如果我们在 42 亿个数据中用二分查找一个数据，最多需要比较 32 次。

O(1) 有可能表示的是一个非常大的常量值，比如 O(1000)、O(10000)。所以，**常量级时间复杂度的算法有时候可能还没有 O(logn) 的算法执行效率高**。

### 二分查找的递归与非递归实现

==最简单==的情况就是有序数组中不存在==重复==元素。

```java
// 循环实现
public int bsearch(int[] a, int n, int value) {
  int low = 0;
  int high = n - 1;

  while (low <= high) {
//      int mid = (low + high) / 2;
    int mid = low + ((high - low) >> 1); // 改进
    if (a[mid] == value) {
      return mid;
    } else if (a[mid] < value) {
      low = mid + 1;
    } else {
      high = mid - 1;
    }
  }
  return -1;
}
```

容易出错的地方：

1. ==循环退出条件==，是 low<=high，而不是 low<high

2. ==mid的取值==
   
   为了防止溢出和提升效率，改进为：`low + ((high - low) >> 1)`

3. ==low和high的更新==

```java
// 二分查找的递归实现
public int bsearch2(int a[], int n, int value) {
  return bsearchInternally(a, 0, n - 1, value);
}

private int bsearchInternally(int[] a, int low, int high, int value) {
  if (low > high) {
    return -1;
  }
  int mid = low + ((high - low) >> 1);
  if (a[mid] == value) {
    return mid;
  } else if (a[mid] < value) {
    return bsearchInternally(a, a[mid + 1], high, value);
  } else {
    return bsearchInternally(a, low, a[mid - 1], value);
  }
```

实际上，二分查找除了用循环来实现，还可以用递归来实现。

```java
// 二分查找的递归实现
public int bsearch(int[] a, int n, int val) {
  return bsearchInternally(a, 0, n - 1, val);
}

private int bsearchInternally(int[] a, int low, int high, int value) {
  if (low > high) return -1;

  int mid =  low + ((high - low) >> 1);
  if (a[mid] == value) {
    return mid;
  } else if (a[mid] < value) {
    return bsearchInternally(a, mid+1, high, value);
  } else {
    return bsearchInternally(a, low, mid-1, value);
  }
}
```



### 二分查找应用场景的局限性

- 首先，二分查找依赖的是==顺序==表结构，简单点说就是数组。(需要按照下标随机访问元素)

- 其次，二分查找针对的是==有序==数据。
  
  二分查找只能用在插入、删除操作不频繁（静态的数据），一次排序多次查找的场景中。针对动态变化的数据集合，二分查找将不再适用（动态的是使用二叉树）。

- 再次，数据量太小不适合二分查找。
  
  > 例外：如果数据之间的比较操作非常耗时，不管数据量大小，我都推荐使用二分查找。比如，数组中存储的都是长度超过 300 的字符串，如此长的两个字符串之间比对大小，就会非常耗时。我们需要尽可能地减少比较次数，而比较次数的减少会大大提高性能，这个时候二分查找就比顺序遍历更有优势。

- 最后，数据量太大也不适合二分查找。
  
  数组为了支持随机访问的特性，要求内存空间连续，对内存的要求比较苛刻。

### 小结

二分查找虽然性能比较优秀，但应用场景也比较有限。底层必须依赖数组，并且还要求数据是有序的。对于较小规模的数据查找，我们直接使用顺序遍历就可以了，二分查找的优势并不明显。二分查找更适合处理**静态数据**，也就是没有频繁的数据插入、删除操作。

### 思考题

> 假设我们有 1000 万个整数数据，每个数据占 8 个字节，如何设计数据结构和算法，快速判断某个整数是否出现在这 1000 万数据中？ 我们希望这个功能不要占用太多的内存空间，最多不要超过 100MB。

内存限制是 100MB，每个数据大小是 8 字节，最简单的办法就是将数据存储在数组中，内存占用差不多是 80MB，符合内存的限制。

可以先对这 1000 万数据从小到大排序，然后再利用二分查找算法，就可以快速地查找想要的数据了。

虽然大部分情况下，用二分查找可以解决的问题，用散列表、二叉树都可以解决。但散列表和二叉树，都需要比较多的额外的内存空间，而二分查找底层依赖的是数组，除了数据本身之外，不需要额外存储其他信息，是最省内存空间的存储方式，所以刚好能在限定的内存大小下解决这个问题。

> 如何编程实现“求一个数的平方根”？要求精确到小数点后 6 位。





> 如果数据使用链表存储，二分查找的时间复杂就会变得很高，那查找的时间复杂度究竟是多少呢？如果你自己推导一下，你就会深刻地认识到，为何我们会选择用数组而不是链表来实现二分查找了。



## 16 二分查找（下）：如何快速定位IP对应的省份地址？🔖

二分查找虽然原理极其简单，但是想要写出没有 Bug 的二分查找并不容易。

> 唐纳德·克努特（Donald E.Knuth）在《计算机程序设计艺术》的第 3 卷《排序和查找》中说到：<u>“尽管第一个二分查找算法于 1946 年出现，然而第一个完全正确的二分查找算法实现直到 1962 年才出现。”</u>

4中常见的二分查找变形问题：

### 查找第一个值等于给定值的元素

### 查找最后一个值等于给定值的元素

### 查找第一个大于等于给定值的元素

### 查找最后一个小于等于给定值的元素





## 17 跳表：为什么Redis一定要用跳表来实现有序集合？

二分查找底层依赖的是==数组随机访问==的特性，所以只能用数组来实现。

如果数据存储在链表中，对链表稍加改造，就可以支持类似“二分”的查找算法。这个改造之后的数据结构叫做**==跳表（Skip list）==**。【可以理解为一种可以进行二分查找的有序链表】

跳表是一种各方面性能都比较优秀的**==动态数据结构==**，可以支持快速地插入、删除、查找操作，写起来也不复杂，甚至可以替代红黑树（Red-black tree）。

### 如何理解“跳表”？

单链表，即便数据是有序的，查找时也是从头遍历链表，O(n)。

![](./images/SJJG+SFZM-17-01.jpg)

为了提高效率。像下图，对链表建立一级“索引”，每两个结点提取一个结点到上一级，down 表示 down 指针，指向下一级结点。

![](./images/SJJG+SFZM-17-02.jpg)

如果我们现在要查找某个结点，比如 16。我们可以先在索引层遍历，当遍历到索引层中值为 13 的结点时，我们发现下一个结点是 17，那要查找的结点 16 肯定就在这两个结点之间。然后我们通过索引层结点的 down 指针，下降到原始链表这一层，继续遍历。这个时候，我们只需要再遍历 2 个结点，就可以找到值等于 16 的这个结点了。这样，原来如果要查找 16，需要遍历 10 个结点，现在只需要遍历 7 个结点。

**加来一层索引之后，查找一个结点需要遍历的结点个数减少了，也就是说查找效率提高了。**

如果再建立一层索引，需要遍历的结点数会更少（1 -> 7 -> 13 -> 13 -> 13 -> 16，6个）。

![](./images/SJJG+SFZM-17-03.jpg)

链表的长度越大，在构建索引之后，查找效率的提升就会非常明显。

![](./images/SJJG+SFZM-17-04.jpg)

**这种链表加多级索引的结构，就是==跳表==。**

### 用跳表查询到底有多快？

在一个单链表中查询某个数据的时间复杂度是 O(n)。

> 那在一个具有多级索引的跳表中，查询某个数据的时间复杂度是多少呢？

###### 两个结点会抽出一个结点作为上一级索引的结点，那第一级索引的结点个数大约就是 n/2，第二级索引的结点个数大约就是 n/4，第三级索引的结点个数大约就是 n/8，依次类推，也就是说，**第 k 级索引的结点个数是第 k-1 级索引的结点个数的 1/2，那第 k级索引结点的个数就是 ==n/(2^k^)==**。

假设索引有 h 级，最高级的索引有 2 个结点。通过上面的公式，我们可以得到 n/(2h)=2，从而求得 h=log2n-1。如果包含原始链表这一层，整个跳表的高度就是 log2n。我们在跳表中查询某个数据的时候，如果每一层都要遍历 m 个结点，那在跳表中查询一个数据的时间复杂度就是 `O(m*logn)`。

**每一级索引都最多只需要遍历 3 个结点（m等于3）。**

假设我们要查找的数据是 x，在第 k 级索引中，我们遍历到 y 结点之后，发现 x 大于 y，小于后面的结点 z，所以我们通过 y 的 down 指针，从第 k 级索引下降到第 k-1 级索引。在第 k-1 级索引中，y 和 z 之间只有 3 个结点（包含 y 和 z），所以，我们在 K-1 级索引中最多只需要遍历 3 个结点，依次类推，每一级索引都最多只需要遍历 3 个结点。

![](images/SJJG+SFZM-17-05.jpg)

所以在跳表中查询任意数据的时间复杂度就是 `O(logn)`。【空间换时间的设计思路】

### 跳表是不是很浪费内存？

跳表的空间复杂度分析：

![](images/SJJG+SFZM-17-06.jpg)

这几个加起来为n-2，所以跳表的空间复杂度是 `O(n)`。也就是n个结点的单链表构造成跳表需要额外接近n个结点的存储空间。

为了降低索引占用的内存空间，可以把间隔扩大，比如三个：

![](images/SJJG+SFZM-17-07.jpg)

为了方便计算，假设最高索引结点个数为1：

![](images/SJJG+SFZM-17-08.jpg)

通过等比数列求和公式，总的索引节点约为n/2。尽管空间复杂度还是 O(n)，但占用空间减少了一半。

实际上，在软件开发中，我们不必太在意索引占用的额外空间。在讲数据结构和算法时，我们习惯性地把要处理的数据看成整数，但是在实际的软件开发中，**原始链表中存储的有可能是很大的对象，而索引结点只需要存储关键值和几个指针，并不需要存储对象**，所以当对象比索引结点大很多时，那索引占用的额外空间就可以忽略了。

### 高效的动态插入和删除

跳表这个动态数据结构，不仅支持查找操作，还支持动态的插入、删除操作，而且插入、删除操作的时间复杂度也是 O(logn)。

对于跳表来说，查找某个结点的时间复杂度是 O(logn)，同样查找某个数据应该插入的位置，时间复杂度也是O(logn)，过程：

![](images/SJJG+SFZM-17-09.jpg)

删除操作。如果这个结点在索引中也有出现，我们除了要删除原始链表中的结点，还要删除索引中的。因为单链表中的删除操作需要拿到要删除结点的前驱结点，然后通过指针操作完成删除。所以在查找要删除的结点的时候，一定要获取前驱结点。当然，如果我们用的是双向链表，就不需要考虑这个问题了。

### 跳表索引动态更新

当我们不停地往跳表中插入数据时，如果我们不更新索引，就有可能出现某 2 个索引结点之间数据非常多的情况。极端情况下，跳表还会退化成单链表。

![](images/SJJG+SFZM-17-10.jpg)

作为一种动态数据结构，需要某种手段来==维护索引与原始链表大小之间的平衡==，也就是说，如果链表中结点多了，索引结点就相应地增加一些，避免复杂度退化，以及查找、插入、删除操作性能下降。

跳表是通过==随机函数==来维护前面提到的“平衡性”。

当往跳表中插入数据的时候，通过一个随机函数，来决定将这个结点插入到哪几级索引中，比如随机函数生成了值 K，那我们就将这个结点添加到第一级到第 K 级这 K 级索引中。

![](images/SJJG+SFZM-17-11.jpg)

随机函数的选择很有讲究。🔖

### 内容小结

跳表是一种动态数据结构，支持快速地插入、删除、查找操作，时间复杂度都是 O(logn)。跳表的空间复杂度是 O(n)。

### 思考

> 为什么 Redis 要用跳表来实现有序集合，而不是红黑树？🔖

Redis 中的有序集合是通过跳表来实现的，严格点讲，其实还用到了散列表。

Redis 中的有序集合支持的核心操作主要有下面这几个：

- 插入一个数据；
- 删除一个数据；
- 查找一个数据；
- 按照区间查找数据（比如查找值在[100, 356]之间的数据）；
- 迭代输出有序序列

其中，插入、删除、查找以及迭代输出有序序列这几个操作，红黑树也可以完成，时间复杂度跟跳表是一样的。但是，**按照区间来查找数据这个操作，红黑树的效率没有跳表高**。

对于按照区间查找数据这个操作，跳表可以做到 O(logn) 的时间复杂度定位区间的起点，然后在原始链表中顺序往后遍历就可以了。这样做非常高效。

当然，Redis 之所以用跳表来实现有序集合，还有其他原因，比如，跳表更容易代码实现。虽然跳表的实现也不简单，但比起红黑树来说还是好懂、好写多了，而简单就意味着可读性好，不容易出错。还有，跳表更加灵活，它可以通过改变索引构建策略，有效平衡执行效率和内存消耗。

不过，跳表也不能完全替代红黑树。因为红黑树比跳表的出现要早一些，很多编程语言中的 Map 类型都是通过红黑树来实现的。我们做业务开发的时候，直接拿来用就可以了，不用费劲自己去实现一个红黑树，但是跳表并没有一个现成的实现，所以在开发中，如果你想使用跳表，必须要自己实现。

> 如果每三个或者五个结点提取一个结点作为上级索引，对应的在跳表中查询数据的时间复杂度是多少呢？





> JDK中的`ConcurrentSkipListMap`和`ConcurrentSkipListSet`。



## 18 散列表（上）：Word文档中的单词拼写检查功能是如何实现的？

==散列表（Hash Table）==，“哈希表”或者“Hash 表”。

### 散列思想

散列表用的是==数组支持按照下标随机访问==数据的特性，所以散列表其实就是==数组的一种扩展，由数组演化而来==。可以说，如果没有数组，就没有散列表。

> 例子1：
> 
> 假如我们有 89 名选手参加学校运动会。为了方便记录成绩，每个选手胸前都会贴上自己的参赛号码。这 89 名选手的编号依次是 1 到 89。现在我们希望编程实现这样一个功能，通过编号快速找到对应的选手信息。你会怎么做呢？
> 
> 可以把这 89 名选手的信息放在数组里。编号为 1 的选手，我们放到数组中下标为 1 的位置；编号为 2 的选手，我们放到数组中下标为 2 的位置。以此类推，编号为 k 的选手放到数组中下标为 k 的位置。
> 
> 因为参赛编号跟数组下标一一对应，当我们需要查询参赛编号为 x 的选手的时候，我们只需要将下标为 x 的数组元素取出来就可以了，时间复杂度就是 O(1)。

在这个例子里，参赛编号是自然数，并且与数组的下标形成一一映射，所以利用数组支持根据下标随机访问的时候，时间复杂度是 O(1) 这一特性，就可以实现快速查找编号对应的选手信息。

> 例子2：
> 
> 假设校长说，参赛编号不能设置得这么简单，要加上年级、班级这些更详细的信息，所以我们把编号的规则稍微修改了一下，用 6 位数字来表示。比如 051167，其中，前两位 05 表示年级，中间两位 11 表示班级，最后两位还是原来的编号 1 到 89。这个时候我们该如何存储选手信息，才能够支持通过编号来快速查找选手信息呢？
> 
> 思路还是跟前面类似。尽管我们不能直接把编号作为数组下标，但我们可以截取参赛编号的后两位作为数组下标，来存取选手信息数据。当通过参赛编号查询选手信息的时候，我们用同样的方法，取参赛编号的后两位，作为数组下标，来读取数组中的数据。

![](./images/SJJG+SFZM-18-01.jpg)

这就是典型的散列思想。参赛选手的编号我们叫做**键（key）**或关键字。

参赛编号转化为数组下标的映射方法就叫作**==散列函数==**（或“Hash函数”“==哈希函数==”）。

散列函数计算得到的值就叫作**==散列值==**（或“Hash 值”“哈希值”）。

散列表用的就是数组支持按照下标随机访问的时候，时间复杂度是 O(1) 的特性。我们==通过散列函数把元素的键值映射为下标==，然后将数据存储在数组中对应下标的位置。当我们按照键值查询元素时，我们用同样的散列函数，将键值转化数组下标，从对应的数组下标的位置取数据。

### 散列函数

散列函数设计的三点基本要求：

1. 散列函数计算得到的散列值是一个非负整数；
2. 如果 key1 = key2，那 hash(key1) == hash(key2)；
3. 如果 key1 ≠ key2，那 hash(key1) ≠ hash(key2)。

真实的情况下，要想找到一个不同的 key 对应的散列值都不一样的散列函数，几乎是不可能的。即便像业界著名的MD5、SHA、CRC等哈希算法，也**无法完全避免**这种**==散列冲突==**。而且，因为数组的存储空间有限，也会加大散列冲突的概率。

### 散列冲突

再好的散列函数也无法避免散列冲突。

常用的散列冲突解决方法有两类：

#### 1 开放寻址法（open addressing）

开放寻址法的核心思想是，**如果出现了散列冲突，我们就重新探测一个空闲位置，将其插入**。

重新探测新位置的方法：

##### 线性探测（Linear Probing）

- 插入

如果某个数据经过散列函数散列之后，存储位置已经被占用了，我们就从当前位置开始，依次往后查找，看是否有空闲位置，直到找到为止。

![黄色表示空闲](./images/SJJG+SFZM-18-02.jpg)

从图中可以看出，散列表的大小为 10，在元素 x 插入散列表之前，已经 6 个元素插入到散列表中。x 经过 Hash 算法之后，被散列到位置下标为 7 的位置，但是这个位置已经有数据了，所以就产生了冲突。于是我们就顺序地往后一个一个找，看有没有空闲的位置，遍历到尾部都没有找到空闲的位置，于是我们再从表头开始找，直到找到空闲位置 2，于是将其插入到这个位置。

- 查找

有点类似插入过程。

通过散列函数求出要查找元素的键值对应的散列值；

然后比较数组中下标为散列值的元素和要查找的元素；

如果相等，则说明就是我们要找的元素，否则就顺序往后依次查找；

如果遍历到数组中的空闲位置，还没有找到，就说明要查找的元素并没有在散列表中。

![](./images/SJJG+SFZM-18-03.jpg)

- 删除

不能单纯地把要删除的元素设置为空。因为影响查找操作。

将删除的元素，特殊标记为 **deleted**。当线性探测查找的时候，遇到标记为 deleted 的空间，并不是停下来，而是继续往下探测。

![](./images/SJJG+SFZM-18-04.jpg)

> 线性探测法的问题：
> 
> 当散列表中插入的数据越来越多时，散列冲突发生的可能性就会越来越大，空闲位置会越来越少，线性探测的时间就会越来越久。极端情况下，我们可能需要探测整个散列表，所以最坏情况下的时间复杂度为 O(n)。
> 
> 查找和删除操作情况类似。

##### 二次探测（Quadratic probing）

跟线性探测很像，线性探测每次探测的步长是 1，二次探测的下标序列就是hash(key)+0，hash(key)+1^2^，hash(key)+2^2^，hash(key)+3^2^……（步长变为原来的平方）。

##### 双重散列（Double hashing）

不仅要使用一个散列函数，而是使用一组散列函数 hash1(key)，hash2(key)，hash3(key)……

先用第一个散列函数，如果计算得到的存储位置已经被占用，再用第二个散列函数，依次类推，直到找到空闲的存储位置。

不管采用哪种探测方法，当散列表中空闲位置不多的时候，散列冲突的概率就会大大提高。为了尽可能保证散列表的操作效率，一般，会尽可能保证散列表中有一定比例的空闲槽位。用**==装载因子（load factor）==**来表示空位的多少。

> **==散列表的装载因子 = 填入表中的元素个数 / 散列表的长度==**

装载因子越大，说明空闲位置越少，冲突越多，散列表的性能会下降。

#### 2 链表法（chaining）

链表法是一种更加常用的散列冲突解决办法，相比开放寻址法，它要简单很多。

![](./images/SJJG+SFZM-18-05.jpg)

在散列表中，每个“桶（bucket）”或者“槽（slot）”会对应一条链表，所有散列值相同的元素我们都放到相同槽位对应的链表中。

插入，只需要通过散列函数计算出对应的散列槽位，将其插入到对应链表中即可，所以插入的时间复杂度是 O(1)。

查找、删除，通过散列函数计算出对应的槽，然后遍历链表查找或者删除。它们的时间复杂度跟链表的长度 k 成正比，也就是 O(k)。

对于散列比较均匀的散列函数来说，理论上讲，k=n/m，其中 n 表示散列中数据的个数，m 表示散列表中“槽”的个数。

### 小结

散列表来源于数组，它借助散列函数对数组这种数据结构进行扩展，利用的是数组支持按照下标随机访问元素的特性。散列表两个核心问题是**散列函数设计和散列冲突解决**。散列冲突有两种常用的解决方法，**开放寻址法和链表法**。散列函数设计的好坏决定了散列冲突的概率，也就决定散列表的性能。

### 思考题🔖

- Word 文档中单词拼写检查功能是如何实现的？
  
  常用的英文单词有 20 万个左右，假设单词的平均长度是 10 个字母，平均一个单词占用 10 个字节的内存空间，那 20 万英文单词大约占 2MB 的存储空间，就算放大 10 倍也就是 20MB。对于现在的计算机来说，这个大小完全可以放在内存里面。所以我们可以用散列表来存储整个英文单词词典。
  
  当用户输入某个英文单词时，我们拿用户输入的单词去散列表中查找。如果查到，则说明拼写正确；如果没有查到，则说明拼写可能有误，给予提示。借助散列表这种数据结构，我们就可以轻松实现快速判断是否存在拼写错误。

- 假设我们有 10 万条 URL 访问日志，如何按照访问次数给 URL 排序？
  
  遍历 10 万条数据，以 URL 为 key，访问次数为 value，存入散列表，同时记录下访问次数的最大值 K，时间复杂度 O(N)。 
  
  如果 K 不是很大，可以使用桶排序，时间复杂度 O(N)。如果 K 非常大（比如大于 10 万），就使用快速排序，复杂度 O(NlogN)。

- 有两个字符串数组，每个数组大约有 10 万条字符串，如何快速找出两个数组中相同的字符串？
  
  以第一个字符串数组构建散列表，key 为字符串，value 为出现次数。再遍历第二个字符串数组，以字符串为 key 在散列表中查找，如果 value 大于零，说明存在相同字符串。时间复杂度 O(N)。



## 19 散列表（中）：如何打造一个工业级水平的散列表？

散列表的查询效率与<u>散列函数、装载因子、散列冲突</u>等都有关系。

在极端情况下，有些恶意的攻击者，还有可能通过精心构造的数据，使得所有的数据经过散列函数之后，都散列到同一个槽里。

**散列表碰撞攻击**

> 如何设计一个可以应对各种异常情况的工业级散列表，来避免在散列冲突的情况下，散列表性能的急剧下降，并且能抵抗散列碰撞攻击？

### 如何设计散列函数？

1. 散列函数的设计不能太复杂。
2. 散列函数生成的值要尽可能**==随机==**并且**==均匀==**分布

实际工作，还要考虑因素有**关键字的长度、特点、分布，散列表的大小**等。

常用的、简单的散列函数的设计方法：

- “数据分析法”。
  
  学生运动会，通过分析参赛编号的特征，把编号中的后两位作为散列值；
  
  手机号码前几位重复的可能性很大，后面几位就比较随机，可以取手机号的后四位作为散列值。

- 实现 Word 拼写检查功能：<u>将单词中每个字母的ASCll 码值“进位”相加，然后再跟散列表的大小求余、取模，作为散列值</u>。比如单词nice，转化成散列值：
  
  ```java
  hash("nice")=(("n" - "a") * 26*26*26 + ("i" - "a")*26*26 + ("c" - "a")*26+ ("e"-"a")) / 78978
  ```

- 其它方法有：<u>直接寻址法、平方取中法、折叠法、随机数法</u>等。

### 装载因子过大了怎么办？

装载因子越大，说明散列表中的元素越多，空闲位置越少，散列冲突的概率就越大。

- 对于没有频繁插入和删除的静态数据集合，容易设计散列函数。

- 对于动态散列表，数据集合是频繁变动的，事先无法预估加入的数据个数。**动态扩容**
  
  针对数组的扩容，数据搬移操作比较简单。但是，针对散列表的扩容，数据搬移操作要复杂很多。因为散列表的大小变了，数据的存储位置也变了，需要通过散列函数重新计算每个数据的存储位置。
  
  ![](./images/SJJG+SFZM-19-01.jpg)

一些数据删除后，如果对空间敏感，可通过**动态缩容**。

### 如何避免低效的扩容？

为了解决一次性扩容耗时过多的情况，可以将扩容操作穿插在插入操作的过程中，分批完成。

![](./images/SJJG+SFZM-19-02.jpg)

对于这期间的查询操作，为了兼容了新、老散列表中的数据，我们先从新散列表中查找，如果没有找到，再去老的散列表中查找。

### 如何选择冲突解决方法？

Java 中 LinkedHashMap 就采用了链表法解决冲突，ThreadLocalMap 是通过线性探测的开放寻址法来解决冲突。

|      | 开放寻址法                                                      | 链表法                                                                              |
| ---- | ---------------------------------------------------------- | -------------------------------------------------------------------------------- |
| 优点   | 不需要很多链表，数据都存储在数组中，可以有效地利用 CPU 缓存加快查询速度；<br />序列化起来比较简单；    | 对内存的利用率更高；<br />链表结点可以在需要的时候再创建，不要事先申请好；<br /><br />装载因子可很大；                     |
| 缺点   | 删除数据麻烦；<br />数据都存储在一个数组中，冲突代价更高；<br />装载因子的上限不能太大，更浪费内存空间； | 链表中的节点在内存中不是连续的，对CPU缓存不友好；<br />链表需要存储指针，比较小的对象比较消耗内存；（如果对象的代销远远大于一个指针的大小，就可以忽略） |
| 适用场景 | 当数据量比较小、装载因子小的时候，适合采用开放寻址法。                                | 基于链表的散列冲突处理方法比较适合存储大对象、大数据量的散列表，而且，比起开放寻址法，它更加灵活，支持更多的优化策略，比如用红黑树代替链表。           |

![对链表改造](./images/SJJG+SFZM-19-03.jpg)

总结一下，基于链表的散列冲突处理方法比较适合存储大对象、大数据量的散列表，而且，比起开放寻址法，它更加灵活，支持更多的优化策略，比如用红黑树代替链表。

### 工业级散列表举例分析🔖

HashMap

1. 初始大小
   
   16。可以根据数量大概量，修改初始大小，减少动态扩容的次数。

2. 装载因子和动态扩容
   
   0.75。每次扩容为原来的两倍。

3. 散列冲突解决方法
   
   HashMap 底层采用**链表法**来解决冲突。
   
   JDK8后，引入红黑树，优化链表过长的情况。当链表长度太长（默认超过 8）时，链表就转换为红黑树，利用红黑树快速增删改查的特点，提高 HashMap 的性能；当红黑树结点个数少于 8 个的时候，又会将红黑树转化为链表。

4. 散列函数
   
   追求的是简单高效、分布均匀。
   
   🔖🔖
   
   ```java
       static final int hash(Object key) {
           int h;
           return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
       }
   ```
   
   hashCode() 返回的是 Java 对象的 hash code。比如 String 类型的对象的 hashCode() 就是：
   
   ```java
       public int hashCode() {
           int h = hash;
           if (h == 0 && value.length > 0) {
               char val[] = value;
   
               for (int i = 0; i < value.length; i++) {
                   h = 31 * h + val[i];
               }
               hash = h;
           }
           return h;
       }
   ```

### 小结

关于散列函数的设计，我们要尽可能让散列后的值随机且均匀分布，这样会尽可能地减少散列冲突，即便冲突之后，分配到每个槽内的数据也比较均匀。除此之外，散列函数的设计也不能太复杂，太复杂就会太耗时间，也会影响散列表的性能。

关于散列冲突解决方法的选择。

大部分情况下，链表法更加普适。而且，我们还可以通过将链表法中的链表改造成其他动态查找数据结构，比如红黑树，来避免散列表时间复杂度退化成 O(n)，抵御散列碰撞攻击。

但是，对于小规模数据、装载因子不高的散列表，比较适合用开放寻址法。

### 思考题

> 如何设计一个工业级的散列函数？

要求：

- 支持快速地查询、插入、删除操作；
- 内存占用合理，不能浪费过多的内存空间；
- 性能稳定，极端情况下，散列表的性能也不会退化到无法接受的情况。

实现：

- 设计一个合适的散列函数；
- 定义装载因子阈值，并且设计动态扩容策略；
- 选择合适的散列冲突解决方法。

> 在你熟悉的编程语言中，哪些数据类型底层是基于散列表实现的？散列函数是如何设计的？散列冲突是通过哪种方法解决的？是否支持动态扩容呢？



## 20 散列表（下）：为什么散列表和链表经常会一起使用？🔖🔖

### LRU缓存淘汰算法

借助散列表，可以把LRU缓存淘汰算法的时间复杂度降低为 O(1)。

![](images/SJJG+SFZM-20-01.jpg)

### Redis 有序集合

实际上，在有序集合中，每个成员对象有两个重要的属性，key（键值）和 score（分值）。我们不仅会通过 score 来查找数据，还会通过 key 来查找数据。

### Java的LinkedHashMap

LinkedHashMap 是通过**双向链表和散列表**这两种数据结构组合实现的。LinkedHashMap 中的“Linked”实际上是指的是双向链表，并非指用链表法解决散列冲突。

### 思考题

> 为什么散列表和链表会经常一块使用？

散列表这种数据结构虽然支持非常高效的数据插入、删除、查找操作，但是散列表中的数据都是通过散列函数打乱之后无规律存储的。也就说，它无法支持按照某种顺序快速地遍历数据。如果希望按照顺序遍历散列表中的数据，那我们需要将散列表中的数据拷贝到数组中，然后排序，再遍历。

因为散列表是动态数据结构，不停地有数据的插入、删除，所以每当我们希望按顺序遍历散列表中的数据的时候，都需要先排序，那效率势必会很低。为了解决这个问题，我们将散列表和链表（或者跳表）结合在一起使用。

>  上面的几个散列表和链表结合使用的例子里，用的都是双向链表。如果把双向链表改成单链表，还能否正常工作呢？为什么呢？

> 假设猎聘网有 10 万名猎头，每个猎头都可以通过做任务（比如发布职位）来积累积分，然后通过积分来下载简历。假设你是猎聘网的一名工程师，如何在内存中存储这 10 万个猎头 ID 和积分信息，让它能够支持这样几个操作：
> 
> - 根据猎头的 ID 快速查找、删除、更新这个猎头的积分信息；
> - 查找积分在某个区间的猎头 ID 列表；
> - 查找按照积分从小到大排名在第 x 位到第 y 位之间的猎头 ID 列表。

> 总结：
> 
> 两种数据结构，链表和数组。 
> 
> 数组占据**随机访问**的优势，却有需要连续内存的缺点。 
> 
> 链表具有**可不连续存储**的优势，但访问查找是线性的。 
> 
> 散列表和链表、跳表的混合使用，是为了结合数组和链表的优势，规避它们的不足。
> 
>  得出数据结构和算法的重要性排行榜：**==连续空间 > 时间 > 碎片空间==**。



## 21 哈希算法（上）：如何防止数据库中的用户信息被脱库？

哈希算法历史悠久，业界著名的哈希算法也有很多，比如 MD5、SHA 等。

如何用哈希算法解决问题。

### 什么是哈希算法？

> hash，翻译为哈希、散列

**将任意长度的二进制值串映射为固定长度的二进制值串**，这个**映射的规则**就是**==哈希算法==**，得到固定长度的二进制值串就是**==哈希值==**。

优秀哈希算法设计要点：

- 从哈希值不能**反向**推导出原始数据（所以哈希算法也叫单向哈希算法）；
- 对输入数据非常**敏感**，哪怕原始数据只修改了一个Bit，最后得到的哈希值也大不相同；
- 散列冲突的**概率**要很小，对于不同的原始数据，哈希值相同的概率非常小；
- 哈希算法的执行**效率**要尽量高效，针对较长的文本，也能快速地计算出哈希值。

哈希算法常见七个应用：安全加密、唯一标识、数据校验、散列函数、负载均衡、数据分片、分布式存储。

### 应用一：安全加密

MD5（MD5 Message-Digest Algorithm，MD5 消息摘要算法），128bit二进制串，一般用32个16进制数表示。

SHA（Secure Hash Algorithm，安全散列算法）

DES（Data Encryption Standard，数据加密标准）

AES（Advanced Encryption Standard，高级加密标准）

> 组合数学（离散数学）一个非常基础的理论，==鸽巢原理（也叫抽屉原理）==：
> 
> 如果有 10 个鸽巢，有 11 只鸽子，那肯定有 1 个鸽巢中的鸽子数量多于 1 个，换句话说就是，肯定有 2 只鸽子在 1 个鸽巢内。

为什么哈希算法无法做到零冲突？🔖

### 应用二：唯一标识

图库

> 任何文件在计算中都可以表示成二进制码串。

为每张图片取一个唯一标识（或者说信息摘要）。比如，我们可以从图片的二进制码串开头取 100 个字节，从中间取 100 个字节，从最后再取 100 个字节，然后将这 300 个字节放到一块，通过哈希算法（比如 MD5），得到一个哈希字符串，用它作为图片的唯一标识。

如果还想继续提高效率，我们可以把每个图片的唯一标识，和相应的图片文件在图库中的路径信息，都存储在散列表中。当要查看某个图片是不是在图库中的时候，我们先通过哈希算法对这个图片取唯一标识，然后在散列表中查找是否存在这个唯一标识。

如果不存在，那就说明这个图片不在图库中；如果存在，我们再通过散列表中存储的文件路径，获取到这个已经存在的图片，跟现在要插入的图片做全量的比对，看是否完全一样。如果一样，就说明已经存在；如果不一样，说明两张图片尽管唯一标识相同，但是并不是相同的图片。

### 应用三：数据校验

BT下载，基于 P2P 协议的。从多个机器上并行下载一个 2GB 的电影，这个电影文件可能会被分割成很多文件块（比如可以分成 100 块，每块大约 20MB）。等所有的文件块都下载完成之后，再组装成一个完整的电影文件就行了。

网络传输是不安全的，下载的文件块有可能是被宿主机器恶意修改过的，又或者下载过程中出现了错误，所以下载的文件块可能不是完整的。如果我们没有能力检测这种恶意修改或者文件下载出错，就会导致最终合并后的电影无法观看，甚至导致电脑中毒。现在的问题是，<u>如何来校验文件块的安全、正确、完整呢？</u>

具体的 BT 协议很复杂，校验方法也有很多，其中一种思路：

通过哈希算法，对 100 个文件块分别取哈希值，并且保存在种子文件中。我们在前面讲过，哈希算法有一个特点，对数据很敏感。只要文件块的内容有一丁点儿的改变，最后计算出的哈希值就会完全不同。所以，当文件块下载完成之后，我们可以通过相同的哈希算法，对下载好的文件块逐一求哈希值，然后跟种子文件中保存的哈希值比对。如果不同，说明这个文件块不完整或者被篡改了，需要再重新从其他宿主机器上下载这个文件块。

### 应用四：散列函数

相对哈希算法的其他应用，散列函数对于散列算法冲突的要求要低很多。即便出现个别散列冲突，只要不是过于严重，我们都可以通过开放寻址法或者链表法解决。

散列函数对于散列算法计算得到的值，是否能反向解密也并不关心。更加关注散列后的值是否能平均分布。

散列函数用的散列算法一般都比较简单，比较追求效率。

### 小结

- 唯一标识，哈希算法可以对大数据做信息摘要，通过一个较短的二进制编码来表示很大的数据。
- 用于校验数据的完整性和正确性。
- 安全加密，我们讲到任何哈希算法都会出现散列冲突，但是这个冲突概率非常小。越是复杂哈希算法越难破解，但同样计算时间也就越长。所以，选择哈希算法的时候，要权衡安全性和计算时间来决定用哪种哈希算法。
- 散列函数，它对哈希算法的要求非常特别，更加看重的是散列的平均性和哈希算法的执行效率。

### 思考题

选择相对安全的加密算法，对用户密码进行加密之后再存储。

**字典攻击**：维护一个常用密码的字典表，把字典中的每个密码用哈希算法计算哈希值，然后拿哈希值跟脱库后的密文比对。

针对字典攻击，我们可以引入一个**盐（salt）**，跟用户的密码组合在一起，增加密码的复杂度。

> 区块链底层的实现原理并不复杂。其中，哈希算法就是它的一个非常重要的理论基础。
> 
> 你能讲一讲区块链使用的是哪种哈希算法吗？是为了解决什么问题而使用的呢？

区块链是一块块区块组成的，每个区块分为两部分：区块头和区块体。 

区块头保存着 自己区块体 和 上一个区块头 的哈希值。 

因为这种链式关系和哈希值的唯一性，只要区块链上任意一个区块被修改过，后面所有区块保存的哈希值就不对了。 

区块链使用的是 SHA256 哈希算法，计算哈希值非常耗时，如果要篡改一个区块，就必须重新计算该区块后面所有的区块的哈希值，短时间内几乎不可能做到。



## 22 哈希算法（下）：哈希算法在分布式系统中有哪些应用？🔖



### 应用五：负载均衡

负载均衡算法有**==轮询、随机、加权轮询==**等。

直接的方法就是，维护一张映射关系表，这张表的内容是客户端 IP 地址或者会话 ID 与服务器编号的映射关系。客户端发出的每次请求，都要先在映射表中查找应该路由到的服务器编号，然后再请求编号对应的服务器。这种方法简单直观，但也有几个弊端：

- 如果客户端很多，映射表可能会很大，比较浪费内存空间；
- 客户端下线、上线，服务器扩容、缩容都会导致映射失效，这样维护映射表的成本就会很大；

可以通过哈希算法，对客户端 IP 地址或者会话 ID 计算哈希值，将取得的哈希值与服务器列表的大小进行取模运算，最终得到的值就是应该被路由到的服务器编号。

### 应用六：数据分片

#### 1 如何统计“搜索关键词”出现的次数？

> 假如我们有 1T 的日志文件，这里面记录了用户的搜索关键词，我们想要快速统计出每个关键词被搜索的次数，该怎么做呢？

两个难点，第一个是搜索日志很大，没办法放到一台机器的内存中。第二个难点是，如果只用一台机器来处理这么巨大的数据，处理时间会很长。

可以先对数据进行分片，然后采用多台机器处理的方法，来提高处理速度。具体的思路是这样的：为了提高处理的速度，我们用 n 台机器并行处理。我们从搜索记录的日志文件中，依次读出每个搜索关键词，并且通过哈希函数计算哈希值，然后再跟 n 取模，最终得到的值，就是应该被分配到的机器编号。

这样，哈希值相同的搜索关键词就被分配到了同一个机器上。也就是说，同一个搜索关键词会被分配到同一个机器上。每个机器会分别计算关键词出现的次数，最后合并起来就是最终的结果。

实际上，这里的处理过程也是 ==MapReduce 的基本设计思想==。

#### 2 如何快速判断图片是否在图库中？

> 假设现在我们的图库中有 1 亿张图片，很显然，在单台机器上构建散列表是行不通的。因为单台机器的内存有限，而 1 亿张图片构建散列表显然远远超过了单台机器的内存上限。

同样可以对数据进行分片，然后采用多机处理。我们准备 n 台机器，让每台机器只维护某一部分图片对应的散列表。我们每次从图库中读取一个图片，计算唯一标识，然后与机器个数 n 求余取模，得到的值就对应要分配的机器编号，然后将这个图片的唯一标识和图片路径发往对应的机器构建散列表。

当我们要判断一个图片是否在图库中的时候，我们通过同样的哈希算法，计算这个图片的唯一标识，然后与机器个数 n 求余取模。假设得到的值是 k，那就去编号 k 的机器构建的散列表中查找。

> 估算一下，给这 1 亿张图片构建散列表大约需要多少台机器。

散列表中每个数据单元包含两个信息，哈希值和图片文件的路径。假设我们通过 MD5 来计算哈希值，那长度就是 128 比特，也就是 16 字节。文件路径长度的上限是 256 字节，我们可以假设平均长度是 128 字节。如果我们用链表法来解决冲突，那还需要存储指针，指针只占用 8 字节。所以，散列表中每个数据单元就占用 152 字节（这里只是估算，并不准确）。

假设一台机器的内存大小为 2GB，散列表的装载因子为 0.75，那一台机器可以给大约 1000 万（2GB*0.75/152）张图片构建散列表。所以，如果要对 1 亿张图片构建索引，需要大约十几台机器。==在工程中，这种估算还是很重要的，能让我们事先对需要投入的资源、资金有个大概的了解，能更好地评估解决方案的可行性。==

实际上，**==针对这种海量数据的处理问题，我们都可以采用多机分布式处理==**。借助这种分片的思路，可以突破单机内存、CPU 等资源的限制。

### 应用七：分布式存储

现在互联网面对的都是海量的数据、海量的用户。为了提高数据的读取、写入能力，一般都采用分布式的方式来存储数据，比如分布式缓存。

<u>该如何决定将哪个数据放到哪个机器上呢？</u>

借用数据分片的思想，即通过哈希算法对数据取哈希值，然后对机器个数取模，这个最终值就是应该存储的缓存机器编号。

![](images/SJJG+SFZM-22-01.jpg)

**==一致性哈希算法==**🔖🔖

[白话解析：一致性哈希算法 consistent hashing](https://www.zsythink.net/archives/1182)

### 小结

在负载均衡应用中，利用哈希算法替代映射表，可以实现一个会话粘滞的负载均衡策略。

在数据分片应用中，通过哈希算法对处理的海量数据进行分片，多机分布式处理，可以突破单机资源的限制。

在分布式存储应用中，利用一致性哈希算法，可以解决缓存等分布式系统的扩容、缩容导致数据大量搬移的难题。

哈希算法其它的应用：==网络协议中的CRC校验==、==Git commit id==等等。





## 23 二叉树基础（上）：什么样的二叉树适合用数组来存储？

线性表结构，栈、队列等等。

非线性表结构，树。

> 二叉树有哪几种存储方式？什么样的二叉树适合用数组来存储？

### 树（Tree）

![](images/SJJG+SFZM-23-01.jpg)

**父节点**

**子节点**

**兄弟节点**

**叶子节点/叶节点**

节点的**==高度==**（Height） = 节点到叶子节点的**最长路径**（边数） 【树的高度 = 根节点的高度】

节点的**==深度==**（Depth） = 根节点到这个节点所经历的**边的个数**

节点的**==层==**（Level） = 节点的深度 + 1

树的高度 = 根节点的高度

![](images/SJJG+SFZM-23-04.jpg)

> “高度”就是从下往上度量；深度”是从上往下度量的；“层数”跟深度类似，不过计数起点是 1。

### 二叉树（Binary Tree）

每个节点**最多**有两个“叉”，分别是**左子节点**和**右子节点**。

二叉树并不要求每个节点都有两个子节点。

![](./images/SJJG+SFZM-23-05.jpg)

编号2的二叉树中，叶子节点全都在<u>最底层</u>，除了叶子节点之外，每个节点都有左右两个子节点，这种二叉树就叫做**==满二叉树==**。

编号3的二叉树中，叶子节点都在<u>最底下两层</u>，最后一层的叶子节点都**靠左**排列，并且除了最后一层，其他层的节点个数都要达到最大，这种二叉树叫做**==完全二叉树==**。

![](images/18413c6597c2850b75367393b401ad60.jpg)

>  如何表示（或者存储）一棵二叉树？

1. 基于指针或者引用的二叉**==链式存储法==**

每个节点有三个字段，其中一个存储数据，另外两个是指向左右子节点的指针。

> 大部分二叉树代码都是通过这种结构来实现的。

![](./images/SJJG+SFZM-23-07.jpg)

2. 基于数组的**==顺序存储法==**

把根节点存储在下标 i = 1 的位置，那左子节点存储在下标 2 * i = 2 的位置，右子节点存储在 2 * i + 1 = 3 的位置。以此类推，B 节点的左子节点存储在 2 * i = 2 * 2 = 4 的位置，右子节点存储在 2 * i + 1 = 2 * 2 + 1 = 5 的位置。

![](images/SJJG+SFZM-23-08.jpg)

总结，如果节点 X 存储在数组中下标为 i 的位置，下标为 2 * i 的位置存储的就是左子节点，下标为 2 * i + 1 的位置存储的就是右子节点。反过来，下标为 i/2 的位置存储就是它的父节点。

通过这种方式，只要知道根节点存储的位置（一般情况下，为了方便计算子节点，根节点会存储在下标为 1 的位置），就可以通过下标计算把整棵树串起来。

完全二叉树只会“浪费”下标为0的存储位置，而非完全二叉树，会浪费较多空间：

![](images/SJJG+SFZM-23-09.jpg)

> ==堆==其实就是一种完全二叉树，最常用的存储方式就是数组。

### 二叉树的遍历

根据节点与它的左右子树节点遍历打印的先后顺序：

- **==前序遍历==**，对于树中的任意节点来说，先打印这个节点，然后再打印它的左子树，最后打印它的右子树。

- **==中序遍历==**，对于树中的任意节点来说，先打印它的左子树，然后再打印它本身，最后打印它的右子树。

- **==后序遍历==**，对于树中的任意节点来说，先打印它的左子树，然后再打印它的右子树，最后打印这个节点本身。

![前、中、后序遍历的顺序图](images/SJJG+SFZM-23-10.jpg)

实际上，**二叉树的前、中、后序遍历就是一个递归的过程**。很容易得出三种遍历的递推公式：

```java
前序遍历的递推公式：
preOrder(r) = print r->preOrder(r->left)->preOrder(r->right)

中序遍历的递推公式：
inOrder(r) = inOrder(r->left)->print r->inOrder(r->right)

后序遍历的递推公式：
postOrder(r) = postOrder(r->left)->postOrder(r->right)->print r
```

代码：

```java
void preOrder(Node* root) {
  if (root == null) return;
  print root // 此处为伪代码，表示打印root节点
  preOrder(root->left);
  preOrder(root->right);
}

void inOrder(Node* root) {
  if (root == null) return;
  inOrder(root->left);
  print root // 此处为伪代码，表示打印root节点
  inOrder(root->right);
}

void postOrder(Node* root) {
  if (root == null) return;
  postOrder(root->left);
  postOrder(root->right);
  print root // 此处为伪代码，表示打印root节点
}
```

从前、中、后序遍历的顺序图看来，每个节点最多会被访问两次，所以遍历操作的时间复杂度，跟节点的个数 n 成正比，也就是说二叉树遍历的时间复杂度是 O(n)。

### 小结

二叉树的每个节点最多有两个子节点，分别是左子节点和右子节点。二叉树中，有两种比较特殊的树，分别是满二叉树和完全二叉树。**满二叉树又是完全二叉树的一种特殊情况**。

二叉树既可以用链式存储，也可以用数组顺序存储。**数组顺序存储的方式比较适合完全二叉树**，其他类型的二叉树用数组存储会比较浪费存储空间。

### 思考🔖

> 给定一组数据，比如 1，3，5，6，9，10。可以构建出多少种不同的二叉树？





> 二叉树还有另外一种遍历方式，也就是按层遍历，你知道如何实现吗？





## 24 二叉树基础（下）：有了如此高效的散列表，为什么还需要二叉树？

二叉查找树最大的特点就是，支持==动态数据集合的快速插入、删除、查找==操作。

> 散列表也是支持这些操作的，并且散列表的这些操作比二叉查找树更高效，时间复杂度是 O(1)。**既然有了这么高效的散列表，使用二叉树的地方是不是都可以替换成散列表呢？有没有哪些地方是散列表做不了，必须要用二叉树来做的呢？**

### 二叉查找树（Binary Search Tree）

**==二叉查找树==**(二叉搜索树)是为了实现**快速查找**而生的。

二叉查找树要求，在树中的任意一个节点，==其左子树中的每个节点的值，都要小于这个节点的值，而右子树节点的值都大于这个节点的值==。

#### 1.查找

如果要查找的数据比根节点的值小，那就在左子树中递归查找；如果要查找的数据比根节点的值大，那就在右子树中递归查找。

![](images/SJJG+SFZM-24-02.jpg)

```java
public class BinarySearchTree {
  private Node tree;

  public Node find(int data) {
    Node p = tree;
    while (p != null) {
      if (data < p.data) p = p.left;
      else if (data > p.data) p = p.right;
      else return p;
    }
    return null;
  }

  public static class Node {
    private int data;
    private Node left;
    private Node right;

    public Node(int data) {
      this.data = data;
    }
  }
}
```

#### 2.插入

如果要插入的数据比节点的数据大，并且节点的右子树为空，就将新数据直接插到右子节点的位置；如果不为空，就再递归遍历右子树，查找插入位置。同理，如果要插入的数据比节点数值小，并且节点的左子树为空，就将新数据插入到左子节点的位置；如果不为空，就再递归遍历左子树，查找插入位置。

![](images/SJJG+SFZM-24-03.jpg)

```java
public void insert(int data) {
  if (tree == null) {
    tree = new Node(data);
    return;
  }

  Node p = tree;
  while (p != null) {
    if (data > p.data) {
      if (p.right == null) {
        p.right = new Node(data);
        return;
      }
      p = p.right;
    } else { // data < p.data
      if (p.left == null) {
        p.left = new Node(data);
        return;
      }
      p = p.left;
    }
  }
}
```

#### 3.删除

二叉查找树的查找、插入操作都比较简单易懂，但是它的删除操作就比较复杂了。分三种情况处理

- 第一种情况是，如果要删除的节点没有子节点，我们只需要直接将父节点中，指向要删除节点的指针置为 null。比如图中的删除节点 55。
- 第二种情况是，如果要删除的节点只有一个子节点（只有左子节点或者右子节点），我们只需要更新父节点中，指向要删除节点的指针，让它指向要删除节点的子节点就可以了。比如图中的删除节点 13。
- 第三种情况是，如果要删除的节点有两个子节点，这就比较复杂了。我们需要找到这个节点的右子树中的最小节点，把它替换到要删除的节点上。然后再删除掉这个最小节点，因为最小节点肯定没有左子节点（如果有左子结点，那就不是最小节点了），所以，我们可以应用上面两条规则来删除这个最小节点。比如图中的删除节点 18。

![](images/SJJG+SFZM-24-04.jpg)

```java
public void delete(int data) {
  Node p = tree; // p指向要删除的节点，初始化指向根节点
  Node pp = null; // pp记录的是p的父节点
  while (p != null && p.data != data) {
    pp = p;
    if (data > p.data) p = p.right;
    else p = p.left;
  }
  if (p == null) return; // 没有找到

  // 要删除的节点有两个子节点
  if (p.left != null && p.right != null) { // 查找右子树中最小节点
    Node minP = p.right;
    Node minPP = p; // minPP表示minP的父节点
    while (minP.left != null) {
      minPP = minP;
      minP = minP.left;
    }
    p.data = minP.data; // 将minP的数据替换到p中
    p = minP; // 下面就变成了删除minP了
    pp = minPP;
  }

  // 删除节点是叶子节点或者仅有一个子节点
  Node child; // p的子节点
  if (p.left != null) child = p.left;
  else if (p.right != null) child = p.right;
  else child = null;

  if (pp == null) tree = child; // 删除的是根节点
  else if (pp.left == p) pp.left = child;
  else pp.right = child;
}
```

删除操作，还有个非常简单、取巧的方法，就是<u>单纯将要删除的节点标记为“已删除”</u>，但是并不真正从树中将这个节点去掉。这样原本删除的节点还需要存储在内存中，比较浪费内存空间，但是删除操作就变得简单了很多。

#### 4.其它操作

**快速地查找最大节点和最小节点、前驱节点和后继节点**。

中序遍历二叉查找树，可以输出有序的数据序列，时间复杂度是 O(n)，非常高效。因此，二叉查找树也叫作==二叉排序树==。

🔖

### 支持重复数据的二叉查找树

实际开发中，会利用对象的某个字段作为键值（key）来构建二叉查找树。把对象中的其他字段叫作==卫星数据==。

如果存储的两个对象键值相同，这种情况该怎么处理呢？

1. 二叉查找树中每一个节点不仅会存储一个数据，因此我们通过链表和支持动态扩容的数组等数据结构，把值相同的数据都存储在同一个节点上。
2. 第二种方法比较不好理解，不过更加优雅。

每个节点仍然只存储一个数据。在查找插入位置的过程中，如果碰到一个节点的值，与要插入数据的值相同，我们就将这个要插入的数据放到这个节点的右子树，也就是说，把这个新插入的数据当作大于这个节点的值来处理。

![](images/SJJG+SFZM-24-05.jpg)

当要查找数据的时候，遇到值相同的节点，我们并不停止查找操作，而是继续在右子树中查找，直到遇到叶子节点，才停止。这样就可以把键值等于要查找值的所有节点都找出来。

![](images/SJJG+SFZM-24-06.jpg)

对于删除操作，我们也需要先查找到每个要删除的节点，然后再按前面讲的删除操作的方法，依次删除。

![](images/SJJG+SFZM-24-07.jpg)

### 二叉查找树的时间复杂度分析

叉查找树的形态各式各样。比如这个图中，对于同一组数据，我们构造了三种二叉查找树。它们的查找、插入、删除操作的执行效率都是不一样的。第一种二叉查找树，根节点的左右子树极度不平衡，已经退化成了链表，所以查找的时间复杂度就变成了 O(n)。

![](images/SJJG+SFZM-24-08.jpg)

分析一个最理想的情况，==完全二叉树==的插入、删除、查找的时间复杂度是多少呢？

从我前面的例子、图，以及还有代码来看，不管操作是插入、删除还是查找，**时间复杂度其实都跟树的高度成正比，也就是 O(height)**。既然这样，现在问题就转变成另外一个了，也就是，如何求一棵包含 n 个节点的完全二叉树的高度？

树的高度就等于最大层数减一，为了方便计算，我们转换成层来表示。从图中可以看出，包含 n 个节点的完全二叉树中，第一层包含 1 个节点，第二层包含 2 个节点，第三层包含 4 个节点，依次类推，下面一层节点个数是上一层的 2 倍，第 K 层包含的节点个数就是 `2^(K-1)`。

不过，对于完全二叉树来说，最后一层的节点个数有点儿不遵守上面的规律了。它包含的节点个数在 1 个到 2^(L-1) 个之间（我们假设最大层数是 L）。如果我们把每一层的节点个数加起来就是总的节点个数 n。也就是说，如果节点的个数是 n，那么 n 满足这样一个关系：

```
n >= 1+2+4+8+...+2^(L-2)+1
n <= 1+2+4+8+...+2^(L-2)+2^(L-1)
```

L 的范围是==[log~2~(n+1), log~2~n +1]==。完全二叉树的层数小于等于 log~2~n +1，也就是说，完全二叉树的高度小于等于 log~2~n。

### 小结

二叉查找树中，每个节点的值都大于左子树节点的值，小于右子树节点的值。不过，这只是针对没有重复数据的情况。对于存在重复数据的二叉查找树，有两种构建方法，一种是让每个节点存储多个值相同的数据；另一种是，每个节点中存储一个数据。针对这种情况，我们只需要稍加改造原来的插入、删除、查找操作即可。

在二叉查找树中，查找、插入、删除等很多操作的时间复杂度都跟树的高度成正比。两个极端情况的时间复杂度分别是 O(n) 和 O(logn)，分别对应二叉树退化成链表的情况和完全二叉树。

为了避免时间复杂度的退化，针对二叉查找树，我们又设计了一种更加复杂的树，平衡二叉查找树，时间复杂度可以做到稳定的 O(logn)。

### 思考题🔖

> 散列表的插入、删除、查找操作的时间复杂度可以做到常量级的 O(1)，非常高效。
> 
> 而二叉查找树在比较平衡的情况下，插入、删除、查找操作时间复杂度才是 O(logn)，相对散列表，好像并没有什么优势，那我们为什么还要用二叉查找树呢？

1. 散列表中的数据是无序存储的，如果要输出有序的数据，需要先进行排序。而对于二叉查找树来说，我们只需要中序遍历，就可以在 O(n) 的时间复杂度内，输出有序的数据序列。
2. 散列表扩容耗时很多，而且当遇到散列冲突时，性能不稳定，尽管二叉查找树的性能不稳定，但是在工程中，我们**最常用的平衡二叉查找树的性能非常稳定，时间复杂度稳定在 O(logn)**。
3. 笼统地来说，尽管散列表的查找等操作的时间复杂度是常量级的，但因为哈希冲突的存在，这个常量不一定比 logn 小，所以实际的查找速度可能不一定比 O(logn) 快。加上哈希函数的耗时，也不一定就比平衡二叉查找树的效率高。
4. 散列表的构造比二叉查找树要复杂，需要考虑的东西很多。比如<u>散列函数的设计、冲突解决办法、扩容、缩容</u>等。平衡二叉查找树只需要考虑**平衡性**这一个问题，而且这个问题的解决方案比较成熟、固定。
5. 为了避免过多的散列冲突，散列表装载因子不能太大，特别是基于开放寻址法解决冲突的散列表，不然会浪费一定的存储空间。

> 如何通过编程，求出一棵给定二叉树的确切高度呢？





## 25 红黑树（上）：为什么工程中都用红黑树这种二叉树？

二叉查找树是最常用的一种二叉树，它支持快速插入、删除、查找操作，各个操作的时间复杂度跟树的高度成正比，理想情况下，时间复杂度是 O(logn)。

二叉查找树在<u>频繁的动态更新过程中</u>，可能会出现树的高度远大于 log~2~n 的情况，从而导致各个操作的效率下降。极端情况下，二叉树会退化为链表，时间复杂度会退化到 O(n)。

### 什么是“平衡二叉查找树”？

**==平衡二叉树==**的严格定义：**二叉树中任意一个节点的左右子树的高度相差不能大于 1**。

![](images/SJJG+SFZM-25-01.jpg)

完全二叉树、满二叉树其实都是平衡二叉树，但是非完全二叉树也有可能是平衡二叉树。

**==平衡二叉查找树==**既是平衡二叉树，又有二叉查找树的特点。最先被发明的平衡二叉查找树是[**AVL树**](https://zh.wikipedia.org/wiki/AVL%E6%A0%91)。

> 我们学习数据结构和算法是为了应用到实际的开发中的，没必去死抠定义。

发明平衡二叉查找树这类数据结构的初衷是，**解决普通二叉查找树在频繁的插入、删除等动态更新的情况下，出现时间复杂度退化的问题**。

所以，平衡二叉查找树中“**平衡**”的意思，其实就是**让整棵树左右看起来比较“对称”、比较“平衡”，不要出现左子树很高、右子树很矮的情况**。这样就能<u>让整棵树的高度相对来说低一些，相应的插入、删除、查找等操作的效率高一些</u>。

所以，如果我们现在设计一个新的平衡二叉查找树，只要树的高度不比 log~2~n 大很多（比如树的高度仍然是对数量级的），尽管它不符合我们前面讲的严格的平衡二叉查找树的定义，但我们仍然可以说，这是一个合格的平衡二叉查找树。

### 如何定义一棵“红黑树”？

平衡二叉查找树有很多，比如，<u>AVL树、Splay Tree（伸展树）、Treap（树堆）</u>等，但最出名的是红黑树。

红黑树，“Red-Black Tree”，简称 R-B Tree，是一种不严格的平衡二叉查找树。

- 红黑树中的节点，一类被标记为黑色，一类被标记为红色。
- 根节点是黑色的；
- 每个叶子节点都是黑色的空节点（NIL），也就是说，**叶子节点不存储数据**；(为了简化红黑树的代码实现而设置的)
- 任何相邻的节点都不能同时为红色，也就是说，**红色节点是被黑色节点隔开的**；
- 每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点；

### 为什么说红黑树是“近似平衡”的？

“平衡”的意思可以等价为**性能不退化**。“近似平衡”就等价为性能不会退化得太严重。

上一节讲过，二叉查找树很多操作的性能都跟树的**高度成正比**。一棵极其平衡的二叉树（满二叉树或完全二叉树）的高度大约是 log~2~n，所以如果要证明红黑树是近似平衡的，我们只需要分析，红黑树的高度是否比较稳定地趋近 log~2~n 就好了。

**首先，我们来看，如果我们将红色节点从红黑树中去掉，那单纯包含黑色节点的红黑树的高度是多少呢？**

红色节点删除之后，有些节点就没有父节点了，它们会直接拿这些节点的祖父节点（父节点的父节点）作为父节点。所以，之前的二叉树就变成了四叉树。

![](images/SJJG+SFZM-25-03.jpg)

🔖🔖



### 小结

**学习数据结构和算法，要学习它的==由来、特性、适用的场景以及它能解决的问题==。**

红黑树是一种平衡二叉查找树。它是为了解决==普通二叉查找树在数据更新的过程中，复杂度退化的问题==而产生的。红黑树的高度近似 log~2~n，所以它是近似平衡，插入、删除、查找操作的时间复杂度都是 O(logn)。

因为红黑树是一种==性能非常稳定的二叉查找树==，所以，在工程中，但凡是用到动态插入、删除、查找数据的场景，都可以用到它。不过，它==实现起来比较复杂==，如果自己写代码实现，难度会有些高，这个时候，我们其实更倾向用跳表来替代它。

### 思考🔖

> 为什么工程中都喜欢用红黑树，而不是其他平衡二叉查找树呢？

Treap、Splay Tree，绝大部分情况下，它们操作的效率都很高，但是也无法避免极端情况下时间复杂度的退化。尽管这种情况出现的概率不大，但是对于单次操作时间非常敏感的场景来说，它们并不适用。

AVL 树是一种高度平衡的二叉树，所以查找的效率非常高，但是，有利就有弊，AVL 树为了维持这种高度的平衡，就要付出更多的代价。每次插入、删除都要做调整，就比较复杂、耗时。所以，对于有频繁的插入、删除操作的数据集合，使用 AVL 树的代价就有点高了。

红黑树只是做到了近似平衡，并不是严格的平衡，所以在维护平衡的成本上，要比 AVL 树要低。

> 动态数据结构支持动态的数据插入、删除、查找操作，除了红黑树，我们前面还学习过哪些呢？能对比一下各自的优势、劣势，以及应用场景吗？

散列表：插入删除查找都是O(1), 是最常用的，但其缺点是不能顺序遍历以及扩容缩容的性能损耗。适用于那些不需要顺序遍历，数据更新不那么频繁的。 

跳表：插入删除查找都是O(logn), 并且能顺序遍历。缺点是空间复杂度O(n)。适用于不那么在意内存空间的，其顺序遍历和区间查找非常方便。 

红黑树：插入删除查找都是O(logn), 中序遍历即是顺序遍历，稳定。缺点是难以实现，去查找不方便。其实跳表更佳，但红黑树已经用于很多地方了。



## 26 红黑树（下）：掌握这些技巧，你也可以实现一个红黑树🔖

红黑树好的是因为它==稳定、高效==的性能，坏的是实现起来实在太==难==了。

### 实现红黑树的基本思想

魔方的复原解法是有固定算法的：遇到哪几面是什么样子，对应就怎么转几下。

红黑树的平衡过程跟魔方复原非常神似，大致过程就是：==遇到什么样的节点排布，我们就对应怎么去调整。==

左旋（rotate left）、右旋（rotate right）

![](images/SJJG+SFZM-26-01.jpg)

### 插入操作的平衡调整

### 删除操作的平衡调整



### 小结

第一点，**把红黑树的平衡调整的过程比作魔方复原，不要过于深究这个算法的正确性**。你只需要明白，只要按照固定的操作步骤，保持插入、删除的过程，不破坏平衡树的定义就行了。

第二点，**找准关注节点，不要搞丢、搞错关注节点**。因为每种操作规则，都是基于关注节点来做的，只有弄对了关注节点，才能对应到正确的操作规则中。在迭代的调整过程中，关注节点在不停地改变，所以，这个过程一定要注意，不要弄丢了关注节点。

第三点，**插入操作的平衡调整比较简单，但是删除操作就比较复杂**。针对删除操作，我们有两次调整，第一次是针对要删除的节点做初步调整，让调整后的红黑树继续满足第四条定义，“每个节点到可达叶子节点的路径都包含相同个数的黑色节点”。但是这个时候，第三条定义就不满足了，有可能会存在两个红色节点相邻的情况。第二次调整就是解决这个问题，让红黑树不存在相邻的红色节点。





## 27 递归树：如何借助树来求解递归算法的时间复杂度？🔖

[12 排序](#12 排序（下）：如何用快排思想在O(n)内查找第K大元素？) 讲过，如何**利用递推公式，求解归并排序、快速排序的时间复杂度**，但是，有些情况，比如快排的平均时间复杂度的分析，用递推公式的话，会涉及非常复杂的数学推导。 

**借助递归树来分析递归算法的时间复杂度**。

### 递归树与时间复杂度分析

递归的思想：**将大问题分解为小问题来求解，然后再将小问题分解为小小问题。**

如果我们把这个一层一层的分解过程画成图，它其实就是一棵树，叫作**==递归树==**。

![斐波那契数列的递归树](./images/SJJG+SFZM-27-01.jpg)

[归并排序算法](#快速排序的原理)

![](./images/SJJG+SFZM-27-02.jpg)

归并算法中分解代价很低，比较耗时的是==归并==操作，也就是把两个子数组合并为大数组。

用高度 h 乘以每一层的时间消耗 n，就得到总的时间复杂度 O(n*h)。

归并排序递归树是一棵满二叉树，满二叉树的高度大约是 log~2~n，所以，归并排序递归实现的时间复杂度就是 O(nlogn)。

### 实战一：分析快速排序的时间复杂度

![](./images/SJJG+SFZM-27-03.jpg)

![](./images/SJJG+SFZM-27-04.jpg)

### 实战二：分析斐波那契数列的时间复杂度



```java
int f(int n) {
  if (n == 1) return 1;
  if (n == 2) return 2;
  return f(n-1) + f(n-2);
}
```



![](images/SJJG+SFZM-27-05.jpg)

### 实战三：分析全排列的时间复杂度





### 小结

有些代码比较适合用递推公式来分析，比如**归并排序的时间复杂度、快速排序的最好情况时间复杂度**；有些比较适合采用递归树来分析，比如**快速排序的平均时间复杂度**。而有些可能两个都不怎么适合使用，比如**二叉树的递归前中后序遍历**。

时间复杂度分析的理论知识并不多，也不复杂，掌握起来也不难，但是，在平时的工作、学习中，面对的代码千差万别，能够灵活应用学到的复杂度分析方法，来分析现有的代码，并不是件简单的事情，所以，平时要多实战、多分析，只有这样，面对任何代码的时间复杂度分析，你才能做到游刃有余、毫不畏惧。

### 思考题

> 1个细胞的生命周期是 3 小时，1 小时分裂一次。求 n 小时后，容器内有多少细胞？请你用已经学过的递归时间复杂度的分析方法，分析一下这个递归问题的时间复杂度。



## 28 堆和堆排序：为什么说堆排序没有快速排序快？

堆是一种特殊的树。

堆排序是一种原地的、时间复杂度为 O(nlogn) 的排序算法。堆的应用场景非常多。

### 如何理解“堆”？

堆作为**==完全二叉树==**的一个特例，特性：

- 最底层节点靠左填充，其他层的节点都被填满。
- 将二叉树的根节点称为“堆顶”，将底层最靠右的节点称为“堆底”。
- 对于==大顶堆==（==小顶堆==），堆顶元素（即根节点）的值分别是最大（最小）的。

![](images/SJJG+SFZM-28-01.jpg)

其中第1个和第2个是大顶堆，第3个是小顶堆，第4个不是堆。

**对于同一组数据，可以构建多种不同形态的堆**。

### 堆常用操作

许多编程语言提供的是「==优先队列 priority queue==」，这是一种抽象数据结构，定义为具有优先级排序的队列。

实际上，**堆通常用作实现优先队列，大顶堆相当于元素按从大到小顺序出队的优先队列**。从使用角度来看，我们可以将“优先队列”和“堆”看作等价的数据结构。

堆的常用操作见下标 ，方法名需要根据编程语言来确定。

![](images/image-20231027194251330.png)

在实际应用中，我们可以直接使用编程语言提供的堆类（或优先队列类）。

> 类似于排序算法中的“从小到大排列”和“从大到小排列”，我们可以通过修改 `Comparator` 来实现“小顶堆”与“大顶堆”之间的转换。

```java
/* 初始化堆 */
// 初始化小顶堆
Queue<Integer> minHeap = new PriorityQueue<>();
// 初始化大顶堆（使用 lambda 表达式修改 Comparator 即可）
Queue<Integer> maxHeap = new PriorityQueue<>((a, b) -> b - a);

/* 元素入堆 */
maxHeap.offer(1);
maxHeap.offer(3);
maxHeap.offer(2);
maxHeap.offer(5);
maxHeap.offer(4);

/* 获取堆顶元素 */
int peek = maxHeap.peek(); // 5

/* 堆顶元素出堆 */
// 出堆元素会形成一个从大到小的序列
peek = maxHeap.poll(); // 5
peek = maxHeap.poll(); // 4
peek = maxHeap.poll(); // 3
peek = maxHeap.poll(); // 2
peek = maxHeap.poll(); // 1

/* 获取堆大小 */
int size = maxHeap.size();

/* 判断堆是否为空 */
boolean isEmpty = maxHeap.isEmpty();

/* 输入列表并建堆 */
minHeap = new PriorityQueue<>(Arrays.asList(1, 3, 2, 5, 4));
```

### 如何实现一个堆？

> 堆都支持哪些操作？
> 
> 如何存储一个堆？

**完全二叉树比较适合用数组来存储。用数组来存储完全二叉树是非常节省存储空间的。**因为不需要存储左右子节点的指针，单纯地通过数组的下标，就可以找到一个节点的左右子节点和父节点。

![](./images/SJJG+SFZM-28-02.jpg)

数组中下标为i的节点的左子节点，就是下标为 `i*2` 的节点，右子节点就是下标为 `i*2+1` 的节点，父节点就是下标为 `i/2` 的节点。

#### 1 往堆中插入一个元素

插入元素时，需要调整位置，让其重新满足堆的特性，这个过程就叫做==堆化（heapify）==。

堆化实际上有两种，从下往上和从上往下。先讲==从下往上==的堆化方法。

![](./images/SJJG+SFZM-28-03.jpg)

堆化非常简单，就是顺着节点所在的路径，向上或者向下，对比，然后交换。

可以让新插入的节点与父节点对比大小。如果不满足子节点小于等于父节点的大小关系，我们就互换两个节点。一直重复这个过程，直到父子节点之间满足刚说的那种大小关系。分解图：

![](./images/SJJG+SFZM-28-04.jpg)

```java
public class Heap {
    /**
     * 数组，从下标1开始存储数据
     */
    private int[] a;
    /**
     * 堆可以存储的最大数据个数
     */
    private int n;
    /**
     * 堆中已经存储的数据个数
     */
    private int count;

    public Heap(int capacity) {
        a = new int[capacity + 1];
        n = capacity;
        count = 0;
    }

    public void insert(int data) {
        if (count >= n) {
            return;
        }
        ++count;
        a[count] = data;
        int i = count;
        // 自下往上堆化
        while (i / 2 > 0 && a[i] > a[i / 2]) {
            swap(a, i, i / 2);
            i = i / 2;
        }
    }

    private void swap(int[] a, int i, int j) {
        int tmp = a[i];
        a[i] = a[j];
        a[j] = tmp;
    }
}
```

#### 2 删除堆顶元素

对于大顶堆，当我们删除堆顶元素之后，就需要把第二大的元素放到堆顶，那第二大元素肯定会出现在左右子节点中。然后我们再迭代地删除第二大节点，以此类推，直到叶子节点被删除。

不过这种方法有点问题，就是最后堆化出来的堆并不满足完全二叉树的特性：

![](./images/SJJG+SFZM-28-05.jpg)

稍微改变一下思路，就可以解决这个问题。把最后一个节点放到堆顶，然后利用同样的父子节点对比方法。对于不满足父子节点大小关系的，互换两个节点，并且重复进行这个过程，直到父子节点之间满足大小关系为止。这就是**==从上往下==的堆化方法**。

因为我们移除的是数组中的最后一个元素，而在堆化的过程中，都是交换操作，不会出现数组中的“空洞”，所以这种方法堆化之后的结果，肯定满足完全二叉树的特性。

![](./images/SJJG+SFZM-28-06.jpg)

```java
 public void removeMax() {
        // 堆中没有数据
        if (count == 0) {
            return;
        }
        a[1] = a[count];
        --count;
        heapify(a, count, 1);
    }

    /**
     * 自上往下堆化
     */
    private void heapify(int[] a, int n, int i) {
        while (true) {
            int maxPos = i;
            if (i * 2 <= n && a[i] < a[i*2]) {
                maxPos = i * 2;
            }
            if (i * 2 + 1 <= n && a[maxPos] < a[i * 2 + 1]) {
                maxPos = i * 2 + 1;
            }
            if (maxPos == i) {
                break;
            }
            swap(a, i, maxPos);
            i = maxPos;
        }
    }
```

一个包含n个节点的完全二叉树，树的高度不会超过log~2~n。堆化的过程是顺着节点所在路径比较交换的，所以堆化的时间复杂度跟树的高度成正比，也就是O(logn)。插入数据和删除堆顶元素的主要逻辑就是堆化，所以，往堆中插入一个元素和删除堆顶元素的时间复杂度都是 O(logn)。

### 如何基于堆实现排序？

借助于堆实现的排序算法，就叫做==堆排序==，它时间复杂度非常稳定，是 `O(nlogn)`，并且它还是原地排序算法。

堆排序分为两个大步骤：

#### 1 建堆

两种思路：

- 第一种是借助前面讲的，在堆中插入一个元素的思路。尽管数组中包含n个数据，但是可以假设，起初堆中只包含一个数据，就是下标为1的数据。然后，我们调用前面讲的插入操作，将下标从 2 到 n 的数据依次插入到堆中。这样我们就将包含n个数据的数组，组织成了堆。

- 第二种实现思路，跟第一种截然相反。第一种建堆思路的处理过程是从前往后处理数组数据，并且每个数据插入堆中时，都是从下往上堆化。而第二种实现思路，是**从后往前处理数组**，并且每个数据都是从上往下堆化。
  
  因为叶子节点往下堆化只能自己跟自己比较，所以我们直接从最后一个非叶子节点开始，依次堆化就行了。

![](images/image-20230321193045098.png)

第二种实现思路翻译成了代码：

```java
   private static void buildHeap(int[] arr) {
        // (arr.length - 1) / 2 为最后一个叶子节点的父节点
        // 也就是最后一个非叶子节点，依次堆化直到根节点
        for (int i = (arr.length - 1) / 2; i >= 0; i--) {
            heapify(arr, arr.length - 1, i);
        }
    }

    /**
     * 堆化
     * @param arr 要堆化的数组
     * @param n   最后堆元素下标
     * @param i   要堆化的元素下标
     */
    private static void heapify(int[] arr, int n, int i) {
        while (true) {
            // 最大值位置
            int maxPos = i;
            // 与左子节点（i * 2 + 1）比较，获取最大值位置
            if (i * 2 + 1 <= n && arr[i] < arr[i * 2 + 1]) {
                maxPos = i * 2 + 1;
            }
            // 最大值与右子节点（i * 2 + 2）比较，获取最大值位置
            if (i * 2 + 2 <= n && arr[maxPos] < arr[i * 2 + 2]) {
                maxPos = i * 2 + 2;
            }
            // 最大值是当前位置结束循环
            if (maxPos == i) {
                break;
            }
            // 与子节点交换位置
            swap(arr, i, maxPos);
            // 以交换后子节点位置接着往下查找
            i = maxPos;
        }
    }
```

对于完全二叉树来说，下标n/2 + 1 到n的节点是叶子节点，不需要堆化，所以buildheap方法中从`(arr.length - 1) / 2`开始循环堆化。

##### 建堆操作的时间复杂度推导

因为叶子节点不需要堆化，所以需要堆化的节点从**倒数第二层**开始。每个节点堆化的过程中，需要比较和交换的节点个数，跟这个节点的高度成正比。

把每一层的节点个数和对应的高度画了出来，只需要将每个节点的高度求和，得出的就是建堆的时间复杂度。

![](images/SJJG+SFZM-28-08.jpg)

将每个非叶子节点的高度求和：

![](images/SJJG+SFZM-28-09.jpg)

把公式左右都乘以 2，就得到另一个公式 S2。将 S2 错位对齐，并且用 S2 减去 S1，可以得到 S。

![](images/SJJG+SFZM-28-10.jpg)

![](images/SJJG+SFZM-28-11.jpg)

> ![](images/image-20231028111818327.png)

因为h=log~2~n，代入公式 S，就能得到 S=O(n)，所以，建堆的时间复杂度就是 ==O(n)==。

#### 2 排序

建堆结束之后，数组中的数据已经是按照**大顶堆**的特性来组织的。数组中的第一个元素就是堆顶，也就是最大的元素。把它跟最后一个元素交换，那最大元素就放到了下标为 n 的位置。

这个过程有点类似上面讲的“删除堆顶元素”的操作，当堆顶元素移除之后，我们把下标为 n 的元素放到堆顶，然后再通过堆化的方法，将剩下的 n−1 个元素重新构建成堆。堆化完成之后，我们再取堆顶的元素，放到下标是 n−1 的位置，一直重复这个过程，直到最后堆中只剩下标为 1 的一个元素，排序工作就完成了。

![](images/SJJG+SFZM-28-12.jpg)

```java
   /**
     * 排序
     * 堆元素是从数组下标0开始
     */
    public static void sort(int[] arr) {
        if (arr.length <= 1) {
            return;
        }

        // 1、建堆
        buildHeap(arr);

        // 2、排序
        int k = arr.length - 1;
        while (k > 0) {
            // 将堆顶元素（最大）与最后一个元素交换位置
            swap(arr, 0, k);
            // 将剩下元素重新堆化，堆顶元素变成最大元素
            heapify(arr, --k, 0);
        }
    }
```

##### 分析堆排序的时间复杂度、空间复杂度以及稳定性

整个堆排序的过程，都只需要**极个别临时存储空间**，所以堆排序是==原地排序算法==。堆排序包括建堆和排序两个操作，建堆过程的时间复杂度是 O(n)，排序过程的时间复杂度是 O(nlogn)，所以，堆排序整体的时间复杂度是 O(nlogn)。

堆排序**不是稳定的排序算法**，因为在排序的过程，存在将堆的最后一个节点跟堆顶节点互换的操作，所以就有可能改变值相同数据的原始相对顺序。

> 上面加色堆中的数据是从数组下标为 1 的位置开始存储。那如果从 0 开始存储，实际上处理思路是没有任何变化的，唯一变化的，可能就是，代码实现的时候，计算子节点和父节点的下标的公式改变了。
> 
> 如果节点的下标是 i，那左子节点的下标就是 `2*i+1`，右子节点的下标就是 `2*i+2`，父节点的下标就是 (i−1)/2。

### 小结

![](images/iShot_2023-10-26_21.49.08.png)

堆被分成了两类，大顶堆和小顶堆。

堆中比较重要的两个操作是**插入一个数据和删除堆顶元素**。这两个操作都要用到**堆化**。插入一个数据的时候，我们把新插入的数据放到数组的最后，然后从下往上堆化；删除堆顶数据的时候，我们把数组中的最后一个元素放到堆顶，然后从上往下堆化。这两个操作时间复杂度都是 O(logn)。

堆的一个经典应用，**堆排序**。堆排序包含两个过程，**建堆和排序**。我们将下标从 n/2 到 1 的节点，依次进行从上到下的堆化操作，然后就可以将数组中的数据组织成堆这种数据结构。接下来，我们迭代地将堆顶的元素放到堆的末尾，并将堆的大小减一，然后再堆化，重复这个过程，直到堆中只剩下一个元素，整个数组中的数据就都有序排列了。

### 思考题

> 两种排序算法的时间复杂度都是 `O(nlogn)`，甚至堆排序比快速排序的时间复杂度还要稳定，但是，在实际的软件开发中，==快速排序的性能要比堆排序好==，这是为什么呢？

1. 堆排序**数据访问的方式**没有快速排序友好。
   
   对于快速排序来说，数据是顺序访问的。而对于堆排序来说，数据是**跳着访问**的。 比如，堆排序中，最重要的一个操作就是数据的堆化。下面这个例子，对堆顶节点进行堆化，会依次访问数组下标是 1，2，4，8 的元素，而不是像快速排序那样，局部顺序访问，所以，这样**对CPU缓存是不友好的**。
   
   ![](images/SJJG+SFZM-28-13.jpg)

2. 对于同样的数据，在排序过程中，堆排序算法的**数据交换次数**要多于快速排序。
   
   讲排序的时候提过两个概念，==有序度==和==逆序度==。对于基于**比较**的排序算法来说，整个排序过程就是由两个基本的操作组成的，**比较和交换（或移动）**。快速排序数据交换的次数不会比逆序度多。
   
   但是堆排序的第一步是建堆，建堆的过程会打乱数据原有的相对先后顺序，导致原数据的有序度降低。比如，对于一组已经有序的数据来说，经过建堆之后，数据反而变得更无序了。

可以通过添加一个记录交换次数的变量，来统计交换次数，可直观得出**堆排序比快速排序交换次数多**。

> 对于完全二叉树来说，下标从 n/2+1 到 n 的都是叶子节点，这个结论是怎么推导出来的呢？

假设堆有n个节点，满二叉树有h层，则满二叉树的总节点数：  

```
2^0+2^1...+2^(h-2)+2^(h-1) = (2^h)-1
```

如果堆为完全二叉树，相同高度，完全二叉树总结点数小于满二叉树节点数，即 

```
n < (2^h)-1， 即 (2^h)>n+1 -----① 
```

完全二叉树1到h-1层节点的数量总和： 

```
2^0+2^1...+2^(h-2) = (2^(h-1))-1=(2^h)/2 -1  -----② 
```

如果数组的第0位也存储数据，由②可知，完全二叉树的第h层开始的节点的下标为i=(2^h)/2 -1，由①，i>((n+1)/2)-1=(n/2)+1 

结论1：如果数组的第0位也存储数据，完全二叉树的节点下标至少开始于(n/2)+1 如果数组的第0位不存储数据，则由②可知，完全二叉树的第h层开始的节点的下标为j=(2^h)/2，由①，j>(n+1)/2=(n/2)+2 

结论2：如果数组的第0位不存储数据，完全二叉树的节点下标至少开始于(n/2)+2 综上，堆（完全二叉树）的叶子节点的下标范围从(n/2)+1到n-1或从(n/2)+2到n，也即堆的叶子节点下标从(n/2)+1到n

> 堆的其他应用？

1. top K 
2. 流里面的中值 
3. 流里面的中位数



## 29 堆的应用：如何快速获取到Top 10最热门的搜索关键词？

搜索引擎的热门搜索排行榜功能是如何实现的？

搜索引擎每天会接收大量的用户搜索请求，它会把这些用户输入的搜索关键词记录下来，然后再离线地统计分析，得到最热门的 Top 10 搜索关键词。

### 堆的应用一：优先级队列

队列最大的特性就是**先进先出**。不过，在优先级队列中，数据的出队顺序不是先进先出，而是**按照优先级来，优先级最高的，最先出队**。

实现一个优先级队列方法有很多，但是用堆来实现是**最直接、最高效的**。这是因为，堆和优先级队列**非常相似**。一个堆就可以看作一个优先级队列。很多时候，它们只是概念上的区分而已。往优先级队列中插入一个元素，就相当于往堆中插入一个元素；从优先级队列中取出优先级最高的元素，就相当于取出堆顶元素。

优先级队列的应用场景非常多。很多数据结构和算法都要依赖它，比如，**赫夫曼编码、图的最短路径、最小生成树算法**等等。

很多语言中，都提供了优先级队列的实现，比如，Java的`PriorityQueue`，C++的 `priority_queue`等。

优先级队列的具体用法：

#### 1 合并有序小文件

> 假设有100个小文件，每个文件的大小是100MB，每个文件中存储的都是有序的字符串。希望将这些100个小文件合并成一个有序的大文件。这里就会用到优先级队列。

整体思路有点像归并排序中的合并函数。从这100个文件中，各取第一个字符串，放入数组中，然后比较大小，把最小的那个字符串放入合并后的大文件中，并从数组中删除。

假设，这个最小的字符串来自于13.txt这个小文件，就再从这个小文件取下一个字符串，放到数组中，重新比较大小，并且选择最小的放入合并后的大文件，将它从数组中删除。依次类推，直到所有的文件中的数据都放入到大文件为止。

使用数据来存储从小文件中取出来的字符串。每次从数组中取最小字符串，都需要循环遍历整个数组，显然，这不是很高效。有没有更加高效方法呢？

更高效的方法是用优先级队列，也可以说是堆。将**从小文件中取出来的字符串放入到小顶堆中**，那堆顶的元素，也就是优先级队列队首的元素，就是最小的字符串。将这个字符串放入到大文件中，并将其从堆中删除。然后再从小文件中取出下一个字符串，放入到堆中。循环这个过程，就可以将100个小文件中的数据依次放入到大文件中。

删除堆顶数据和往堆中插入数据的时间复杂度都是 O(logn)，n 表示堆中的数据个数，这里就是100。这就比原来数组存储的方式高效了很多。

#### 2 高性能定时器🔖

假设我们有一个定时器，定时器中维护了很多定时任务，每个任务都设定了一个要触发执行的时间点。定时器每过一个很小的单位时间（比如 1 秒），就扫描一遍任务，看是否有任务到达设定的执行时间。如果到达了，就拿出来执行。

但是，这样每过1秒就扫描一遍任务列表的做法比较低效，主要原因有两点：

- 第一，任务的约定执行时间离当前时间可能还有很久，这样前面很多次扫描其实都是徒劳的；

- 第二，每次都要扫描整个任务列表，如果任务列表很大的话，势必会比较耗时。

使用优先级队列来解决上面的问题。按照任务设定的执行时间，将这些任务存储在优先级队列中，队列首部（也就是小顶堆的堆顶）存储的是最先执行的任务。

这样，定时器就不需要每隔 1 秒就扫描一遍任务列表了。它拿队首任务的执行时间点，与当前时间点相减，得到一个时间间隔 T。

这个时间间隔 T 就是，从当前时间开始，需要等待多久，才会有第一个任务需要被执行。这样，定时器就可以设定在 T 秒之后，再来执行任务。从当前时间点到（T-1）秒这段时间里，定时器都不需要做任何事情。

当 T 秒时间过去之后，定时器取优先级队列中队首的任务执行。然后再计算新的队首任务的执行时间点与当前时间点的差值，把这个值作为定时器执行下一个任务需要等待的时间。

这样，定时器既不用间隔 1 秒就轮询一次，也不用遍历整个任务列表，性能也就提高了。

### 堆的应用二：利用堆求Top K

把这种求Top K 的问题抽象成两类。

- 一类是针对**静态数据集合**，也就是说数据集合事先确定，不会再变。
- 另一类是针对**动态数据集合**，也就是说数据集合事先并不确定，有数据动态地加入到集合中。

针对静态数据，如何在一个包含 n 个数据的数组中，查找前 K 大数据呢？

可以维护一个大小为 K 的小顶堆，顺序遍历数组，从数组中取出数据与堆顶元素比较。如果比堆顶元素大，我们就把堆顶元素删除，并且将这个元素插入到堆中；如果比堆顶元素小，则不做处理，继续遍历数组。这样等数组中的数据都遍历完之后，堆中的数据就是前 K 大数据了。

遍历数组需要 O(n) 的时间复杂度，一次堆化操作需要 O(logK) 的时间复杂度，所以最坏情况下，n 个元素都入堆一次，时间复杂度就是 O(nlogK)。

针对动态数据求得 Top K 就是实时 Top K。比如，一个数据集合中有两个操作，一个是添加数据，另一个询问当前的前 K 大数据。

如果每次询问前 K 大数据，都基于当前的数据重新计算的话，那时间复杂度就是 O(nlogK)，n 表示当前的数据的大小。实际上，可以一直都维护一个 K 大小的小顶堆，当有数据被添加到集合中时，就拿它与堆顶的元素对比。如果比堆顶元素大，就把堆顶元素删除，并且将这个元素插入到堆中；如果比堆顶元素小，则不做处理。这样，无论任何时候需要查询当前的前 K 大数据，都可以立刻返回给他。

### 堆的应用三：利用堆求中位数

如何求动态数据集合中的中位数？

中位数，就是处在**中间位置的那个数**。如果数据的个数是奇数，把数据从小到大排列，那第n/2 + 1个数据就是中位数（注意：假设数据是从 0 开始编号的）；如果数据的个数是偶数的话，那处于中间位置的数据有两个，第n/2个和第n/2 + 1个数据，这个时候，我们可以随意取一个作为中位数，比如取两个数中靠前的那个，就是第n/2个数据。

![](images/SJJG+SFZM-29-02.jpg)

对于一组**静态数据**，中位数是固定的，可以先排序，第n/2个数据就是中位数。每次询问中位数的时候，我们直接返回这个固定的值就好了。所以，**尽管排序的代价比较大，但是边际成本会很小**。

但是，如果面对的是**动态数据**集合，中位数在不停地变动，如果再用先排序的方法，每次询问中位数的时候，都要先进行排序，那效率就不高了。

借助堆就不用排序，可以非常高效地实现求中位数操作。需要维护两个堆，一个大顶堆，一个小顶堆。大顶堆中存储前半部分数据，小顶堆中存储后半部分数据，且小顶堆中的数据都大于大顶堆中的数据。

也就是说，如果有n个数据，n是偶数，我们从小到大排序，那前n/2个数据存储在大顶堆中，后n/2个数据存储在小顶堆中。这样，大顶堆中的堆顶元素就是要找的中位数。如果 n是奇数，情况是类似的，大顶堆就存储 n/2 + 1 个数据，小顶堆中就存储 n/2 个数据。

![](images/SJJG+SFZM-29-03.jpg)

数据是动态变化的，当新添加一个数据的时候，如果新加入的数据小于等于大顶堆的堆顶元素，就将这个新数据插入到大顶堆；否则，就将这个新数据插入到小顶堆。

这个时候就有可能出现，两个堆中的数据个数不符合前面约定的情况：如果 n 是偶数，两个堆中的数据个数都是n/2；如果 n 是奇数，大顶堆有n/2 + 1个数据，小顶堆有 n/2个数据。这个时候，可以从一个堆中不停地将堆顶元素移动到另一个堆，通过这样的调整，来让两个堆中的数据满足上面的约定。

![](images/SJJG+SFZM-29-04.jpg)

于是，就可以利用两个堆，一个大顶堆、一个小顶堆，实现在动态数据集合中求中位数的操作。插入数据因为需要涉及堆化，所以时间复杂度变成了 O(logn)，但是求中位数我们只需要返回大顶堆的堆顶元素就可以了，所以时间复杂度就是 O(1)。

实际上，**利用两个堆不仅可以快速求出中位数，还可以快速求其他百分位的数据**，原理是类似的，根据百分比分配大小顶堆中结点数量即可。

> 如何快速求接口的**99% 响应时间**？
>
> **中位数**的概念就是将数据从小到大排列，处于中间位置，就叫中位数，这个数据会大于等于前面 50% 的数据。
>
> **99百分位数**的概念可以类比中位数，如果将一组数据从小到大排列，这个 99 百分位数就是大于前面99%数据的那个数据。
>
> 举例说明，假设有 100 个数据，分别是 1，2，3，……，100，那 99 百分位数就是 99，因为小于等于 99 的数占总个数的 99%。
>
> ![](images/SJJG+SFZM-29-05.jpg)
>
> 如果有 100 个接口访问请求，每个接口请求的响应时间都不同，比如 55 毫秒、100 毫秒、23 毫秒等，我们把这 100 个接口的响应时间按照从小到大排列，排在第 99 的那个数据就是**99%响应时间**，也叫 **99百分位响应时间**。
>
> 总结，如果有 n 个数据，将数据从小到大排列之后，99百分位数大约就是第 n*99% 个数据，同类，80百分位数大约就是第 `n*80%` 个数据。

维护两个堆，一个大顶堆，一个小顶堆。假设当前总数据的个数是 n，大顶堆中保存 `n*99%` 个数据，小顶堆中保存 `n*1%` 个数据。大顶堆堆顶的数据就是我们要找的99% 响应时间。

每次插入一个数据的时候，要**判断这个数据跟大顶堆和小顶堆堆顶数据的大小关系**，然后决定插入到哪个堆中。如果这个新插入的数据比大顶堆的堆顶数据小，那就插入大顶堆；如果这个新插入的数据比小顶堆的堆顶数据大，那就插入小顶堆。

但是，为了保持大顶堆中的数据占 99%，小顶堆中的数据占 1%，在每次新插入数据之后，都要重新计算，这个时候大顶堆和小顶堆中的数据个数，是否还符合 99:1 这个比例。如果不符合，就将一个堆中的数据**移动**到另一个堆，直到满足这个比例。

每次插入数据，可能会涉及几个数据的堆化操作，所以时间复杂度是 O(logn)。每次求 99% 响应时间的时候，直接返回大顶堆中的堆顶数据即可，时间复杂度是 O(1)。

### 小结

![](images/iShot_2023-10-28_12.11.32.png)

优先级队列是一种特殊的队列，优先级高的数据先出队，而不再像普通的队列那样，先进先出。实际上，堆就可以看作优先级队列，只是称谓不一样罢了。

求Top K问题又可以分为针对静态数据和针对动态数据，只需要利用一个堆，就可以做到非常高效率地查询 Top K 的数据。

求中位数实际上还有很多变形，比如求 99 百分位数据、90 百分位数据等，处理的思路都是一样的，即利用两个堆，一个大顶堆，一个小顶堆，随着数据的动态添加，动态调整两个堆中的数据，最后大顶堆的堆顶元素就是要求的数据。

### 思考题

> 有一个包含 10 亿个搜索关键词的日志文件，如何快速获取到 Top 10 最热门的搜索关键词呢？
>
> 处理这个问题，有很多高级的解决方法，比如使用 MapReduce 等。但是，如果将处理的场景限定为单机，可以使用的内存为 1GB。那这个问题该如何解决呢？

因为用户搜索的关键词，有很多可能都是重复的，所以首先要统计每个搜索关键词出现的频率。我们可以通过散列表、平衡二叉查找树或者其他一些支持**快速查找、插入**的数据结构，来记录关键词及其出现的次数。

假设选用散列表。顺序扫描这 10 亿个搜索关键词。当扫描到某个关键词时，去散列表中查询。如果存在，我们就将对应的次数加1；如果不存在，就将它插入到散列表，并记录次数为 1。以此类推，等遍历完这 10 亿个搜索关键词之后，散列表中就存储了**不重复的搜索关键词以及出现的次数**。

然后，再根据前面讲的用堆求 Top K 的方法，建立一个大小为 10 的小顶堆，遍历散列表，依次取出每个搜索关键词及对应出现的次数，然后与堆顶的搜索关键词对比。如果出现次数比堆顶搜索关键词的次数多，那就删除堆顶的关键词，将这个出现次数更多的关键词加入到堆中。

以此类推，当遍历完整个散列表中的搜索关键词之后，堆中的搜索关键词就是出现次数最多的 Top 10 搜索关键词了。

不知道你发现了没有，上面的解决思路其实存在漏洞。10亿的关键词还是很多的。假设 10 亿条搜索关键词中不重复的有 1 亿条，如果每个搜索关键词的平均长度是 50 个字节，那存储 1 亿个关键词起码需要 5GB 的内存空间，而散列表因为要避免频繁冲突，不会选择太大的装载因子，所以消耗的内存空间就更多了。而我们的机器只有 1GB 的可用内存空间，所以我们无法一次性将所有的搜索关键词加入到内存中。这个时候该怎么办呢？

在哈希算法那一节讲过，**相同数据经过哈希算法得到的哈希值是一样的**。可以根据哈希算法的这个特点，将 10 亿条搜索关键词先通过哈希算法分片到 10 个文件中。

具体可以这样做：创建 10 个空文件 00，01，02，……，09。遍历这 10 亿个关键词，并且通过某个哈希算法对其求哈希值，然后哈希值同10取模，得到的结果就是这个搜索关键词应该被分到的文件编号。

对这 10 亿个关键词分片之后，每个文件都只有 1 亿的关键词，去除掉重复的，可能就只有 1000 万个，每个关键词平均 50 个字节，所以总的大小就是 500MB。1GB 的内存完全可以放得下。

针对每个包含 1 亿条搜索关键词的文件，利用散列表和堆，分别求出 Top 10，然后把这个 10 个 Top 10 放在一块，然后取这 100 个关键词中，出现次数最多的 10 个关键词，这就是这 10 亿数据中的 Top 10 最频繁的搜索关键词了。

> 有一个访问量非常大的新闻网站，我们希望将点击量排名Top 10的新闻摘要，滚动显示在网站首页 banner 上，并且每隔1小时更新一次。如果你是负责开发这个功能的工程师，你会如何来实现呢？🔖





## 30 图的表示：如何存储微博、微信等社交网络中的好友关系？

![](images/image-20231028201435712.png)

涉及图的算法：**图的搜索、最短路径、最小生成树、二分图**等等。

### 如何理解“图”？

图（Graph） ，比树更加复杂的非线性表结构。

==顶点（vertex）== 

==边（edge）==，图中的一个顶点可以与任意其他顶点建立连接关系。

![](images/SJJG+SFZM-30-01.jpg)

整个微信的好友关系就可以用一张图来表示，其中，每个用户有多少个好友，对应到图中，就叫做顶点的**==度（degree）==**，就是**跟顶点相连接的边的条数**。

微博的社交关系比微信更复杂一点，微博允许**单向关注**。

==“有向图”==  ==“无向图”==

![](images/SJJG+SFZM-30-02.jpg)

在有向图中，度分为**==入度==（In-degree）**和**==出度==（Out-degree）**。

> 对应微博，入度就表示有多少粉丝，出度就表示关注了多少人。

QQ的社交关系比微博更复杂一点，好有关系有**亲密度**。

**==带权图==（weighted graph）**，每条边都有一个权重（weight）（可以通过这个权重来表示QQ好友间的亲密度）。

![](images/SJJG+SFZM-30-03.jpg)

#### 邻接矩阵存储方法

图最直观的一种存储方法：**==邻接矩阵==**（Adjacency Matrix）。

邻接矩阵的底层依赖一个**二维数组**。

对于无向图来说，如果顶点i与顶点j之间有边，我们就将`A[i][j]`和`A[j][i]`标记为 1。

对于有向图来说，如果顶点i到顶点j之间，有一条箭头从顶点i指向顶点j的边，那我们就将`A[i][j]`标记为1；同理，如果有一条箭头从顶点 j 指向顶点 i 的边，就将`A[j][i]`标记为1。

对于带权图，数组中就存储相应的权重。

![](images/SJJG+SFZM-30-04.jpg)

用邻接矩阵来表示一个图，虽然简单、直观，但是比较**浪费存储空间**。

对于无向图来说，如果 `A[i][j]`等于 1，那 `A[j][i]`也肯定等于 1。实际上，我们只需要存储一个就可以了。也就是说，无向图的二维数组中，如果我们将其用对角线划分为上下两部分，那我们只需要利用上面或者下面这样一半的空间就足够了，另外一半白白浪费掉了。

> ==稀疏图==（Sparse Matrix）：顶点很多，但每个顶点的边并不多。

对于稀疏图，邻接矩阵存储方式更加浪费空间。比如微信有好几亿的用户，对应到图上就是好几亿的顶点。但是每个用户的好友并不会很多，一般也就三五百个而已。如果我们用邻接矩阵来存储，那绝大部分的存储空间都被浪费了。

邻接矩阵存储方法的优点：

- 存储方式简单、直接，因为基于数组，所以在获取两个顶点的关系时，就非常高效。

- 方便计算。用邻接矩阵的方式存储图，可以将很多图的运算转换成矩阵之间的运算。比如求解最短路径问题时会提到一个[Floyd-Warshall 算法]([Floyd-Warshall算法 - 维基百科，自由的百科全书 (wikipedia.org)](https://zh.wikipedia.org/wiki/Floyd-Warshall算法))🔖，就是利用矩阵循环相乘若干次得到结果。
  

#### 邻接表存储方法

**==邻接表==（Adjacency List）**（有点像散列表）

> ==邻接矩阵空间大，耗时少；邻接表空间小，耗时多==。【时间、空间复杂度互换的设计思想】

每个顶点对应一条链表，链表中存储的是与这个顶点相连接的其他顶点（有向图，存储的是指向的顶）。

有向图的邻接表存储方式：

![有向图的邻接表存储方式](images/SJJG+SFZM-30-05.jpg)

- 每个顶点对应一条链表，链表中存储的是与这个顶点相连接的其他顶点。
- 上图是有向图的邻接表存储方式，每个顶点对应的链表里面，存储的是**指向的顶点**。
- 无向图稍有不同，每个顶点的链表中存储的，是跟这个顶点有边**相连的顶点**。

现在如果要确定，是否存在一条从顶点2到顶点4的边，那就要遍历顶点2对应的那条链表，看链表中是否存在顶点4。

链表的存储方式对缓存不友好。所以，比起邻接矩阵的存储方式，在邻接表中查询两个顶点之间的关系就没那么高效了。



> 如果链表太长，可以像散列表那样，把链表改成其他更高效的数据结构，比如平衡二叉查找树、跳表等。
>
> 实际开发中，选择用红黑树，这样就可以更加快速地查找两个顶点之间是否存在边了。
>
> 还可以将链表改成有序动态数组，可以通过二分查找的方法来快速定位两个顶点之间否是存在边。

### 思考题

**数据结构是为算法服务的，所以具体选择哪种存储方法，与期望支持的操作有关系。**

> 如何存储微博、微信等这些社交网络的好友关系吗？

以微博为例，针对微博用户关系，假设需要支持下面这样几个操作：

- 判断用户 A 是否关注了用户B；
- 判断用户 A 是否是用户 B的粉丝；
- 用户 A 关注用户 B；
- 用户 A 取消关注用户 B；
- 根据用户名称的首字母排序，分页获取用户的粉丝列表；
- 根据用户名称的首字母排序，分页获取用户的关注列表。

因为社交网络是一张稀疏图，所以采用领接表来存储。不过，只用一个邻接表存储这种有向图是不够的，查找某个用户关注了哪些用户非常容易，但是如果要想知道某个用户都被哪些用户关注了，也就是用户的**粉丝列表**，是非常困难的。

因此需要一个==逆邻接表==。邻接表中存储了用户的关注关系，逆邻接表中存储的是用户的被关注关系。

![](images/SJJG+SFZM-30-06.jpg)



基础的邻接表不适合快速判断两个用户之间是否是关注与被关注的关系，要将邻接表中的链表改为支持快速查找的**动态数据结构**。红黑树、跳表、有序动态数组还是散列表呢？

因为需要<u>按照用户名称的首字母排序，分页</u>来获取用户的粉丝列表或者关注列表，用==跳表==这种结构再合适不过了。这是因为，**跳表插入、删除、查找都非常高效，时间复杂度是 O(logn)，空间复杂度上稍高，是 O(n)**。最重要的一点，跳表中存储的数据本来就是**有序的**了，分页获取粉丝列表或关注列表，就非常高效。

如果对于小规模的数据，比如社交网络中只有几万、几十万个用户，可以将整个社交关系存储在内存中，上面的解决思路是没有问题的。但是如果像微博那样有上亿的用户，数据规模太大，就无法全部存储在内存中了。这个时候该怎么办呢？

可以通过哈希算法等==数据分片==方式，将邻接表存储在不同的机器上。下图里，在机器 1 上存储顶点 1，2，3 的邻接表，在机器 2 上，存储顶点 4，5 的邻接表。逆邻接表的处理方式也一样。当要查询顶点与顶点关系的时候，就利用同样的哈希算法，先定位顶点所在的机器，然后再在相应的机器上查找。

![](images/SJJG+SFZM-30-07.jpg)



另外一种解决思路，就是利用外部存储（比如硬盘），因为外部存储的存储空间要比内存会宽裕很多。数据库是我们经常用来持久化存储关系数据的。

为了高效地支持前面定义的操作，可以在表上建立多个索引，比如第一列、第二列，给这两列都建立索引。

![](images/7339595c631660dc87559bec2ddf928f.jpg)

> 像微信这种无向图，应该怎么存储呢？

也可以使用邻接表的方式存储每个人所对应的好友列表。为了支持快速查找，好友列表可以使用红黑树存储。

> 除了社交网络可以用图来表示之外，符合图这种结构特点的例子还有很多，比如==知识图谱==（Knowledge Graph）。关于图这种数据结构，你还能想到其他生活或者工作中的例子吗？

地图 

网络 

Gradle这个编译工具，内部组织task的方式用的是有向图 

Android framework层提供了一个CoordinatorLayout，其内部协调子view的联动，也是用的图

> 解决现实问题的时候当存储图有多种选择，例如: 1.用邻接表自己存 2.关系型库 3.图数据库 ，那么这三种方式每一种的适用场景，优缺点分别是什么呢？该如何取舍

内存中用邻接表；

要持久化存储就用数据库；

超大图，并且涉及大量图计算，用专业的图数据库。

### 小结

无向图、有向图、带权图、顶点、边、度、入度、出度

图的两个主要的存储方式：邻接矩阵和邻接表。

邻接表改进升级版，即将链表换成更加高效的动态数据结构，比如平衡二叉查找树、跳表、散列表等。

![](images/iShot_2023-12-02_19.46.09.png)





## 31 深度和广度优先搜索：如何找出社交网络中的三度好友关系？

==六度分割理论==，你与世界上的另一个人间隔的关系不会超过六度，也就是说**平均只需要六步就可以联系到任何两个互不相识的人**。

### 什么是“搜索”算法？

**==算法是作用于具体数据结构之上的==**，深度优先搜索算法和广度优先搜索算法都是基于“图”这种数据结构的。这是因为，图这种数据结构的**==表达能力==**很强，**大部分涉及搜索的场景都可以抽象成“图”**。

图上的搜索算法，可以理解为**在图中找出从一个顶点出发，到另一个顶点的==路径==**。具体方法有很多，比如最简单、最“暴力”的深度优先、广度优先搜索，还有`A*`、`IDA*` 等启发式搜索算法。

```java
public class Graph {
    /**
     * 顶点的个数
     */
    private int v;
    /**
     * 邻接表
     */
    private LinkedList<Integer> adj[];

    public Graph(int v) {
        this.v = v;
        adj = new LinkedList[v];
        for (int i = 0; i < v; ++i) {
            adj[i] = new LinkedList<>();
        }
    }

    /**
     * 无向图一条边存两次
     */
    public void addEdge(int s, int t) {
        adj[s].add(t);
        adj[t].add(s);
    }
}
```

深度优先搜索算法和广度优先搜索算法，既可以用在无向图，也可以用在有向图上。以无向图为例。

### 广度优先搜索

==广度优先搜索（Breadth-First-Search，BFS）==，就是一种“地毯式”层层推进的搜索策略，即<u>先查找离起始顶点最近的，然后是次近的，依次往外搜索</u>。

![](./images/SJJG+SFZM-31-01.jpg)

BFS原理简单，代码实现复杂。

```java
    /**
     *
     * @param s 起始顶点
     * @param t 终止顶点
     */
    public void bfs(int s, int t) {
        if (s == t) {
            return;
        }
        // 记录顶点是否已经被访问
        boolean[] visited = new boolean[v];
        visited[s] = true;
        // 已经被访问、但相连的顶点还没有被访问的顶点
        Queue<Integer> queue = new LinkedList<>();
        queue.add(s);
        // 记录搜索路径。反向存储，prev[w]存储的是，顶点w是从哪个前驱顶点遍历过来的。
        int[] prev = new int[v];
        int w = queue.poll();
        for (int i = 0; i < adj[w].size(); ++i) {
            int q = adj[w].get(i);
            if (!visited[q]) {
                prev[q] = w;
                if (q == t) {
                    print(prev, s, t);
                    return;
                }
                visited[q] = true;
                queue.add(q);
            }
        }

    }
    /**
     * 递归打印s->t路径
     * @param prev
     * @param s
     * @param t
     */
    private void print(int[] prev, int s, int t) {
        if (prev[t] != -1 && t != s) {
            print(prev, s, prev[t]);
        }
        System.out.print(t + "");
    }
```

三个辅助变量：

- `visited` 是用来记录已经被访问的顶点，用来避免顶点被重复访问。
- `queue` 是一个队列，用来存储已经被访问、但相连的顶点还没有被访问的顶点。
- `prev` 用来记录搜索路径。

广度优先搜索的分解图：

![](images/SJJG+SFZM-31-02.png)



> 广度优先搜索的时间、空间复杂度是多少呢？

最坏情况下，终止顶点 t 离起始顶点 s 很远，需要遍历完整个图才能找到。这个时候，每个顶点都要进出一遍队列，每个边也都会被访问一次，所以，广度优先搜索的时间复杂度是 O(V+E)，其中，V 表示顶点的个数，E 表示边的个数。当然，对于一个==连通图==来说，也就是说一个图中的所有顶点都是连通的，E 肯定要大于等于 V-1，所以，广度优先搜索的时间复杂度也可以简写为 `O(E)`。

广度优先搜索的空间消耗主要在几个辅助变量 visited 数组、queue 队列、prev 数组上。这三个存储空间的大小都不会超过顶点的个数，所以空间复杂度是 `O(V)`。

### 深度优先搜索

==深度优先搜索（Depth-First-Search，DFS）== 。

最直观的例子就是“走迷宫”。

假设你站在迷宫的某个岔路口，然后想找到出口。你随意选择一个岔路口来走，走着走着发现走不通的时候，你就回退到上一个岔路口，重新选择一条路继续走，直到最终找到出口。这种走法就是一种深度优先搜索策略。

![](./images/SJJG+SFZM-31-03.jpg)

实线箭头表示遍历，虚线箭头表示回退。从图中我们可以看出，深度优先搜索找出来的路径，并不是顶点 s 到顶点 t 的最短路径。

深度优先搜索用的是一种比较著名的算法思想，**==回溯思想==**。这种思想解决问题的过程，非常适合用递归来实现。

```java
    /**
     * 全局变量或者类成员变量
     * 作用：当已经找到终止顶点 t 之后，就不再递归地继续查找了。
     */
    boolean found = false;

    /**
     * 深度优先搜索
     * @param s
     * @param t
     */
    public void dfs(int s, int t) {
        found = false;
        boolean[] visited = new boolean[v];
        int[] prev = new int[v];
        for (int i = 0; i < v; ++i) {
            prev[i] = -1;
        }
        recurDfs(s, t, visited, prev);
        print(prev, s, t);
    }

    private void recurDfs(int w, int t, boolean[] visited, int[] prev) {
        if (found == true) {
            return;
        }
        visited[w] = true;
        if (w == t) {
            found = true;
            return;
        }
        for (int i = 0; i < adj[w].size(); ++i) {
            int q = adj[w].get(i);
            if (!visited[q]) {
                prev[q] = w;
                recurDfs(q, t, visited, prev);
            }
        }
    }
```

> 深度优先搜索的时间、空间复杂度是多少呢？

每条边最多会被访问两次，一次是遍历，一次是回退。所以，图上的深度优先搜索算法的时间复杂度是 O(E)，E 表示边的个数。

深度优先搜索算法的消耗内存主要是 visited、prev 数组和递归调用栈。visited、prev 数组的大小跟顶点的个数 V 成正比，递归调用栈的最大深度不会超过顶点的个数，所以总的空间复杂度就是 O(V)。

### 小结

广度优先搜索和深度优先搜索是图上的两种最常用、最基本的搜索算法，也被叫作**暴力搜索算法**。这两种搜索算法仅适用于状态空间不大，也就是说**图不大的搜索**。

广度优先搜索，地毯式层层推进，从起始顶点开始，依次往外遍历；借助队列来实现，遍历得到的路径就是，起始顶点到终止顶点的最短路径。

深度优先搜索用的是回溯思想，非常适合用递归实现。换种说法，深度优先搜索是借助栈来实现的。

在执行效率方面，两者的时间复杂度都是 O(E)，空间复杂度是 O(V)。

### 思考题

> 如何找出社交网络中某个用户的三度好友关系？
> 
> 适合用图的广度优先搜索算法来解决

因为广度优先搜索是层层往外推进的。首先，遍历与起始顶点最近的一层顶点，也就是用户的一度好友，然后再遍历与用户距离的边数为 2 的顶点，也就是二度好友关系，以及与用户距离的边数为 3 的顶点，也就是三度好友关系。

只需要稍加改造一下广度优先搜索代码，用一个数组来记录每个顶点与起始顶点的距离，非常容易就可以找出三度好友关系。

> 能否用深度优先搜索来解决上面的三度好友关系？





> 学习数据结构最难的不是理解和掌握原理，而是能**==灵活地将各种场景和问题抽象成对应的数据结构和算法==**。
>
> 如何将迷宫抽象成一个图吗？或者换个说法，如何在计算机中存储一个迷宫？



